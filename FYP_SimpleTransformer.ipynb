{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yfDEI_het0Pz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from typing import Tuple\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.utils.data import dataset\n",
        "import math\n",
        "import torchtext\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vBAU7ge81C3G"
      },
      "outputs": [],
      "source": [
        "from torchtext.datasets import PennTreebank "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9MbhQHjn1fJU"
      },
      "outputs": [],
      "source": [
        "train_iter = PennTreebank(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = torchtext.vocab.build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZA9YwoHJ1h2t"
      },
      "outputs": [],
      "source": [
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "  \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "  data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ePsxOaE21iil"
      },
      "outputs": [],
      "source": [
        "train_iter, val_iter, test_iter = PennTreebank()\n",
        "train_data= data_process(train_iter)\n",
        "val_data= data_process(val_iter)\n",
        "test_data= data_process(test_iter)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qlf2Huho1kzw"
      },
      "outputs": [],
      "source": [
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_tnP8iT61nkI"
      },
      "outputs": [],
      "source": [
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ovQTLnrh1sDf"
      },
      "outputs": [],
      "source": [
        "bptt = 35\n",
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TYlrKUU51sj3"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size: int, embedding_dim: int, num_head: int, hidden_dim: int,\n",
        "                 num_layers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(embedding_dim, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(embedding_dim, num_head, hidden_dim, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.encoder = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.decoder = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Tensor, shape [seq_len, batch_size]\n",
        "            src_mask: Tensor, shape [seq_len, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape [seq_len, batch_size, vocab_size]\n",
        "        \"\"\"\n",
        "        src = self.encoder(src) * math.sqrt(self.embedding_dim)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
        "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
        "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "B1P2dpJ_18PH"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-math.log(10000.0) / embedding_dim))\n",
        "        pe = torch.zeros(max_len, 1, embedding_dim)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cmILnFf02AeZ"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vocab)  # size of vocabulary\n",
        "embedding_dim = 200  # embedding dimension\n",
        "hidden_dim = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
        "num_layers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "num_head = 2  # number of heads in nn.MultiheadAttention\n",
        "dropout = 0.2\n",
        "model = TransformerModel(vocab_size, embedding_dim, num_head, hidden_dim, num_layers, dropout).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JPNci_2v3MhU"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0  # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "def train(model: nn.Module) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "\n",
        "    num_batches = len(train_data) // bptt\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        seq_len = data.size(0)\n",
        "        if seq_len != bptt:  # only on last batch\n",
        "            src_mask = src_mask[:seq_len, :seq_len]\n",
        "        output = model(data, src_mask)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "G5UtWFjB3XZk"
      },
      "outputs": [],
      "source": [
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(eval_data, i)\n",
        "            seq_len = data.size(0)\n",
        "            if seq_len != bptt:\n",
        "                src_mask = src_mask[:seq_len, :seq_len]\n",
        "            output = model(data, src_mask)\n",
        "            output_flat = output.view(-1, vocab_size)\n",
        "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(eval_data) - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fN3e5RE53X-J",
        "outputId": "641d5ef2-aac7-42cb-bf2b-82c802dd79cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 1320 batches | lr 5.00 | ms/batch 64.81 | loss  7.15 | ppl  1271.34\n",
            "| epoch   1 |   400/ 1320 batches | lr 5.00 | ms/batch 62.74 | loss  6.08 | ppl   436.11\n",
            "| epoch   1 |   600/ 1320 batches | lr 5.00 | ms/batch 63.80 | loss  5.83 | ppl   341.02\n",
            "| epoch   1 |   800/ 1320 batches | lr 5.00 | ms/batch 60.89 | loss  5.66 | ppl   287.75\n",
            "| epoch   1 |  1000/ 1320 batches | lr 5.00 | ms/batch 63.61 | loss  5.59 | ppl   268.26\n",
            "| epoch   1 |  1200/ 1320 batches | lr 5.00 | ms/batch 61.76 | loss  5.48 | ppl   240.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 86.41s | valid loss  5.45 | valid ppl   233.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 1320 batches | lr 4.75 | ms/batch 61.86 | loss  5.41 | ppl   224.57\n",
            "| epoch   2 |   400/ 1320 batches | lr 4.75 | ms/batch 61.77 | loss  5.35 | ppl   210.67\n",
            "| epoch   2 |   600/ 1320 batches | lr 4.75 | ms/batch 64.05 | loss  5.32 | ppl   203.90\n",
            "| epoch   2 |   800/ 1320 batches | lr 4.75 | ms/batch 63.61 | loss  5.25 | ppl   191.46\n",
            "| epoch   2 |  1000/ 1320 batches | lr 4.75 | ms/batch 64.46 | loss  5.26 | ppl   193.24\n",
            "| epoch   2 |  1200/ 1320 batches | lr 4.75 | ms/batch 66.31 | loss  5.17 | ppl   176.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 88.06s | valid loss  5.32 | valid ppl   204.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 1320 batches | lr 4.51 | ms/batch 67.14 | loss  5.17 | ppl   176.63\n",
            "| epoch   3 |   400/ 1320 batches | lr 4.51 | ms/batch 67.66 | loss  5.14 | ppl   171.09\n",
            "| epoch   3 |   600/ 1320 batches | lr 4.51 | ms/batch 68.70 | loss  5.11 | ppl   166.38\n",
            "| epoch   3 |   800/ 1320 batches | lr 4.51 | ms/batch 69.05 | loss  5.07 | ppl   159.34\n",
            "| epoch   3 |  1000/ 1320 batches | lr 4.51 | ms/batch 70.91 | loss  5.10 | ppl   163.55\n",
            "| epoch   3 |  1200/ 1320 batches | lr 4.51 | ms/batch 71.25 | loss  5.01 | ppl   150.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 95.21s | valid loss  5.26 | valid ppl   192.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/ 1320 batches | lr 4.29 | ms/batch 186.05 | loss  5.03 | ppl   152.83\n",
            "| epoch   4 |   400/ 1320 batches | lr 4.29 | ms/batch 246.89 | loss  5.00 | ppl   149.12\n",
            "| epoch   4 |   600/ 1320 batches | lr 4.29 | ms/batch 250.89 | loss  4.98 | ppl   145.98\n",
            "| epoch   4 |   800/ 1320 batches | lr 4.29 | ms/batch 253.28 | loss  4.94 | ppl   140.43\n",
            "| epoch   4 |  1000/ 1320 batches | lr 4.29 | ms/batch 256.74 | loss  4.98 | ppl   145.34\n",
            "| epoch   4 |  1200/ 1320 batches | lr 4.29 | ms/batch 253.32 | loss  4.89 | ppl   133.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 326.91s | valid loss  5.24 | valid ppl   189.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/ 1320 batches | lr 4.07 | ms/batch 259.45 | loss  4.92 | ppl   137.67\n",
            "| epoch   5 |   400/ 1320 batches | lr 4.07 | ms/batch 259.69 | loss  4.90 | ppl   133.76\n",
            "| epoch   5 |   600/ 1320 batches | lr 4.07 | ms/batch 255.94 | loss  4.88 | ppl   131.68\n",
            "| epoch   5 |   800/ 1320 batches | lr 4.07 | ms/batch 250.54 | loss  4.84 | ppl   126.75\n",
            "| epoch   5 |  1000/ 1320 batches | lr 4.07 | ms/batch 74.17 | loss  4.89 | ppl   132.43\n",
            "| epoch   5 |  1200/ 1320 batches | lr 4.07 | ms/batch 69.30 | loss  4.80 | ppl   121.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 245.46s | valid loss  5.22 | valid ppl   185.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/ 1320 batches | lr 3.87 | ms/batch 69.94 | loss  4.84 | ppl   125.87\n",
            "| epoch   6 |   400/ 1320 batches | lr 3.87 | ms/batch 69.55 | loss  4.81 | ppl   122.44\n",
            "| epoch   6 |   600/ 1320 batches | lr 3.87 | ms/batch 68.82 | loss  4.79 | ppl   120.77\n",
            "| epoch   6 |   800/ 1320 batches | lr 3.87 | ms/batch 68.41 | loss  4.76 | ppl   117.18\n",
            "| epoch   6 |  1000/ 1320 batches | lr 3.87 | ms/batch 68.13 | loss  4.81 | ppl   122.43\n",
            "| epoch   6 |  1200/ 1320 batches | lr 3.87 | ms/batch 68.40 | loss  4.72 | ppl   111.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 94.43s | valid loss  5.18 | valid ppl   177.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/ 1320 batches | lr 3.68 | ms/batch 67.91 | loss  4.76 | ppl   116.84\n",
            "| epoch   7 |   400/ 1320 batches | lr 3.68 | ms/batch 67.38 | loss  4.73 | ppl   113.84\n",
            "| epoch   7 |   600/ 1320 batches | lr 3.68 | ms/batch 67.31 | loss  4.72 | ppl   112.09\n",
            "| epoch   7 |   800/ 1320 batches | lr 3.68 | ms/batch 69.09 | loss  4.69 | ppl   108.98\n",
            "| epoch   7 |  1000/ 1320 batches | lr 3.68 | ms/batch 68.23 | loss  4.74 | ppl   114.06\n",
            "| epoch   7 |  1200/ 1320 batches | lr 3.68 | ms/batch 67.69 | loss  4.65 | ppl   104.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 93.23s | valid loss  5.16 | valid ppl   174.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/ 1320 batches | lr 3.49 | ms/batch 73.56 | loss  4.70 | ppl   109.56\n",
            "| epoch   8 |   400/ 1320 batches | lr 3.49 | ms/batch 68.45 | loss  4.67 | ppl   106.20\n",
            "| epoch   8 |   600/ 1320 batches | lr 3.49 | ms/batch 70.44 | loss  4.65 | ppl   104.46\n",
            "| epoch   8 |   800/ 1320 batches | lr 3.49 | ms/batch 69.80 | loss  4.62 | ppl   101.96\n",
            "| epoch   8 |  1000/ 1320 batches | lr 3.49 | ms/batch 69.11 | loss  4.67 | ppl   106.97\n",
            "| epoch   8 |  1200/ 1320 batches | lr 3.49 | ms/batch 67.83 | loss  4.58 | ppl    97.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 96.00s | valid loss  5.18 | valid ppl   177.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/ 1320 batches | lr 3.32 | ms/batch 70.31 | loss  4.63 | ppl   102.95\n",
            "| epoch   9 |   400/ 1320 batches | lr 3.32 | ms/batch 69.44 | loss  4.61 | ppl   100.19\n",
            "| epoch   9 |   600/ 1320 batches | lr 3.32 | ms/batch 70.40 | loss  4.60 | ppl    99.11\n",
            "| epoch   9 |   800/ 1320 batches | lr 3.32 | ms/batch 69.64 | loss  4.56 | ppl    96.06\n",
            "| epoch   9 |  1000/ 1320 batches | lr 3.32 | ms/batch 69.37 | loss  4.62 | ppl   101.45\n",
            "| epoch   9 |  1200/ 1320 batches | lr 3.32 | ms/batch 69.67 | loss  4.52 | ppl    91.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 95.65s | valid loss  5.17 | valid ppl   176.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/ 1320 batches | lr 3.15 | ms/batch 69.38 | loss  4.58 | ppl    97.29\n",
            "| epoch  10 |   400/ 1320 batches | lr 3.15 | ms/batch 69.21 | loss  4.55 | ppl    94.77\n",
            "| epoch  10 |   600/ 1320 batches | lr 3.15 | ms/batch 70.55 | loss  4.54 | ppl    93.70\n",
            "| epoch  10 |   800/ 1320 batches | lr 3.15 | ms/batch 70.18 | loss  4.51 | ppl    90.97\n",
            "| epoch  10 |  1000/ 1320 batches | lr 3.15 | ms/batch 68.77 | loss  4.57 | ppl    96.23\n",
            "| epoch  10 |  1200/ 1320 batches | lr 3.15 | ms/batch 69.57 | loss  4.47 | ppl    87.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 95.20s | valid loss  5.20 | valid ppl   181.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   200/ 1320 batches | lr 2.99 | ms/batch 70.90 | loss  4.52 | ppl    92.26\n",
            "| epoch  11 |   400/ 1320 batches | lr 2.99 | ms/batch 70.89 | loss  4.49 | ppl    89.30\n",
            "| epoch  11 |   600/ 1320 batches | lr 2.99 | ms/batch 69.05 | loss  4.49 | ppl    88.81\n",
            "| epoch  11 |   800/ 1320 batches | lr 2.99 | ms/batch 70.24 | loss  4.45 | ppl    85.88\n",
            "| epoch  11 |  1000/ 1320 batches | lr 2.99 | ms/batch 72.14 | loss  4.51 | ppl    90.81\n",
            "| epoch  11 |  1200/ 1320 batches | lr 2.99 | ms/batch 72.71 | loss  4.42 | ppl    82.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 97.19s | valid loss  5.18 | valid ppl   177.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |   200/ 1320 batches | lr 2.84 | ms/batch 68.56 | loss  4.48 | ppl    88.30\n",
            "| epoch  12 |   400/ 1320 batches | lr 2.84 | ms/batch 69.09 | loss  4.45 | ppl    85.41\n",
            "| epoch  12 |   600/ 1320 batches | lr 2.84 | ms/batch 68.94 | loss  4.44 | ppl    84.91\n",
            "| epoch  12 |   800/ 1320 batches | lr 2.84 | ms/batch 69.65 | loss  4.41 | ppl    82.19\n",
            "| epoch  12 |  1000/ 1320 batches | lr 2.84 | ms/batch 69.74 | loss  4.47 | ppl    87.27\n",
            "| epoch  12 |  1200/ 1320 batches | lr 2.84 | ms/batch 69.40 | loss  4.37 | ppl    79.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 94.87s | valid loss  5.19 | valid ppl   179.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   200/ 1320 batches | lr 2.70 | ms/batch 69.99 | loss  4.44 | ppl    84.38\n",
            "| epoch  13 |   400/ 1320 batches | lr 2.70 | ms/batch 71.27 | loss  4.41 | ppl    82.26\n",
            "| epoch  13 |   600/ 1320 batches | lr 2.70 | ms/batch 68.99 | loss  4.40 | ppl    81.14\n",
            "| epoch  13 |   800/ 1320 batches | lr 2.70 | ms/batch 67.95 | loss  4.36 | ppl    78.63\n",
            "| epoch  13 |  1000/ 1320 batches | lr 2.70 | ms/batch 69.65 | loss  4.42 | ppl    83.41\n",
            "| epoch  13 |  1200/ 1320 batches | lr 2.70 | ms/batch 68.99 | loss  4.33 | ppl    75.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 95.13s | valid loss  5.20 | valid ppl   180.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   200/ 1320 batches | lr 2.57 | ms/batch 67.85 | loss  4.39 | ppl    80.72\n",
            "| epoch  14 |   400/ 1320 batches | lr 2.57 | ms/batch 69.61 | loss  4.36 | ppl    78.65\n",
            "| epoch  14 |   600/ 1320 batches | lr 2.57 | ms/batch 68.31 | loss  4.35 | ppl    77.67\n",
            "| epoch  14 |   800/ 1320 batches | lr 2.57 | ms/batch 67.00 | loss  4.33 | ppl    75.91\n",
            "| epoch  14 |  1000/ 1320 batches | lr 2.57 | ms/batch 69.54 | loss  4.39 | ppl    80.35\n",
            "| epoch  14 |  1200/ 1320 batches | lr 2.57 | ms/batch 69.64 | loss  4.29 | ppl    72.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 94.33s | valid loss  5.21 | valid ppl   183.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |   200/ 1320 batches | lr 2.44 | ms/batch 69.40 | loss  4.36 | ppl    78.07\n",
            "| epoch  15 |   400/ 1320 batches | lr 2.44 | ms/batch 70.05 | loss  4.33 | ppl    75.77\n",
            "| epoch  15 |   600/ 1320 batches | lr 2.44 | ms/batch 67.59 | loss  4.31 | ppl    74.80\n",
            "| epoch  15 |   800/ 1320 batches | lr 2.44 | ms/batch 69.28 | loss  4.29 | ppl    72.78\n",
            "| epoch  15 |  1000/ 1320 batches | lr 2.44 | ms/batch 68.57 | loss  4.35 | ppl    77.35\n",
            "| epoch  15 |  1200/ 1320 batches | lr 2.44 | ms/batch 66.01 | loss  4.25 | ppl    69.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 94.25s | valid loss  5.21 | valid ppl   182.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   200/ 1320 batches | lr 2.32 | ms/batch 70.52 | loss  4.32 | ppl    75.46\n",
            "| epoch  16 |   400/ 1320 batches | lr 2.32 | ms/batch 66.19 | loss  4.29 | ppl    73.02\n",
            "| epoch  16 |   600/ 1320 batches | lr 2.32 | ms/batch 69.43 | loss  4.28 | ppl    72.41\n",
            "| epoch  16 |   800/ 1320 batches | lr 2.32 | ms/batch 67.32 | loss  4.25 | ppl    70.34\n",
            "| epoch  16 |  1000/ 1320 batches | lr 2.32 | ms/batch 69.07 | loss  4.31 | ppl    74.72\n",
            "| epoch  16 |  1200/ 1320 batches | lr 2.32 | ms/batch 68.72 | loss  4.21 | ppl    67.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 94.21s | valid loss  5.22 | valid ppl   184.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   200/ 1320 batches | lr 2.20 | ms/batch 70.27 | loss  4.29 | ppl    72.63\n",
            "| epoch  17 |   400/ 1320 batches | lr 2.20 | ms/batch 66.57 | loss  4.26 | ppl    70.67\n",
            "| epoch  17 |   600/ 1320 batches | lr 2.20 | ms/batch 70.32 | loss  4.26 | ppl    70.53\n",
            "| epoch  17 |   800/ 1320 batches | lr 2.20 | ms/batch 69.84 | loss  4.22 | ppl    68.13\n",
            "| epoch  17 |  1000/ 1320 batches | lr 2.20 | ms/batch 68.79 | loss  4.28 | ppl    72.05\n",
            "| epoch  17 |  1200/ 1320 batches | lr 2.20 | ms/batch 70.41 | loss  4.17 | ppl    64.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 94.81s | valid loss  5.21 | valid ppl   182.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   200/ 1320 batches | lr 2.09 | ms/batch 69.87 | loss  4.26 | ppl    70.60\n",
            "| epoch  18 |   400/ 1320 batches | lr 2.09 | ms/batch 69.96 | loss  4.22 | ppl    68.37\n",
            "| epoch  18 |   600/ 1320 batches | lr 2.09 | ms/batch 68.27 | loss  4.22 | ppl    67.93\n",
            "| epoch  18 |   800/ 1320 batches | lr 2.09 | ms/batch 66.37 | loss  4.19 | ppl    66.06\n",
            "| epoch  18 |  1000/ 1320 batches | lr 2.09 | ms/batch 68.81 | loss  4.25 | ppl    70.16\n",
            "| epoch  18 |  1200/ 1320 batches | lr 2.09 | ms/batch 69.67 | loss  4.15 | ppl    63.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 94.42s | valid loss  5.20 | valid ppl   181.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |   200/ 1320 batches | lr 1.99 | ms/batch 68.76 | loss  4.23 | ppl    68.51\n",
            "| epoch  19 |   400/ 1320 batches | lr 1.99 | ms/batch 69.01 | loss  4.19 | ppl    66.32\n",
            "| epoch  19 |   600/ 1320 batches | lr 1.99 | ms/batch 67.79 | loss  4.19 | ppl    65.92\n",
            "| epoch  19 |   800/ 1320 batches | lr 1.99 | ms/batch 68.65 | loss  4.16 | ppl    64.02\n",
            "| epoch  19 |  1000/ 1320 batches | lr 1.99 | ms/batch 69.47 | loss  4.22 | ppl    67.78\n",
            "| epoch  19 |  1200/ 1320 batches | lr 1.99 | ms/batch 69.59 | loss  4.12 | ppl    61.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 94.66s | valid loss  5.21 | valid ppl   183.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |   200/ 1320 batches | lr 1.89 | ms/batch 71.35 | loss  4.20 | ppl    66.71\n",
            "| epoch  20 |   400/ 1320 batches | lr 1.89 | ms/batch 71.06 | loss  4.17 | ppl    64.77\n",
            "| epoch  20 |   600/ 1320 batches | lr 1.89 | ms/batch 70.29 | loss  4.16 | ppl    64.05\n",
            "| epoch  20 |   800/ 1320 batches | lr 1.89 | ms/batch 70.64 | loss  4.13 | ppl    62.32\n",
            "| epoch  20 |  1000/ 1320 batches | lr 1.89 | ms/batch 69.02 | loss  4.19 | ppl    66.32\n",
            "| epoch  20 |  1200/ 1320 batches | lr 1.89 | ms/batch 69.05 | loss  4.09 | ppl    59.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 96.18s | valid loss  5.22 | valid ppl   185.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |   200/ 1320 batches | lr 1.79 | ms/batch 70.27 | loss  4.18 | ppl    65.10\n",
            "| epoch  21 |   400/ 1320 batches | lr 1.79 | ms/batch 70.06 | loss  4.15 | ppl    63.17\n",
            "| epoch  21 |   600/ 1320 batches | lr 1.79 | ms/batch 69.10 | loss  4.14 | ppl    62.51\n",
            "| epoch  21 |   800/ 1320 batches | lr 1.79 | ms/batch 69.12 | loss  4.10 | ppl    60.61\n",
            "| epoch  21 |  1000/ 1320 batches | lr 1.79 | ms/batch 69.38 | loss  4.17 | ppl    64.53\n",
            "| epoch  21 |  1200/ 1320 batches | lr 1.79 | ms/batch 69.35 | loss  4.07 | ppl    58.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 95.53s | valid loss  5.22 | valid ppl   184.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |   200/ 1320 batches | lr 1.70 | ms/batch 70.74 | loss  4.15 | ppl    63.45\n",
            "| epoch  22 |   400/ 1320 batches | lr 1.70 | ms/batch 69.02 | loss  4.12 | ppl    61.72\n",
            "| epoch  22 |   600/ 1320 batches | lr 1.70 | ms/batch 69.77 | loss  4.11 | ppl    60.80\n",
            "| epoch  22 |   800/ 1320 batches | lr 1.70 | ms/batch 69.12 | loss  4.09 | ppl    59.69\n",
            "| epoch  22 |  1000/ 1320 batches | lr 1.70 | ms/batch 69.22 | loss  4.14 | ppl    62.97\n",
            "| epoch  22 |  1200/ 1320 batches | lr 1.70 | ms/batch 69.49 | loss  4.04 | ppl    56.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 95.52s | valid loss  5.23 | valid ppl   186.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |   200/ 1320 batches | lr 1.62 | ms/batch 70.78 | loss  4.13 | ppl    61.91\n",
            "| epoch  23 |   400/ 1320 batches | lr 1.62 | ms/batch 70.36 | loss  4.10 | ppl    60.13\n",
            "| epoch  23 |   600/ 1320 batches | lr 1.62 | ms/batch 70.31 | loss  4.09 | ppl    59.48\n",
            "| epoch  23 |   800/ 1320 batches | lr 1.62 | ms/batch 70.56 | loss  4.06 | ppl    57.84\n",
            "| epoch  23 |  1000/ 1320 batches | lr 1.62 | ms/batch 71.12 | loss  4.12 | ppl    61.47\n",
            "| epoch  23 |  1200/ 1320 batches | lr 1.62 | ms/batch 70.98 | loss  4.01 | ppl    55.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 97.20s | valid loss  5.24 | valid ppl   188.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |   200/ 1320 batches | lr 1.54 | ms/batch 72.57 | loss  4.10 | ppl    60.54\n",
            "| epoch  24 |   400/ 1320 batches | lr 1.54 | ms/batch 72.87 | loss  4.08 | ppl    58.98\n",
            "| epoch  24 |   600/ 1320 batches | lr 1.54 | ms/batch 71.68 | loss  4.07 | ppl    58.39\n",
            "| epoch  24 |   800/ 1320 batches | lr 1.54 | ms/batch 70.71 | loss  4.04 | ppl    56.67\n",
            "| epoch  24 |  1000/ 1320 batches | lr 1.54 | ms/batch 70.39 | loss  4.10 | ppl    60.16\n",
            "| epoch  24 |  1200/ 1320 batches | lr 1.54 | ms/batch 69.93 | loss  4.00 | ppl    54.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 97.49s | valid loss  5.23 | valid ppl   187.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |   200/ 1320 batches | lr 1.46 | ms/batch 69.28 | loss  4.08 | ppl    59.41\n",
            "| epoch  25 |   400/ 1320 batches | lr 1.46 | ms/batch 68.46 | loss  4.05 | ppl    57.56\n",
            "| epoch  25 |   600/ 1320 batches | lr 1.46 | ms/batch 68.78 | loss  4.04 | ppl    57.03\n",
            "| epoch  25 |   800/ 1320 batches | lr 1.46 | ms/batch 69.13 | loss  4.02 | ppl    55.72\n",
            "| epoch  25 |  1000/ 1320 batches | lr 1.46 | ms/batch 68.75 | loss  4.08 | ppl    59.18\n",
            "| epoch  25 |  1200/ 1320 batches | lr 1.46 | ms/batch 68.38 | loss  3.97 | ppl    53.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 94.28s | valid loss  5.24 | valid ppl   189.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |   200/ 1320 batches | lr 1.39 | ms/batch 69.52 | loss  4.07 | ppl    58.28\n",
            "| epoch  26 |   400/ 1320 batches | lr 1.39 | ms/batch 68.64 | loss  4.04 | ppl    56.58\n",
            "| epoch  26 |   600/ 1320 batches | lr 1.39 | ms/batch 68.92 | loss  4.02 | ppl    55.81\n",
            "| epoch  26 |   800/ 1320 batches | lr 1.39 | ms/batch 69.94 | loss  4.01 | ppl    54.88\n",
            "| epoch  26 |  1000/ 1320 batches | lr 1.39 | ms/batch 70.30 | loss  4.06 | ppl    58.00\n",
            "| epoch  26 |  1200/ 1320 batches | lr 1.39 | ms/batch 69.59 | loss  3.96 | ppl    52.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 95.39s | valid loss  5.23 | valid ppl   187.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |   200/ 1320 batches | lr 1.32 | ms/batch 70.35 | loss  4.04 | ppl    57.10\n",
            "| epoch  27 |   400/ 1320 batches | lr 1.32 | ms/batch 69.85 | loss  4.02 | ppl    55.64\n",
            "| epoch  27 |   600/ 1320 batches | lr 1.32 | ms/batch 70.16 | loss  4.01 | ppl    55.19\n",
            "| epoch  27 |   800/ 1320 batches | lr 1.32 | ms/batch 69.79 | loss  3.98 | ppl    53.69\n",
            "| epoch  27 |  1000/ 1320 batches | lr 1.32 | ms/batch 69.29 | loss  4.05 | ppl    57.12\n",
            "| epoch  27 |  1200/ 1320 batches | lr 1.32 | ms/batch 70.74 | loss  3.94 | ppl    51.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 96.25s | valid loss  5.24 | valid ppl   188.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |   200/ 1320 batches | lr 1.25 | ms/batch 72.15 | loss  4.03 | ppl    56.28\n",
            "| epoch  28 |   400/ 1320 batches | lr 1.25 | ms/batch 71.08 | loss  4.00 | ppl    54.81\n",
            "| epoch  28 |   600/ 1320 batches | lr 1.25 | ms/batch 71.13 | loss  3.99 | ppl    54.31\n",
            "| epoch  28 |   800/ 1320 batches | lr 1.25 | ms/batch 71.69 | loss  3.97 | ppl    52.84\n",
            "| epoch  28 |  1000/ 1320 batches | lr 1.25 | ms/batch 71.77 | loss  4.03 | ppl    56.19\n",
            "| epoch  28 |  1200/ 1320 batches | lr 1.25 | ms/batch 70.44 | loss  3.92 | ppl    50.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 97.81s | valid loss  5.26 | valid ppl   192.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |   200/ 1320 batches | lr 1.19 | ms/batch 72.36 | loss  4.02 | ppl    55.57\n",
            "| epoch  29 |   400/ 1320 batches | lr 1.19 | ms/batch 71.37 | loss  3.99 | ppl    53.95\n",
            "| epoch  29 |   600/ 1320 batches | lr 1.19 | ms/batch 71.01 | loss  3.98 | ppl    53.32\n",
            "| epoch  29 |   800/ 1320 batches | lr 1.19 | ms/batch 71.82 | loss  3.96 | ppl    52.24\n",
            "| epoch  29 |  1000/ 1320 batches | lr 1.19 | ms/batch 72.47 | loss  4.01 | ppl    55.16\n",
            "| epoch  29 |  1200/ 1320 batches | lr 1.19 | ms/batch 70.97 | loss  3.90 | ppl    49.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 98.18s | valid loss  5.25 | valid ppl   190.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |   200/ 1320 batches | lr 1.13 | ms/batch 71.84 | loss  4.00 | ppl    54.40\n",
            "| epoch  30 |   400/ 1320 batches | lr 1.13 | ms/batch 71.63 | loss  3.97 | ppl    53.22\n",
            "| epoch  30 |   600/ 1320 batches | lr 1.13 | ms/batch 70.70 | loss  3.96 | ppl    52.43\n",
            "| epoch  30 |   800/ 1320 batches | lr 1.13 | ms/batch 71.34 | loss  3.94 | ppl    51.41\n",
            "| epoch  30 |  1000/ 1320 batches | lr 1.13 | ms/batch 70.90 | loss  4.00 | ppl    54.49\n",
            "| epoch  30 |  1200/ 1320 batches | lr 1.13 | ms/batch 71.67 | loss  3.89 | ppl    49.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 97.87s | valid loss  5.25 | valid ppl   189.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  31 |   200/ 1320 batches | lr 1.07 | ms/batch 74.18 | loss  3.99 | ppl    53.85\n",
            "| epoch  31 |   400/ 1320 batches | lr 1.07 | ms/batch 72.23 | loss  3.96 | ppl    52.38\n",
            "| epoch  31 |   600/ 1320 batches | lr 1.07 | ms/batch 71.47 | loss  3.95 | ppl    51.74\n",
            "| epoch  31 |   800/ 1320 batches | lr 1.07 | ms/batch 70.86 | loss  3.93 | ppl    50.85\n",
            "| epoch  31 |  1000/ 1320 batches | lr 1.07 | ms/batch 72.16 | loss  3.99 | ppl    53.85\n",
            "| epoch  31 |  1200/ 1320 batches | lr 1.07 | ms/batch 71.52 | loss  3.87 | ppl    48.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 98.92s | valid loss  5.27 | valid ppl   194.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |   200/ 1320 batches | lr 1.02 | ms/batch 74.27 | loss  3.97 | ppl    53.10\n",
            "| epoch  32 |   400/ 1320 batches | lr 1.02 | ms/batch 73.94 | loss  3.94 | ppl    51.59\n",
            "| epoch  32 |   600/ 1320 batches | lr 1.02 | ms/batch 74.65 | loss  3.94 | ppl    51.26\n",
            "| epoch  32 |   800/ 1320 batches | lr 1.02 | ms/batch 72.96 | loss  3.91 | ppl    49.99\n",
            "| epoch  32 |  1000/ 1320 batches | lr 1.02 | ms/batch 71.69 | loss  3.97 | ppl    52.88\n",
            "| epoch  32 |  1200/ 1320 batches | lr 1.02 | ms/batch 72.04 | loss  3.86 | ppl    47.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 100.27s | valid loss  5.26 | valid ppl   192.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |   200/ 1320 batches | lr 0.97 | ms/batch 71.68 | loss  3.96 | ppl    52.43\n",
            "| epoch  33 |   400/ 1320 batches | lr 0.97 | ms/batch 70.94 | loss  3.94 | ppl    51.27\n",
            "| epoch  33 |   600/ 1320 batches | lr 0.97 | ms/batch 70.70 | loss  3.92 | ppl    50.50\n",
            "| epoch  33 |   800/ 1320 batches | lr 0.97 | ms/batch 70.47 | loss  3.90 | ppl    49.25\n",
            "| epoch  33 |  1000/ 1320 batches | lr 0.97 | ms/batch 71.02 | loss  3.96 | ppl    52.29\n",
            "| epoch  33 |  1200/ 1320 batches | lr 0.97 | ms/batch 71.01 | loss  3.85 | ppl    47.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 97.36s | valid loss  5.26 | valid ppl   191.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |   200/ 1320 batches | lr 0.92 | ms/batch 71.36 | loss  3.95 | ppl    51.81\n",
            "| epoch  34 |   400/ 1320 batches | lr 0.92 | ms/batch 71.16 | loss  3.93 | ppl    50.69\n",
            "| epoch  34 |   600/ 1320 batches | lr 0.92 | ms/batch 72.33 | loss  3.91 | ppl    49.74\n",
            "| epoch  34 |   800/ 1320 batches | lr 0.92 | ms/batch 72.58 | loss  3.89 | ppl    48.72\n",
            "| epoch  34 |  1000/ 1320 batches | lr 0.92 | ms/batch 71.84 | loss  3.95 | ppl    51.73\n",
            "| epoch  34 |  1200/ 1320 batches | lr 0.92 | ms/batch 71.58 | loss  3.84 | ppl    46.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 98.50s | valid loss  5.26 | valid ppl   192.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |   200/ 1320 batches | lr 0.87 | ms/batch 71.99 | loss  3.94 | ppl    51.22\n",
            "| epoch  35 |   400/ 1320 batches | lr 0.87 | ms/batch 72.11 | loss  3.91 | ppl    50.00\n",
            "| epoch  35 |   600/ 1320 batches | lr 0.87 | ms/batch 71.45 | loss  3.90 | ppl    49.20\n",
            "| epoch  35 |   800/ 1320 batches | lr 0.87 | ms/batch 71.41 | loss  3.87 | ppl    48.18\n",
            "| epoch  35 |  1000/ 1320 batches | lr 0.87 | ms/batch 71.42 | loss  3.94 | ppl    51.17\n",
            "| epoch  35 |  1200/ 1320 batches | lr 0.87 | ms/batch 71.48 | loss  3.83 | ppl    46.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 98.26s | valid loss  5.26 | valid ppl   193.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |   200/ 1320 batches | lr 0.83 | ms/batch 71.59 | loss  3.92 | ppl    50.52\n",
            "| epoch  36 |   400/ 1320 batches | lr 0.83 | ms/batch 72.88 | loss  3.90 | ppl    49.35\n",
            "| epoch  36 |   600/ 1320 batches | lr 0.83 | ms/batch 71.88 | loss  3.89 | ppl    48.82\n",
            "| epoch  36 |   800/ 1320 batches | lr 0.83 | ms/batch 70.82 | loss  3.87 | ppl    47.86\n",
            "| epoch  36 |  1000/ 1320 batches | lr 0.83 | ms/batch 71.56 | loss  3.92 | ppl    50.39\n",
            "| epoch  36 |  1200/ 1320 batches | lr 0.83 | ms/batch 72.27 | loss  3.82 | ppl    45.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 98.58s | valid loss  5.27 | valid ppl   195.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  37 |   200/ 1320 batches | lr 0.79 | ms/batch 72.93 | loss  3.92 | ppl    50.26\n",
            "| epoch  37 |   400/ 1320 batches | lr 0.79 | ms/batch 71.89 | loss  3.89 | ppl    49.06\n",
            "| epoch  37 |   600/ 1320 batches | lr 0.79 | ms/batch 71.17 | loss  3.88 | ppl    48.24\n",
            "| epoch  37 |   800/ 1320 batches | lr 0.79 | ms/batch 71.65 | loss  3.86 | ppl    47.27\n",
            "| epoch  37 |  1000/ 1320 batches | lr 0.79 | ms/batch 71.11 | loss  3.91 | ppl    49.97\n",
            "| epoch  37 |  1200/ 1320 batches | lr 0.79 | ms/batch 71.53 | loss  3.81 | ppl    45.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 98.37s | valid loss  5.27 | valid ppl   193.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |   200/ 1320 batches | lr 0.75 | ms/batch 72.90 | loss  3.91 | ppl    49.71\n",
            "| epoch  38 |   400/ 1320 batches | lr 0.75 | ms/batch 72.14 | loss  3.88 | ppl    48.45\n",
            "| epoch  38 |   600/ 1320 batches | lr 0.75 | ms/batch 72.71 | loss  3.87 | ppl    47.76\n",
            "| epoch  38 |   800/ 1320 batches | lr 0.75 | ms/batch 72.06 | loss  3.84 | ppl    46.73\n",
            "| epoch  38 |  1000/ 1320 batches | lr 0.75 | ms/batch 71.41 | loss  3.90 | ppl    49.57\n",
            "| epoch  38 |  1200/ 1320 batches | lr 0.75 | ms/batch 71.49 | loss  3.80 | ppl    44.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 98.93s | valid loss  5.28 | valid ppl   195.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |   200/ 1320 batches | lr 0.71 | ms/batch 72.71 | loss  3.90 | ppl    49.20\n",
            "| epoch  39 |   400/ 1320 batches | lr 0.71 | ms/batch 71.68 | loss  3.87 | ppl    48.04\n",
            "| epoch  39 |   600/ 1320 batches | lr 0.71 | ms/batch 72.19 | loss  3.86 | ppl    47.39\n",
            "| epoch  39 |   800/ 1320 batches | lr 0.71 | ms/batch 72.55 | loss  3.84 | ppl    46.50\n",
            "| epoch  39 |  1000/ 1320 batches | lr 0.71 | ms/batch 71.91 | loss  3.89 | ppl    49.05\n",
            "| epoch  39 |  1200/ 1320 batches | lr 0.71 | ms/batch 72.93 | loss  3.79 | ppl    44.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 99.15s | valid loss  5.29 | valid ppl   197.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |   200/ 1320 batches | lr 0.68 | ms/batch 72.04 | loss  3.89 | ppl    48.91\n",
            "| epoch  40 |   400/ 1320 batches | lr 0.68 | ms/batch 71.59 | loss  3.86 | ppl    47.70\n",
            "| epoch  40 |   600/ 1320 batches | lr 0.68 | ms/batch 71.58 | loss  3.85 | ppl    47.05\n",
            "| epoch  40 |   800/ 1320 batches | lr 0.68 | ms/batch 72.09 | loss  3.83 | ppl    45.93\n",
            "| epoch  40 |  1000/ 1320 batches | lr 0.68 | ms/batch 71.37 | loss  3.89 | ppl    48.71\n",
            "| epoch  40 |  1200/ 1320 batches | lr 0.68 | ms/batch 72.19 | loss  3.79 | ppl    44.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 98.48s | valid loss  5.29 | valid ppl   198.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  41 |   200/ 1320 batches | lr 0.64 | ms/batch 72.37 | loss  3.88 | ppl    48.50\n",
            "| epoch  41 |   400/ 1320 batches | lr 0.64 | ms/batch 71.72 | loss  3.86 | ppl    47.35\n",
            "| epoch  41 |   600/ 1320 batches | lr 0.64 | ms/batch 71.94 | loss  3.84 | ppl    46.59\n",
            "| epoch  41 |   800/ 1320 batches | lr 0.64 | ms/batch 71.71 | loss  3.82 | ppl    45.60\n",
            "| epoch  41 |  1000/ 1320 batches | lr 0.64 | ms/batch 72.51 | loss  3.88 | ppl    48.21\n",
            "| epoch  41 |  1200/ 1320 batches | lr 0.64 | ms/batch 72.14 | loss  3.77 | ppl    43.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 98.91s | valid loss  5.29 | valid ppl   198.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  42 |   200/ 1320 batches | lr 0.61 | ms/batch 73.36 | loss  3.87 | ppl    48.10\n",
            "| epoch  42 |   400/ 1320 batches | lr 0.61 | ms/batch 72.26 | loss  3.85 | ppl    47.03\n",
            "| epoch  42 |   600/ 1320 batches | lr 0.61 | ms/batch 73.05 | loss  3.83 | ppl    46.17\n",
            "| epoch  42 |   800/ 1320 batches | lr 0.61 | ms/batch 71.91 | loss  3.81 | ppl    45.26\n",
            "| epoch  42 |  1000/ 1320 batches | lr 0.61 | ms/batch 72.55 | loss  3.87 | ppl    47.96\n",
            "| epoch  42 |  1200/ 1320 batches | lr 0.61 | ms/batch 72.31 | loss  3.77 | ppl    43.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 99.39s | valid loss  5.28 | valid ppl   196.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  43 |   200/ 1320 batches | lr 0.58 | ms/batch 73.22 | loss  3.87 | ppl    47.79\n",
            "| epoch  43 |   400/ 1320 batches | lr 0.58 | ms/batch 72.08 | loss  3.84 | ppl    46.57\n",
            "| epoch  43 |   600/ 1320 batches | lr 0.58 | ms/batch 72.00 | loss  3.83 | ppl    45.90\n",
            "| epoch  43 |   800/ 1320 batches | lr 0.58 | ms/batch 71.09 | loss  3.80 | ppl    44.85\n",
            "| epoch  43 |  1000/ 1320 batches | lr 0.58 | ms/batch 71.46 | loss  3.86 | ppl    47.64\n",
            "| epoch  43 |  1200/ 1320 batches | lr 0.58 | ms/batch 73.61 | loss  3.76 | ppl    42.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 99.26s | valid loss  5.28 | valid ppl   195.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  44 |   200/ 1320 batches | lr 0.55 | ms/batch 72.97 | loss  3.86 | ppl    47.66\n",
            "| epoch  44 |   400/ 1320 batches | lr 0.55 | ms/batch 72.88 | loss  3.83 | ppl    46.24\n",
            "| epoch  44 |   600/ 1320 batches | lr 0.55 | ms/batch 73.72 | loss  3.82 | ppl    45.66\n",
            "| epoch  44 |   800/ 1320 batches | lr 0.55 | ms/batch 72.35 | loss  3.80 | ppl    44.55\n",
            "| epoch  44 |  1000/ 1320 batches | lr 0.55 | ms/batch 71.98 | loss  3.85 | ppl    47.22\n",
            "| epoch  44 |  1200/ 1320 batches | lr 0.55 | ms/batch 72.45 | loss  3.75 | ppl    42.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 99.66s | valid loss  5.29 | valid ppl   198.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  45 |   200/ 1320 batches | lr 0.52 | ms/batch 72.78 | loss  3.86 | ppl    47.25\n",
            "| epoch  45 |   400/ 1320 batches | lr 0.52 | ms/batch 72.32 | loss  3.83 | ppl    45.92\n",
            "| epoch  45 |   600/ 1320 batches | lr 0.52 | ms/batch 72.24 | loss  3.81 | ppl    45.21\n",
            "| epoch  45 |   800/ 1320 batches | lr 0.52 | ms/batch 71.57 | loss  3.79 | ppl    44.47\n",
            "| epoch  45 |  1000/ 1320 batches | lr 0.52 | ms/batch 72.64 | loss  3.85 | ppl    46.86\n",
            "| epoch  45 |  1200/ 1320 batches | lr 0.52 | ms/batch 71.78 | loss  3.75 | ppl    42.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 99.01s | valid loss  5.29 | valid ppl   199.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  46 |   200/ 1320 batches | lr 0.50 | ms/batch 72.98 | loss  3.85 | ppl    47.07\n",
            "| epoch  46 |   400/ 1320 batches | lr 0.50 | ms/batch 70.97 | loss  3.82 | ppl    45.68\n",
            "| epoch  46 |   600/ 1320 batches | lr 0.50 | ms/batch 74.51 | loss  3.81 | ppl    45.10\n",
            "| epoch  46 |   800/ 1320 batches | lr 0.50 | ms/batch 73.03 | loss  3.78 | ppl    44.02\n",
            "| epoch  46 |  1000/ 1320 batches | lr 0.50 | ms/batch 75.16 | loss  3.84 | ppl    46.71\n",
            "| epoch  46 |  1200/ 1320 batches | lr 0.50 | ms/batch 73.78 | loss  3.74 | ppl    42.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 100.73s | valid loss  5.30 | valid ppl   199.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  47 |   200/ 1320 batches | lr 0.47 | ms/batch 75.21 | loss  3.84 | ppl    46.69\n",
            "| epoch  47 |   400/ 1320 batches | lr 0.47 | ms/batch 73.79 | loss  3.81 | ppl    45.26\n",
            "| epoch  47 |   600/ 1320 batches | lr 0.47 | ms/batch 73.72 | loss  3.81 | ppl    44.93\n",
            "| epoch  47 |   800/ 1320 batches | lr 0.47 | ms/batch 74.26 | loss  3.78 | ppl    43.92\n",
            "| epoch  47 |  1000/ 1320 batches | lr 0.47 | ms/batch 74.56 | loss  3.84 | ppl    46.44\n",
            "| epoch  47 |  1200/ 1320 batches | lr 0.47 | ms/batch 73.49 | loss  3.73 | ppl    41.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 101.92s | valid loss  5.30 | valid ppl   200.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  48 |   200/ 1320 batches | lr 0.45 | ms/batch 74.76 | loss  3.83 | ppl    46.26\n",
            "| epoch  48 |   400/ 1320 batches | lr 0.45 | ms/batch 75.33 | loss  3.81 | ppl    45.09\n",
            "| epoch  48 |   600/ 1320 batches | lr 0.45 | ms/batch 73.91 | loss  3.80 | ppl    44.75\n",
            "| epoch  48 |   800/ 1320 batches | lr 0.45 | ms/batch 73.94 | loss  3.77 | ppl    43.58\n",
            "| epoch  48 |  1000/ 1320 batches | lr 0.45 | ms/batch 74.47 | loss  3.83 | ppl    46.29\n",
            "| epoch  48 |  1200/ 1320 batches | lr 0.45 | ms/batch 74.17 | loss  3.73 | ppl    41.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 102.14s | valid loss  5.30 | valid ppl   200.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  49 |   200/ 1320 batches | lr 0.43 | ms/batch 75.22 | loss  3.83 | ppl    46.04\n",
            "| epoch  49 |   400/ 1320 batches | lr 0.43 | ms/batch 73.70 | loss  3.80 | ppl    44.70\n",
            "| epoch  49 |   600/ 1320 batches | lr 0.43 | ms/batch 73.57 | loss  3.79 | ppl    44.43\n",
            "| epoch  49 |   800/ 1320 batches | lr 0.43 | ms/batch 74.02 | loss  3.77 | ppl    43.46\n",
            "| epoch  49 |  1000/ 1320 batches | lr 0.43 | ms/batch 74.42 | loss  3.82 | ppl    45.83\n",
            "| epoch  49 |  1200/ 1320 batches | lr 0.43 | ms/batch 73.92 | loss  3.73 | ppl    41.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 101.66s | valid loss  5.30 | valid ppl   200.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  50 |   200/ 1320 batches | lr 0.40 | ms/batch 74.31 | loss  3.82 | ppl    45.71\n",
            "| epoch  50 |   400/ 1320 batches | lr 0.40 | ms/batch 73.92 | loss  3.80 | ppl    44.72\n",
            "| epoch  50 |   600/ 1320 batches | lr 0.40 | ms/batch 74.07 | loss  3.79 | ppl    44.27\n",
            "| epoch  50 |   800/ 1320 batches | lr 0.40 | ms/batch 75.02 | loss  3.77 | ppl    43.30\n",
            "| epoch  50 |  1000/ 1320 batches | lr 0.40 | ms/batch 73.98 | loss  3.82 | ppl    45.69\n",
            "| epoch  50 |  1200/ 1320 batches | lr 0.40 | ms/batch 73.93 | loss  3.72 | ppl    41.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 101.68s | valid loss  5.30 | valid ppl   200.27\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 50\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model)\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = copy.deepcopy(model)\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVRNeHzg8oJF",
        "outputId": "53089f84-016f-41e3-fba8-686765740a27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  5.11 | test ppl   164.99\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
        "      f'test ppl {test_ppl:8.2f}')\n",
        "print('=' * 89)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(best_model.state_dict(), 'C:/Users/Colum/Documents/CS4125Repo/cs4125_project/FYP/Models/Transformer_Model.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "9778a034fa636adf5723e513073b31ecce0d3523c0ed5b8bcdd8a89b84fb4c6c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
