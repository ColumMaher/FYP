{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfDEI_het0Pz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from typing import Tuple\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.utils.data import dataset\n",
        "import math\n",
        "import torchtext\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install torchdata"
      ],
      "metadata": {
        "id": "5LFUg_2g0_HU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import PennTreebank "
      ],
      "metadata": {
        "id": "vBAU7ge81C3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter = PennTreebank(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = torchtext.vocab.build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])"
      ],
      "metadata": {
        "id": "9MbhQHjn1fJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "  \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "  data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
      ],
      "metadata": {
        "id": "ZA9YwoHJ1h2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter, val_iter, test_iter = PennTreebank()\n",
        "train_data= data_process(train_iter)\n",
        "val_data= data_process(val_iter)\n",
        "test_data= data_process(test_iter)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "ePsxOaE21iil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device)"
      ],
      "metadata": {
        "id": "qlf2Huho1kzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ],
      "metadata": {
        "id": "_tnP8iT61nkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bptt = 35\n",
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target"
      ],
      "metadata": {
        "id": "ovQTLnrh1sDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size: int, embedding_dim: int, num_head: int, hidden_dim: int,\n",
        "                 num_layers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(embedding_dim, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(embedding_dim, num_head, hidden_dim, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.encoder = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.decoder = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Tensor, shape [seq_len, batch_size]\n",
        "            src_mask: Tensor, shape [seq_len, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape [seq_len, batch_size, vocab_size]\n",
        "        \"\"\"\n",
        "        src = self.encoder(src) * math.sqrt(self.embedding_dim)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
        "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
        "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
      ],
      "metadata": {
        "id": "TYlrKUU51sj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-math.log(10000.0) / embedding_dim))\n",
        "        pe = torch.zeros(max_len, 1, embedding_dim)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "B1P2dpJ_18PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)  # size of vocabulary\n",
        "embedding_dim = 200  # embedding dimension\n",
        "hidden_dim = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
        "num_layers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "num_head = 2  # number of heads in nn.MultiheadAttention\n",
        "dropout = 0.2\n",
        "model = TransformerModel(vocab_size, embedding_dim, num_head, hidden_dim, num_layers, dropout).to(device)"
      ],
      "metadata": {
        "id": "cmILnFf02AeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0  # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "def train(model: nn.Module) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "\n",
        "    num_batches = len(train_data) // bptt\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        seq_len = data.size(0)\n",
        "        if seq_len != bptt:  # only on last batch\n",
        "            src_mask = src_mask[:seq_len, :seq_len]\n",
        "        output = model(data, src_mask)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ],
      "metadata": {
        "id": "JPNci_2v3MhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(eval_data, i)\n",
        "            seq_len = data.size(0)\n",
        "            if seq_len != bptt:\n",
        "                src_mask = src_mask[:seq_len, :seq_len]\n",
        "            output = model(data, src_mask)\n",
        "            output_flat = output.view(-1, vocab_size)\n",
        "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(eval_data) - 1)"
      ],
      "metadata": {
        "id": "G5UtWFjB3XZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 50\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model)\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = copy.deepcopy(model)\n",
        "\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fN3e5RE53X-J",
        "outputId": "641d5ef2-aac7-42cb-bf2b-82c802dd79cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/ 1320 batches | lr 5.00 | ms/batch 23.53 | loss  7.23 | ppl  1386.68\n",
            "| epoch   1 |   400/ 1320 batches | lr 5.00 | ms/batch  7.13 | loss  6.10 | ppl   443.70\n",
            "| epoch   1 |   600/ 1320 batches | lr 5.00 | ms/batch  7.17 | loss  5.85 | ppl   346.08\n",
            "| epoch   1 |   800/ 1320 batches | lr 5.00 | ms/batch  7.01 | loss  5.66 | ppl   288.31\n",
            "| epoch   1 |  1000/ 1320 batches | lr 5.00 | ms/batch  7.27 | loss  5.59 | ppl   267.79\n",
            "| epoch   1 |  1200/ 1320 batches | lr 5.00 | ms/batch  7.04 | loss  5.48 | ppl   240.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 13.12s | valid loss  5.50 | valid ppl   243.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 1320 batches | lr 4.75 | ms/batch  7.14 | loss  5.42 | ppl   225.94\n",
            "| epoch   2 |   400/ 1320 batches | lr 4.75 | ms/batch  6.95 | loss  5.36 | ppl   212.15\n",
            "| epoch   2 |   600/ 1320 batches | lr 4.75 | ms/batch  7.09 | loss  5.32 | ppl   204.79\n",
            "| epoch   2 |   800/ 1320 batches | lr 4.75 | ms/batch  6.97 | loss  5.25 | ppl   191.17\n",
            "| epoch   2 |  1000/ 1320 batches | lr 4.75 | ms/batch  7.12 | loss  5.26 | ppl   192.22\n",
            "| epoch   2 |  1200/ 1320 batches | lr 4.75 | ms/batch  7.08 | loss  5.18 | ppl   177.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time:  9.73s | valid loss  5.33 | valid ppl   205.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 1320 batches | lr 4.51 | ms/batch  7.03 | loss  5.18 | ppl   177.02\n",
            "| epoch   3 |   400/ 1320 batches | lr 4.51 | ms/batch  6.92 | loss  5.14 | ppl   170.57\n",
            "| epoch   3 |   600/ 1320 batches | lr 4.51 | ms/batch  7.07 | loss  5.11 | ppl   166.44\n",
            "| epoch   3 |   800/ 1320 batches | lr 4.51 | ms/batch  7.07 | loss  5.07 | ppl   158.91\n",
            "| epoch   3 |  1000/ 1320 batches | lr 4.51 | ms/batch  7.03 | loss  5.10 | ppl   163.27\n",
            "| epoch   3 |  1200/ 1320 batches | lr 4.51 | ms/batch  6.98 | loss  5.01 | ppl   150.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time:  9.75s | valid loss  5.27 | valid ppl   193.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/ 1320 batches | lr 4.29 | ms/batch  7.02 | loss  5.03 | ppl   152.53\n",
            "| epoch   4 |   400/ 1320 batches | lr 4.29 | ms/batch  7.11 | loss  5.00 | ppl   149.05\n",
            "| epoch   4 |   600/ 1320 batches | lr 4.29 | ms/batch  7.17 | loss  4.99 | ppl   146.22\n",
            "| epoch   4 |   800/ 1320 batches | lr 4.29 | ms/batch  7.10 | loss  4.94 | ppl   139.66\n",
            "| epoch   4 |  1000/ 1320 batches | lr 4.29 | ms/batch  7.18 | loss  4.97 | ppl   144.58\n",
            "| epoch   4 |  1200/ 1320 batches | lr 4.29 | ms/batch  7.21 | loss  4.89 | ppl   133.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time:  9.86s | valid loss  5.21 | valid ppl   182.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/ 1320 batches | lr 4.07 | ms/batch  7.26 | loss  4.92 | ppl   137.02\n",
            "| epoch   5 |   400/ 1320 batches | lr 4.07 | ms/batch  7.08 | loss  4.90 | ppl   133.92\n",
            "| epoch   5 |   600/ 1320 batches | lr 4.07 | ms/batch  7.04 | loss  4.89 | ppl   132.54\n",
            "| epoch   5 |   800/ 1320 batches | lr 4.07 | ms/batch  7.12 | loss  4.84 | ppl   126.31\n",
            "| epoch   5 |  1000/ 1320 batches | lr 4.07 | ms/batch  8.56 | loss  4.88 | ppl   131.23\n",
            "| epoch   5 |  1200/ 1320 batches | lr 4.07 | ms/batch  7.59 | loss  4.79 | ppl   120.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 10.21s | valid loss  5.18 | valid ppl   177.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/ 1320 batches | lr 3.87 | ms/batch  7.25 | loss  4.83 | ppl   125.53\n",
            "| epoch   6 |   400/ 1320 batches | lr 3.87 | ms/batch  7.07 | loss  4.81 | ppl   123.02\n",
            "| epoch   6 |   600/ 1320 batches | lr 3.87 | ms/batch  7.06 | loss  4.79 | ppl   120.80\n",
            "| epoch   6 |   800/ 1320 batches | lr 3.87 | ms/batch  7.05 | loss  4.76 | ppl   116.80\n",
            "| epoch   6 |  1000/ 1320 batches | lr 3.87 | ms/batch  7.36 | loss  4.81 | ppl   122.19\n",
            "| epoch   6 |  1200/ 1320 batches | lr 3.87 | ms/batch  7.06 | loss  4.72 | ppl   111.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time:  9.84s | valid loss  5.16 | valid ppl   173.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/ 1320 batches | lr 3.68 | ms/batch  7.14 | loss  4.76 | ppl   117.18\n",
            "| epoch   7 |   400/ 1320 batches | lr 3.68 | ms/batch  7.34 | loss  4.74 | ppl   114.35\n",
            "| epoch   7 |   600/ 1320 batches | lr 3.68 | ms/batch  7.23 | loss  4.72 | ppl   112.57\n",
            "| epoch   7 |   800/ 1320 batches | lr 3.68 | ms/batch  7.10 | loss  4.69 | ppl   109.24\n",
            "| epoch   7 |  1000/ 1320 batches | lr 3.68 | ms/batch  7.25 | loss  4.74 | ppl   114.14\n",
            "| epoch   7 |  1200/ 1320 batches | lr 3.68 | ms/batch  7.16 | loss  4.65 | ppl   104.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time:  9.95s | valid loss  5.18 | valid ppl   176.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/ 1320 batches | lr 3.49 | ms/batch  7.21 | loss  4.70 | ppl   109.80\n",
            "| epoch   8 |   400/ 1320 batches | lr 3.49 | ms/batch  7.34 | loss  4.68 | ppl   107.28\n",
            "| epoch   8 |   600/ 1320 batches | lr 3.49 | ms/batch  7.34 | loss  4.65 | ppl   104.76\n",
            "| epoch   8 |   800/ 1320 batches | lr 3.49 | ms/batch  7.07 | loss  4.63 | ppl   102.21\n",
            "| epoch   8 |  1000/ 1320 batches | lr 3.49 | ms/batch  7.23 | loss  4.67 | ppl   106.81\n",
            "| epoch   8 |  1200/ 1320 batches | lr 3.49 | ms/batch  7.29 | loss  4.58 | ppl    97.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time:  9.98s | valid loss  5.17 | valid ppl   175.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/ 1320 batches | lr 3.32 | ms/batch  7.14 | loss  4.64 | ppl   104.02\n",
            "| epoch   9 |   400/ 1320 batches | lr 3.32 | ms/batch  7.11 | loss  4.61 | ppl   100.66\n",
            "| epoch   9 |   600/ 1320 batches | lr 3.32 | ms/batch  7.10 | loss  4.60 | ppl    99.04\n",
            "| epoch   9 |   800/ 1320 batches | lr 3.32 | ms/batch  7.31 | loss  4.57 | ppl    96.08\n",
            "| epoch   9 |  1000/ 1320 batches | lr 3.32 | ms/batch  7.11 | loss  4.62 | ppl   101.50\n",
            "| epoch   9 |  1200/ 1320 batches | lr 3.32 | ms/batch  7.32 | loss  4.52 | ppl    91.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time:  9.91s | valid loss  5.16 | valid ppl   174.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/ 1320 batches | lr 3.15 | ms/batch  7.10 | loss  4.59 | ppl    98.09\n",
            "| epoch  10 |   400/ 1320 batches | lr 3.15 | ms/batch  7.25 | loss  4.55 | ppl    94.71\n",
            "| epoch  10 |   600/ 1320 batches | lr 3.15 | ms/batch  7.16 | loss  4.54 | ppl    93.50\n",
            "| epoch  10 |   800/ 1320 batches | lr 3.15 | ms/batch  7.24 | loss  4.51 | ppl    91.23\n",
            "| epoch  10 |  1000/ 1320 batches | lr 3.15 | ms/batch  7.31 | loss  4.57 | ppl    96.39\n",
            "| epoch  10 |  1200/ 1320 batches | lr 3.15 | ms/batch  7.25 | loss  4.47 | ppl    87.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time:  9.95s | valid loss  5.18 | valid ppl   177.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   200/ 1320 batches | lr 2.99 | ms/batch  7.20 | loss  4.54 | ppl    93.47\n",
            "| epoch  11 |   400/ 1320 batches | lr 2.99 | ms/batch  7.13 | loss  4.50 | ppl    90.47\n",
            "| epoch  11 |   600/ 1320 batches | lr 2.99 | ms/batch  7.15 | loss  4.49 | ppl    88.89\n",
            "| epoch  11 |   800/ 1320 batches | lr 2.99 | ms/batch  7.17 | loss  4.46 | ppl    86.89\n",
            "| epoch  11 |  1000/ 1320 batches | lr 2.99 | ms/batch  7.24 | loss  4.52 | ppl    91.83\n",
            "| epoch  11 |  1200/ 1320 batches | lr 2.99 | ms/batch  7.32 | loss  4.42 | ppl    83.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time:  9.94s | valid loss  5.16 | valid ppl   174.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |   200/ 1320 batches | lr 2.84 | ms/batch  7.21 | loss  4.49 | ppl    88.92\n",
            "| epoch  12 |   400/ 1320 batches | lr 2.84 | ms/batch  7.19 | loss  4.46 | ppl    86.47\n",
            "| epoch  12 |   600/ 1320 batches | lr 2.84 | ms/batch  7.23 | loss  4.44 | ppl    84.74\n",
            "| epoch  12 |   800/ 1320 batches | lr 2.84 | ms/batch  7.18 | loss  4.42 | ppl    83.05\n",
            "| epoch  12 |  1000/ 1320 batches | lr 2.84 | ms/batch  7.26 | loss  4.47 | ppl    87.56\n",
            "| epoch  12 |  1200/ 1320 batches | lr 2.84 | ms/batch  7.18 | loss  4.38 | ppl    79.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time:  9.95s | valid loss  5.15 | valid ppl   173.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   200/ 1320 batches | lr 2.70 | ms/batch  7.22 | loss  4.45 | ppl    85.26\n",
            "| epoch  13 |   400/ 1320 batches | lr 2.70 | ms/batch  7.23 | loss  4.42 | ppl    82.95\n",
            "| epoch  13 |   600/ 1320 batches | lr 2.70 | ms/batch  7.18 | loss  4.40 | ppl    81.30\n",
            "| epoch  13 |   800/ 1320 batches | lr 2.70 | ms/batch  7.21 | loss  4.37 | ppl    79.38\n",
            "| epoch  13 |  1000/ 1320 batches | lr 2.70 | ms/batch  7.20 | loss  4.43 | ppl    84.22\n",
            "| epoch  13 |  1200/ 1320 batches | lr 2.70 | ms/batch  7.31 | loss  4.34 | ppl    76.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time:  9.99s | valid loss  5.20 | valid ppl   180.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   200/ 1320 batches | lr 2.57 | ms/batch  7.30 | loss  4.41 | ppl    82.06\n",
            "| epoch  14 |   400/ 1320 batches | lr 2.57 | ms/batch  7.20 | loss  4.38 | ppl    79.55\n",
            "| epoch  14 |   600/ 1320 batches | lr 2.57 | ms/batch  7.31 | loss  4.36 | ppl    78.09\n",
            "| epoch  14 |   800/ 1320 batches | lr 2.57 | ms/batch  7.27 | loss  4.34 | ppl    76.52\n",
            "| epoch  14 |  1000/ 1320 batches | lr 2.57 | ms/batch  7.28 | loss  4.39 | ppl    80.91\n",
            "| epoch  14 |  1200/ 1320 batches | lr 2.57 | ms/batch  7.32 | loss  4.30 | ppl    73.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 10.04s | valid loss  5.22 | valid ppl   185.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |   200/ 1320 batches | lr 2.44 | ms/batch  7.39 | loss  4.36 | ppl    78.60\n",
            "| epoch  15 |   400/ 1320 batches | lr 2.44 | ms/batch  7.21 | loss  4.34 | ppl    76.49\n",
            "| epoch  15 |   600/ 1320 batches | lr 2.44 | ms/batch  7.23 | loss  4.32 | ppl    75.34\n",
            "| epoch  15 |   800/ 1320 batches | lr 2.44 | ms/batch  7.18 | loss  4.30 | ppl    73.63\n",
            "| epoch  15 |  1000/ 1320 batches | lr 2.44 | ms/batch  7.24 | loss  4.36 | ppl    77.99\n",
            "| epoch  15 |  1200/ 1320 batches | lr 2.44 | ms/batch  7.25 | loss  4.26 | ppl    70.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time:  9.99s | valid loss  5.18 | valid ppl   177.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   200/ 1320 batches | lr 2.32 | ms/batch  7.37 | loss  4.33 | ppl    76.12\n",
            "| epoch  16 |   400/ 1320 batches | lr 2.32 | ms/batch  7.21 | loss  4.30 | ppl    73.98\n",
            "| epoch  16 |   600/ 1320 batches | lr 2.32 | ms/batch  7.24 | loss  4.28 | ppl    72.49\n",
            "| epoch  16 |   800/ 1320 batches | lr 2.32 | ms/batch  7.22 | loss  4.26 | ppl    71.07\n",
            "| epoch  16 |  1000/ 1320 batches | lr 2.32 | ms/batch  7.24 | loss  4.32 | ppl    75.43\n",
            "| epoch  16 |  1200/ 1320 batches | lr 2.32 | ms/batch  7.35 | loss  4.22 | ppl    68.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 10.01s | valid loss  5.20 | valid ppl   181.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   200/ 1320 batches | lr 2.20 | ms/batch  7.31 | loss  4.30 | ppl    73.81\n",
            "| epoch  17 |   400/ 1320 batches | lr 2.20 | ms/batch  7.19 | loss  4.27 | ppl    71.43\n",
            "| epoch  17 |   600/ 1320 batches | lr 2.20 | ms/batch  7.25 | loss  4.25 | ppl    70.35\n",
            "| epoch  17 |   800/ 1320 batches | lr 2.20 | ms/batch  7.31 | loss  4.23 | ppl    68.93\n",
            "| epoch  17 |  1000/ 1320 batches | lr 2.20 | ms/batch  7.29 | loss  4.29 | ppl    72.76\n",
            "| epoch  17 |  1200/ 1320 batches | lr 2.20 | ms/batch  7.33 | loss  4.19 | ppl    65.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 10.04s | valid loss  5.22 | valid ppl   184.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   200/ 1320 batches | lr 2.09 | ms/batch  7.24 | loss  4.27 | ppl    71.23\n",
            "| epoch  18 |   400/ 1320 batches | lr 2.09 | ms/batch  7.29 | loss  4.24 | ppl    69.10\n",
            "| epoch  18 |   600/ 1320 batches | lr 2.09 | ms/batch  7.25 | loss  4.22 | ppl    68.17\n",
            "| epoch  18 |   800/ 1320 batches | lr 2.09 | ms/batch  7.23 | loss  4.20 | ppl    66.86\n",
            "| epoch  18 |  1000/ 1320 batches | lr 2.09 | ms/batch  7.41 | loss  4.26 | ppl    70.59\n",
            "| epoch  18 |  1200/ 1320 batches | lr 2.09 | ms/batch  7.35 | loss  4.16 | ppl    64.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 10.42s | valid loss  5.21 | valid ppl   183.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |   200/ 1320 batches | lr 1.99 | ms/batch  9.99 | loss  4.24 | ppl    69.51\n",
            "| epoch  19 |   400/ 1320 batches | lr 1.99 | ms/batch  7.73 | loss  4.21 | ppl    67.18\n",
            "| epoch  19 |   600/ 1320 batches | lr 1.99 | ms/batch  7.28 | loss  4.19 | ppl    66.13\n",
            "| epoch  19 |   800/ 1320 batches | lr 1.99 | ms/batch  7.24 | loss  4.18 | ppl    65.06\n",
            "| epoch  19 |  1000/ 1320 batches | lr 1.99 | ms/batch  7.34 | loss  4.23 | ppl    68.47\n",
            "| epoch  19 |  1200/ 1320 batches | lr 1.99 | ms/batch  7.35 | loss  4.13 | ppl    62.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 10.70s | valid loss  5.23 | valid ppl   186.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |   200/ 1320 batches | lr 1.89 | ms/batch  7.32 | loss  4.21 | ppl    67.69\n",
            "| epoch  20 |   400/ 1320 batches | lr 1.89 | ms/batch  7.35 | loss  4.18 | ppl    65.43\n",
            "| epoch  20 |   600/ 1320 batches | lr 1.89 | ms/batch  7.20 | loss  4.17 | ppl    64.72\n",
            "| epoch  20 |   800/ 1320 batches | lr 1.89 | ms/batch  7.29 | loss  4.15 | ppl    63.21\n",
            "| epoch  20 |  1000/ 1320 batches | lr 1.89 | ms/batch  7.23 | loss  4.20 | ppl    66.80\n",
            "| epoch  20 |  1200/ 1320 batches | lr 1.89 | ms/batch  7.28 | loss  4.10 | ppl    60.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 10.02s | valid loss  5.25 | valid ppl   190.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |   200/ 1320 batches | lr 1.79 | ms/batch  7.36 | loss  4.19 | ppl    66.00\n",
            "| epoch  21 |   400/ 1320 batches | lr 1.79 | ms/batch  7.36 | loss  4.15 | ppl    63.70\n",
            "| epoch  21 |   600/ 1320 batches | lr 1.79 | ms/batch  7.32 | loss  4.14 | ppl    62.91\n",
            "| epoch  21 |   800/ 1320 batches | lr 1.79 | ms/batch  7.30 | loss  4.12 | ppl    61.85\n",
            "| epoch  21 |  1000/ 1320 batches | lr 1.79 | ms/batch  7.28 | loss  4.18 | ppl    65.44\n",
            "| epoch  21 |  1200/ 1320 batches | lr 1.79 | ms/batch  7.33 | loss  4.08 | ppl    59.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 10.10s | valid loss  5.22 | valid ppl   184.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |   200/ 1320 batches | lr 1.70 | ms/batch  7.34 | loss  4.16 | ppl    64.20\n",
            "| epoch  22 |   400/ 1320 batches | lr 1.70 | ms/batch  7.37 | loss  4.13 | ppl    62.21\n",
            "| epoch  22 |   600/ 1320 batches | lr 1.70 | ms/batch  7.25 | loss  4.11 | ppl    60.96\n",
            "| epoch  22 |   800/ 1320 batches | lr 1.70 | ms/batch  7.32 | loss  4.10 | ppl    60.07\n",
            "| epoch  22 |  1000/ 1320 batches | lr 1.70 | ms/batch  7.26 | loss  4.16 | ppl    63.86\n",
            "| epoch  22 |  1200/ 1320 batches | lr 1.70 | ms/batch  7.42 | loss  4.05 | ppl    57.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 10.08s | valid loss  5.23 | valid ppl   187.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |   200/ 1320 batches | lr 1.62 | ms/batch  7.36 | loss  4.15 | ppl    63.14\n",
            "| epoch  23 |   400/ 1320 batches | lr 1.62 | ms/batch  7.37 | loss  4.11 | ppl    60.80\n",
            "| epoch  23 |   600/ 1320 batches | lr 1.62 | ms/batch  7.30 | loss  4.09 | ppl    60.01\n",
            "| epoch  23 |   800/ 1320 batches | lr 1.62 | ms/batch  7.33 | loss  4.07 | ppl    58.75\n",
            "| epoch  23 |  1000/ 1320 batches | lr 1.62 | ms/batch  7.27 | loss  4.13 | ppl    62.37\n",
            "| epoch  23 |  1200/ 1320 batches | lr 1.62 | ms/batch  7.41 | loss  4.03 | ppl    56.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 10.11s | valid loss  5.23 | valid ppl   186.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |   200/ 1320 batches | lr 1.54 | ms/batch  7.32 | loss  4.12 | ppl    61.56\n",
            "| epoch  24 |   400/ 1320 batches | lr 1.54 | ms/batch  7.31 | loss  4.09 | ppl    59.73\n",
            "| epoch  24 |   600/ 1320 batches | lr 1.54 | ms/batch  7.33 | loss  4.07 | ppl    58.65\n",
            "| epoch  24 |   800/ 1320 batches | lr 1.54 | ms/batch  7.30 | loss  4.05 | ppl    57.58\n",
            "| epoch  24 |  1000/ 1320 batches | lr 1.54 | ms/batch  7.32 | loss  4.11 | ppl    60.77\n",
            "| epoch  24 |  1200/ 1320 batches | lr 1.54 | ms/batch  7.26 | loss  4.01 | ppl    55.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 10.09s | valid loss  5.24 | valid ppl   188.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |   200/ 1320 batches | lr 1.46 | ms/batch  7.37 | loss  4.10 | ppl    60.60\n",
            "| epoch  25 |   400/ 1320 batches | lr 1.46 | ms/batch  7.36 | loss  4.07 | ppl    58.54\n",
            "| epoch  25 |   600/ 1320 batches | lr 1.46 | ms/batch  7.33 | loss  4.05 | ppl    57.48\n",
            "| epoch  25 |   800/ 1320 batches | lr 1.46 | ms/batch  7.29 | loss  4.03 | ppl    56.47\n",
            "| epoch  25 |  1000/ 1320 batches | lr 1.46 | ms/batch  7.43 | loss  4.09 | ppl    59.69\n",
            "| epoch  25 |  1200/ 1320 batches | lr 1.46 | ms/batch  7.37 | loss  3.99 | ppl    53.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 10.15s | valid loss  5.26 | valid ppl   192.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |   200/ 1320 batches | lr 1.39 | ms/batch  7.34 | loss  4.08 | ppl    59.25\n",
            "| epoch  26 |   400/ 1320 batches | lr 1.39 | ms/batch  7.37 | loss  4.05 | ppl    57.15\n",
            "| epoch  26 |   600/ 1320 batches | lr 1.39 | ms/batch  7.42 | loss  4.04 | ppl    56.59\n",
            "| epoch  26 |   800/ 1320 batches | lr 1.39 | ms/batch  7.46 | loss  4.01 | ppl    55.42\n",
            "| epoch  26 |  1000/ 1320 batches | lr 1.39 | ms/batch  7.41 | loss  4.07 | ppl    58.51\n",
            "| epoch  26 |  1200/ 1320 batches | lr 1.39 | ms/batch  7.32 | loss  3.97 | ppl    52.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 10.20s | valid loss  5.25 | valid ppl   190.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |   200/ 1320 batches | lr 1.32 | ms/batch  7.35 | loss  4.06 | ppl    58.00\n",
            "| epoch  27 |   400/ 1320 batches | lr 1.32 | ms/batch  7.34 | loss  4.03 | ppl    55.99\n",
            "| epoch  27 |   600/ 1320 batches | lr 1.32 | ms/batch  7.37 | loss  4.02 | ppl    55.55\n",
            "| epoch  27 |   800/ 1320 batches | lr 1.32 | ms/batch  7.32 | loss  4.00 | ppl    54.38\n",
            "| epoch  27 |  1000/ 1320 batches | lr 1.32 | ms/batch  7.33 | loss  4.05 | ppl    57.68\n",
            "| epoch  27 |  1200/ 1320 batches | lr 1.32 | ms/batch  7.33 | loss  3.95 | ppl    51.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 10.13s | valid loss  5.28 | valid ppl   196.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |   200/ 1320 batches | lr 1.25 | ms/batch  7.45 | loss  4.05 | ppl    57.17\n",
            "| epoch  28 |   400/ 1320 batches | lr 1.25 | ms/batch  7.41 | loss  4.01 | ppl    55.33\n",
            "| epoch  28 |   600/ 1320 batches | lr 1.25 | ms/batch  7.41 | loss  4.00 | ppl    54.54\n",
            "| epoch  28 |   800/ 1320 batches | lr 1.25 | ms/batch  7.37 | loss  3.98 | ppl    53.33\n",
            "| epoch  28 |  1000/ 1320 batches | lr 1.25 | ms/batch  7.34 | loss  4.04 | ppl    56.77\n",
            "| epoch  28 |  1200/ 1320 batches | lr 1.25 | ms/batch  7.44 | loss  3.93 | ppl    50.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 10.20s | valid loss  5.26 | valid ppl   191.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |   200/ 1320 batches | lr 1.19 | ms/batch  7.43 | loss  4.03 | ppl    56.05\n",
            "| epoch  29 |   400/ 1320 batches | lr 1.19 | ms/batch  7.37 | loss  4.00 | ppl    54.66\n",
            "| epoch  29 |   600/ 1320 batches | lr 1.19 | ms/batch  7.49 | loss  3.98 | ppl    53.61\n",
            "| epoch  29 |   800/ 1320 batches | lr 1.19 | ms/batch  7.33 | loss  3.96 | ppl    52.50\n",
            "| epoch  29 |  1000/ 1320 batches | lr 1.19 | ms/batch  7.44 | loss  4.02 | ppl    55.73\n",
            "| epoch  29 |  1200/ 1320 batches | lr 1.19 | ms/batch  7.35 | loss  3.92 | ppl    50.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 10.22s | valid loss  5.26 | valid ppl   192.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |   200/ 1320 batches | lr 1.13 | ms/batch  7.41 | loss  4.02 | ppl    55.76\n",
            "| epoch  30 |   400/ 1320 batches | lr 1.13 | ms/batch  7.38 | loss  3.98 | ppl    53.67\n",
            "| epoch  30 |   600/ 1320 batches | lr 1.13 | ms/batch  7.47 | loss  3.97 | ppl    52.99\n",
            "| epoch  30 |   800/ 1320 batches | lr 1.13 | ms/batch  7.49 | loss  3.95 | ppl    51.83\n",
            "| epoch  30 |  1000/ 1320 batches | lr 1.13 | ms/batch  7.43 | loss  4.01 | ppl    54.99\n",
            "| epoch  30 |  1200/ 1320 batches | lr 1.13 | ms/batch  7.37 | loss  3.91 | ppl    49.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 10.24s | valid loss  5.27 | valid ppl   194.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  31 |   200/ 1320 batches | lr 1.07 | ms/batch  7.55 | loss  4.00 | ppl    54.72\n",
            "| epoch  31 |   400/ 1320 batches | lr 1.07 | ms/batch  7.39 | loss  3.97 | ppl    52.95\n",
            "| epoch  31 |   600/ 1320 batches | lr 1.07 | ms/batch  7.52 | loss  3.95 | ppl    52.08\n",
            "| epoch  31 |   800/ 1320 batches | lr 1.07 | ms/batch  7.40 | loss  3.93 | ppl    51.12\n",
            "| epoch  31 |  1000/ 1320 batches | lr 1.07 | ms/batch  7.46 | loss  3.99 | ppl    54.13\n",
            "| epoch  31 |  1200/ 1320 batches | lr 1.07 | ms/batch  7.45 | loss  3.89 | ppl    48.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 10.29s | valid loss  5.27 | valid ppl   193.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |   200/ 1320 batches | lr 1.02 | ms/batch  7.57 | loss  3.99 | ppl    53.94\n",
            "| epoch  32 |   400/ 1320 batches | lr 1.02 | ms/batch  7.47 | loss  3.96 | ppl    52.39\n",
            "| epoch  32 |   600/ 1320 batches | lr 1.02 | ms/batch  7.39 | loss  3.94 | ppl    51.55\n",
            "| epoch  32 |   800/ 1320 batches | lr 1.02 | ms/batch  7.42 | loss  3.92 | ppl    50.22\n",
            "| epoch  32 |  1000/ 1320 batches | lr 1.02 | ms/batch  7.40 | loss  3.98 | ppl    53.34\n",
            "| epoch  32 |  1200/ 1320 batches | lr 1.02 | ms/batch  7.39 | loss  3.88 | ppl    48.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 10.26s | valid loss  5.28 | valid ppl   195.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |   200/ 1320 batches | lr 0.97 | ms/batch  7.36 | loss  3.98 | ppl    53.26\n",
            "| epoch  33 |   400/ 1320 batches | lr 0.97 | ms/batch  7.39 | loss  3.94 | ppl    51.51\n",
            "| epoch  33 |   600/ 1320 batches | lr 0.97 | ms/batch  7.38 | loss  3.93 | ppl    50.88\n",
            "| epoch  33 |   800/ 1320 batches | lr 0.97 | ms/batch  7.41 | loss  3.91 | ppl    49.72\n",
            "| epoch  33 |  1000/ 1320 batches | lr 0.97 | ms/batch  7.46 | loss  3.97 | ppl    52.85\n",
            "| epoch  33 |  1200/ 1320 batches | lr 0.97 | ms/batch  7.36 | loss  3.87 | ppl    47.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 10.20s | valid loss  5.27 | valid ppl   195.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |   200/ 1320 batches | lr 0.92 | ms/batch  7.48 | loss  3.96 | ppl    52.61\n",
            "| epoch  34 |   400/ 1320 batches | lr 0.92 | ms/batch  7.41 | loss  3.93 | ppl    51.00\n",
            "| epoch  34 |   600/ 1320 batches | lr 0.92 | ms/batch  7.41 | loss  3.91 | ppl    50.08\n",
            "| epoch  34 |   800/ 1320 batches | lr 0.92 | ms/batch  7.37 | loss  3.90 | ppl    49.27\n",
            "| epoch  34 |  1000/ 1320 batches | lr 0.92 | ms/batch  7.46 | loss  3.95 | ppl    52.10\n",
            "| epoch  34 |  1200/ 1320 batches | lr 0.92 | ms/batch  7.49 | loss  3.85 | ppl    47.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 10.26s | valid loss  5.28 | valid ppl   196.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |   200/ 1320 batches | lr 0.87 | ms/batch  7.39 | loss  3.95 | ppl    52.07\n",
            "| epoch  35 |   400/ 1320 batches | lr 0.87 | ms/batch  7.40 | loss  3.92 | ppl    50.63\n",
            "| epoch  35 |   600/ 1320 batches | lr 0.87 | ms/batch  7.35 | loss  3.91 | ppl    49.68\n",
            "| epoch  35 |   800/ 1320 batches | lr 0.87 | ms/batch  7.41 | loss  3.89 | ppl    48.67\n",
            "| epoch  35 |  1000/ 1320 batches | lr 0.87 | ms/batch  7.41 | loss  3.94 | ppl    51.67\n",
            "| epoch  35 |  1200/ 1320 batches | lr 0.87 | ms/batch  7.37 | loss  3.84 | ppl    46.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 10.19s | valid loss  5.27 | valid ppl   193.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |   200/ 1320 batches | lr 0.83 | ms/batch  7.43 | loss  3.94 | ppl    51.55\n",
            "| epoch  36 |   400/ 1320 batches | lr 0.83 | ms/batch  7.39 | loss  3.91 | ppl    49.99\n",
            "| epoch  36 |   600/ 1320 batches | lr 0.83 | ms/batch  7.54 | loss  3.90 | ppl    49.23\n",
            "| epoch  36 |   800/ 1320 batches | lr 0.83 | ms/batch  7.38 | loss  3.87 | ppl    47.98\n",
            "| epoch  36 |  1000/ 1320 batches | lr 0.83 | ms/batch  7.37 | loss  3.93 | ppl    51.12\n",
            "| epoch  36 |  1200/ 1320 batches | lr 0.83 | ms/batch  7.46 | loss  3.83 | ppl    46.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 10.24s | valid loss  5.29 | valid ppl   198.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  37 |   200/ 1320 batches | lr 0.79 | ms/batch  7.44 | loss  3.94 | ppl    51.26\n",
            "| epoch  37 |   400/ 1320 batches | lr 0.79 | ms/batch  7.38 | loss  3.90 | ppl    49.36\n",
            "| epoch  37 |   600/ 1320 batches | lr 0.79 | ms/batch  7.47 | loss  3.88 | ppl    48.66\n",
            "| epoch  37 |   800/ 1320 batches | lr 0.79 | ms/batch  7.39 | loss  3.86 | ppl    47.59\n",
            "| epoch  37 |  1000/ 1320 batches | lr 0.79 | ms/batch  7.52 | loss  3.93 | ppl    50.70\n",
            "| epoch  37 |  1200/ 1320 batches | lr 0.79 | ms/batch  7.40 | loss  3.82 | ppl    45.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 10.24s | valid loss  5.29 | valid ppl   197.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |   200/ 1320 batches | lr 0.75 | ms/batch  7.47 | loss  3.92 | ppl    50.54\n",
            "| epoch  38 |   400/ 1320 batches | lr 0.75 | ms/batch  7.35 | loss  3.89 | ppl    49.00\n",
            "| epoch  38 |   600/ 1320 batches | lr 0.75 | ms/batch  7.39 | loss  3.87 | ppl    48.13\n",
            "| epoch  38 |   800/ 1320 batches | lr 0.75 | ms/batch  7.39 | loss  3.85 | ppl    46.95\n",
            "| epoch  38 |  1000/ 1320 batches | lr 0.75 | ms/batch  7.37 | loss  3.91 | ppl    49.92\n",
            "| epoch  38 |  1200/ 1320 batches | lr 0.75 | ms/batch  7.35 | loss  3.81 | ppl    45.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 10.17s | valid loss  5.30 | valid ppl   199.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |   200/ 1320 batches | lr 0.71 | ms/batch  7.44 | loss  3.91 | ppl    50.10\n",
            "| epoch  39 |   400/ 1320 batches | lr 0.71 | ms/batch  7.38 | loss  3.88 | ppl    48.48\n",
            "| epoch  39 |   600/ 1320 batches | lr 0.71 | ms/batch  7.37 | loss  3.87 | ppl    47.98\n",
            "| epoch  39 |   800/ 1320 batches | lr 0.71 | ms/batch  7.34 | loss  3.84 | ppl    46.67\n",
            "| epoch  39 |  1000/ 1320 batches | lr 0.71 | ms/batch  7.39 | loss  3.91 | ppl    49.67\n",
            "| epoch  39 |  1200/ 1320 batches | lr 0.71 | ms/batch  7.33 | loss  3.81 | ppl    44.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 10.16s | valid loss  5.30 | valid ppl   199.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |   200/ 1320 batches | lr 0.68 | ms/batch  7.51 | loss  3.90 | ppl    49.53\n",
            "| epoch  40 |   400/ 1320 batches | lr 0.68 | ms/batch  7.45 | loss  3.87 | ppl    48.06\n",
            "| epoch  40 |   600/ 1320 batches | lr 0.68 | ms/batch  7.36 | loss  3.86 | ppl    47.60\n",
            "| epoch  40 |   800/ 1320 batches | lr 0.68 | ms/batch  7.34 | loss  3.83 | ppl    46.29\n",
            "| epoch  40 |  1000/ 1320 batches | lr 0.68 | ms/batch  7.40 | loss  3.90 | ppl    49.26\n",
            "| epoch  40 |  1200/ 1320 batches | lr 0.68 | ms/batch  7.46 | loss  3.80 | ppl    44.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 10.22s | valid loss  5.29 | valid ppl   197.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  41 |   200/ 1320 batches | lr 0.64 | ms/batch  7.44 | loss  3.90 | ppl    49.17\n",
            "| epoch  41 |   400/ 1320 batches | lr 0.64 | ms/batch  7.39 | loss  3.87 | ppl    47.77\n",
            "| epoch  41 |   600/ 1320 batches | lr 0.64 | ms/batch  7.46 | loss  3.85 | ppl    46.95\n",
            "| epoch  41 |   800/ 1320 batches | lr 0.64 | ms/batch  7.38 | loss  3.82 | ppl    45.72\n",
            "| epoch  41 |  1000/ 1320 batches | lr 0.64 | ms/batch  7.34 | loss  3.89 | ppl    48.81\n",
            "| epoch  41 |  1200/ 1320 batches | lr 0.64 | ms/batch  7.34 | loss  3.79 | ppl    44.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 10.18s | valid loss  5.29 | valid ppl   197.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  42 |   200/ 1320 batches | lr 0.61 | ms/batch  7.44 | loss  3.89 | ppl    48.86\n",
            "| epoch  42 |   400/ 1320 batches | lr 0.61 | ms/batch  7.36 | loss  3.86 | ppl    47.41\n",
            "| epoch  42 |   600/ 1320 batches | lr 0.61 | ms/batch  7.36 | loss  3.84 | ppl    46.59\n",
            "| epoch  42 |   800/ 1320 batches | lr 0.61 | ms/batch  7.31 | loss  3.82 | ppl    45.72\n",
            "| epoch  42 |  1000/ 1320 batches | lr 0.61 | ms/batch  7.39 | loss  3.88 | ppl    48.55\n",
            "| epoch  42 |  1200/ 1320 batches | lr 0.61 | ms/batch  7.36 | loss  3.78 | ppl    43.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 10.18s | valid loss  5.29 | valid ppl   197.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  43 |   200/ 1320 batches | lr 0.58 | ms/batch  7.47 | loss  3.88 | ppl    48.31\n",
            "| epoch  43 |   400/ 1320 batches | lr 0.58 | ms/batch  7.45 | loss  3.85 | ppl    47.17\n",
            "| epoch  43 |   600/ 1320 batches | lr 0.58 | ms/batch  7.50 | loss  3.84 | ppl    46.44\n",
            "| epoch  43 |   800/ 1320 batches | lr 0.58 | ms/batch  7.40 | loss  3.81 | ppl    45.14\n",
            "| epoch  43 |  1000/ 1320 batches | lr 0.58 | ms/batch  7.40 | loss  3.87 | ppl    48.12\n",
            "| epoch  43 |  1200/ 1320 batches | lr 0.58 | ms/batch  7.52 | loss  3.78 | ppl    43.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 10.28s | valid loss  5.29 | valid ppl   199.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  44 |   200/ 1320 batches | lr 0.55 | ms/batch  7.44 | loss  3.88 | ppl    48.28\n",
            "| epoch  44 |   400/ 1320 batches | lr 0.55 | ms/batch  7.41 | loss  3.85 | ppl    46.96\n",
            "| epoch  44 |   600/ 1320 batches | lr 0.55 | ms/batch  7.36 | loss  3.83 | ppl    46.20\n",
            "| epoch  44 |   800/ 1320 batches | lr 0.55 | ms/batch  7.37 | loss  3.80 | ppl    44.84\n",
            "| epoch  44 |  1000/ 1320 batches | lr 0.55 | ms/batch  7.34 | loss  3.87 | ppl    47.73\n",
            "| epoch  44 |  1200/ 1320 batches | lr 0.55 | ms/batch  7.36 | loss  3.76 | ppl    43.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 10.18s | valid loss  5.30 | valid ppl   200.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  45 |   200/ 1320 batches | lr 0.52 | ms/batch  7.44 | loss  3.87 | ppl    47.84\n",
            "| epoch  45 |   400/ 1320 batches | lr 0.52 | ms/batch  7.39 | loss  3.84 | ppl    46.63\n",
            "| epoch  45 |   600/ 1320 batches | lr 0.52 | ms/batch  7.36 | loss  3.82 | ppl    45.67\n",
            "| epoch  45 |   800/ 1320 batches | lr 0.52 | ms/batch  7.35 | loss  3.80 | ppl    44.62\n",
            "| epoch  45 |  1000/ 1320 batches | lr 0.52 | ms/batch  7.42 | loss  3.87 | ppl    47.71\n",
            "| epoch  45 |  1200/ 1320 batches | lr 0.52 | ms/batch  7.43 | loss  3.76 | ppl    42.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 10.19s | valid loss  5.30 | valid ppl   200.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  46 |   200/ 1320 batches | lr 0.50 | ms/batch  7.44 | loss  3.86 | ppl    47.46\n",
            "| epoch  46 |   400/ 1320 batches | lr 0.50 | ms/batch  7.37 | loss  3.83 | ppl    46.11\n",
            "| epoch  46 |   600/ 1320 batches | lr 0.50 | ms/batch  7.36 | loss  3.82 | ppl    45.49\n",
            "| epoch  46 |   800/ 1320 batches | lr 0.50 | ms/batch  7.95 | loss  3.79 | ppl    44.46\n",
            "| epoch  46 |  1000/ 1320 batches | lr 0.50 | ms/batch  9.23 | loss  3.86 | ppl    47.45\n",
            "| epoch  46 |  1200/ 1320 batches | lr 0.50 | ms/batch  9.29 | loss  3.76 | ppl    43.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 11.05s | valid loss  5.30 | valid ppl   200.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  47 |   200/ 1320 batches | lr 0.47 | ms/batch  7.41 | loss  3.86 | ppl    47.24\n",
            "| epoch  47 |   400/ 1320 batches | lr 0.47 | ms/batch  7.41 | loss  3.83 | ppl    45.91\n",
            "| epoch  47 |   600/ 1320 batches | lr 0.47 | ms/batch  7.39 | loss  3.82 | ppl    45.40\n",
            "| epoch  47 |   800/ 1320 batches | lr 0.47 | ms/batch  7.37 | loss  3.78 | ppl    43.98\n",
            "| epoch  47 |  1000/ 1320 batches | lr 0.47 | ms/batch  7.37 | loss  3.85 | ppl    47.05\n",
            "| epoch  47 |  1200/ 1320 batches | lr 0.47 | ms/batch  7.32 | loss  3.75 | ppl    42.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 10.16s | valid loss  5.30 | valid ppl   201.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  48 |   200/ 1320 batches | lr 0.45 | ms/batch  7.50 | loss  3.85 | ppl    47.03\n",
            "| epoch  48 |   400/ 1320 batches | lr 0.45 | ms/batch  7.32 | loss  3.82 | ppl    45.78\n",
            "| epoch  48 |   600/ 1320 batches | lr 0.45 | ms/batch  7.41 | loss  3.80 | ppl    44.85\n",
            "| epoch  48 |   800/ 1320 batches | lr 0.45 | ms/batch  7.35 | loss  3.78 | ppl    43.94\n",
            "| epoch  48 |  1000/ 1320 batches | lr 0.45 | ms/batch  7.41 | loss  3.84 | ppl    46.74\n",
            "| epoch  48 |  1200/ 1320 batches | lr 0.45 | ms/batch  7.41 | loss  3.74 | ppl    42.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 10.21s | valid loss  5.30 | valid ppl   200.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  49 |   200/ 1320 batches | lr 0.43 | ms/batch  7.46 | loss  3.85 | ppl    46.90\n",
            "| epoch  49 |   400/ 1320 batches | lr 0.43 | ms/batch  7.41 | loss  3.82 | ppl    45.49\n",
            "| epoch  49 |   600/ 1320 batches | lr 0.43 | ms/batch  7.41 | loss  3.80 | ppl    44.84\n",
            "| epoch  49 |   800/ 1320 batches | lr 0.43 | ms/batch  7.39 | loss  3.77 | ppl    43.59\n",
            "| epoch  49 |  1000/ 1320 batches | lr 0.43 | ms/batch  7.49 | loss  3.84 | ppl    46.49\n",
            "| epoch  49 |  1200/ 1320 batches | lr 0.43 | ms/batch  7.39 | loss  3.74 | ppl    42.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 10.23s | valid loss  5.31 | valid ppl   203.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  50 |   200/ 1320 batches | lr 0.40 | ms/batch  7.38 | loss  3.84 | ppl    46.64\n",
            "| epoch  50 |   400/ 1320 batches | lr 0.40 | ms/batch  7.42 | loss  3.81 | ppl    45.18\n",
            "| epoch  50 |   600/ 1320 batches | lr 0.40 | ms/batch  7.35 | loss  3.80 | ppl    44.56\n",
            "| epoch  50 |   800/ 1320 batches | lr 0.40 | ms/batch  7.37 | loss  3.77 | ppl    43.50\n",
            "| epoch  50 |  1000/ 1320 batches | lr 0.40 | ms/batch  7.30 | loss  3.83 | ppl    46.26\n",
            "| epoch  50 |  1200/ 1320 batches | lr 0.40 | ms/batch  7.34 | loss  3.74 | ppl    42.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 10.14s | valid loss  5.32 | valid ppl   203.99\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
        "      f'test ppl {test_ppl:8.2f}')\n",
        "print('=' * 89)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVRNeHzg8oJF",
        "outputId": "53089f84-016f-41e3-fba8-686765740a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  5.09 | test ppl   161.90\n",
            "=========================================================================================\n"
          ]
        }
      ]
    }
  ]
}