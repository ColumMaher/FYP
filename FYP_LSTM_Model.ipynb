{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from typing import Tuple\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.utils.data import dataset\n",
        "import math\n",
        "import torchtext\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "ZxVrhiF0Mh00"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install torchdata"
      ],
      "metadata": {
        "id": "pD7cbU9dwHYS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import PennTreebank "
      ],
      "metadata": {
        "id": "YOX72cCBS71c"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter = PennTreebank(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = torchtext.vocab.build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])"
      ],
      "metadata": {
        "id": "0FioMPfTdBJT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "  \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "  data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
      ],
      "metadata": {
        "id": "S5Ae9CdAyZQH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter, val_iter, test_iter = PennTreebank()\n",
        "train_data= data_process(train_iter)\n",
        "val_data= data_process(val_iter)\n",
        "test_data= data_process(test_iter)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "R5cZes9WiHFJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device)"
      ],
      "metadata": {
        "id": "VVTtf0cPycRW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ],
      "metadata": {
        "id": "Vz5mzO8wycub"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0-OPgEJ0lgm",
        "outputId": "00b09869-e726-4b1e-83e4-84951696f08e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([9892,  871,    2,   17,   12,    3,  318,   29, 2356,   34],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bptt = 80\n",
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target"
      ],
      "metadata": {
        "id": "jN-g8V7ByvRH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
        "                    dropout=dropout_rate, batch_first=True)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    self.linear = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    if tie_weights:\n",
        "          assert embedding_dim == hidden_dim, 'cannot tie, check dims'\n",
        "          self.linear.weight = self.embedding.weight\n",
        "    self.init_weights()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x is a batch of input sequences\n",
        "    x = self.embedding(x)\n",
        "    x, _ = self.lstm(x)\n",
        "    x = self.linear(x)\n",
        "    return x\n",
        "\n",
        "  def init_weights(self):\n",
        "    init_range_emb = 0.1\n",
        "    init_range_other = 1/math.sqrt(self.hidden_dim)\n",
        "    self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
        "    self.linear.weight.data.uniform_(-init_range_other, init_range_other)\n",
        "    self.linear.bias.data.zero_()\n",
        "    for i in range(self.num_layers):\n",
        "        self.lstm.all_weights[i][0] = torch.FloatTensor(self.embedding_dim,\n",
        "                self.hidden_dim).uniform_(-init_range_other, init_range_other) \n",
        "        self.lstm.all_weights[i][1] = torch.FloatTensor(self.hidden_dim, \n",
        "                self.hidden_dim).uniform_(-init_range_other, init_range_other)\n",
        "\n",
        "  def init_state(self, sequence_length):\n",
        "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n",
        "                torch.zeros(self.num_layers, sequence_length, self.lstm_size))       "
      ],
      "metadata": {
        "id": "xYF8ErZ2zE6b"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 100\n",
        "num_layers = 2\n",
        "dropout_rate = 0.4\n",
        "tie_weights = True\n",
        "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "5vPNWmqoUWmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e57ab253-8b84-463c-a80c-66e05b5d44c7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (embedding): Embedding(9922, 100)\n",
              "  (lstm): LSTM(100, 100, num_layers=2, batch_first=True, dropout=0.4)\n",
              "  (dropout): Dropout(p=0.4, inplace=False)\n",
              "  (linear): Linear(in_features=100, out_features=9922, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 20.0  # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "def train(model: nn.Module) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "    \n",
        "    num_batches = len(train_data) // bptt\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        seq_len = data.size(0)\n",
        "        output = model(data)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ],
      "metadata": {
        "id": "VbUtSuzbUbea"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(eval_data, i)\n",
        "            seq_len = data.size(0)\n",
        "            output = model(data)\n",
        "            output_flat = output.view(-1, vocab_size)\n",
        "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(eval_data) - 1)"
      ],
      "metadata": {
        "id": "5SjEJ3OY91mw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 50\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model)\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = copy.deepcopy(model)\n",
        "\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RQth0cB_Jj8",
        "outputId": "982b6580-975e-4de5-8f3a-ffc756e874b4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/ 1155 batches | lr 20.00 | ms/batch 10.33 | loss  6.90 | ppl   988.96\n",
            "| epoch   1 |   400/ 1155 batches | lr 20.00 | ms/batch  4.55 | loss  6.21 | ppl   498.12\n",
            "| epoch   1 |   600/ 1155 batches | lr 20.00 | ms/batch  4.55 | loss  5.99 | ppl   398.23\n",
            "| epoch   1 |   800/ 1155 batches | lr 20.00 | ms/batch  4.55 | loss  5.86 | ppl   352.43\n",
            "| epoch   1 |  1000/ 1155 batches | lr 20.00 | ms/batch  4.58 | loss  5.77 | ppl   320.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time:  6.58s | valid loss  5.73 | valid ppl   308.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 1155 batches | lr 19.00 | ms/batch  4.56 | loss  5.69 | ppl   295.28\n",
            "| epoch   2 |   400/ 1155 batches | lr 19.00 | ms/batch  4.56 | loss  5.62 | ppl   276.07\n",
            "| epoch   2 |   600/ 1155 batches | lr 19.00 | ms/batch  4.59 | loss  5.58 | ppl   265.80\n",
            "| epoch   2 |   800/ 1155 batches | lr 19.00 | ms/batch  4.58 | loss  5.55 | ppl   256.08\n",
            "| epoch   2 |  1000/ 1155 batches | lr 19.00 | ms/batch  4.57 | loss  5.52 | ppl   250.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time:  5.45s | valid loss  5.56 | valid ppl   260.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 1155 batches | lr 18.05 | ms/batch  4.62 | loss  5.50 | ppl   243.83\n",
            "| epoch   3 |   400/ 1155 batches | lr 18.05 | ms/batch  4.61 | loss  5.46 | ppl   235.13\n",
            "| epoch   3 |   600/ 1155 batches | lr 18.05 | ms/batch  4.61 | loss  5.45 | ppl   232.19\n",
            "| epoch   3 |   800/ 1155 batches | lr 18.05 | ms/batch  4.62 | loss  5.42 | ppl   226.83\n",
            "| epoch   3 |  1000/ 1155 batches | lr 18.05 | ms/batch  4.62 | loss  5.42 | ppl   225.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time:  5.50s | valid loss  5.49 | valid ppl   242.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/ 1155 batches | lr 17.15 | ms/batch  4.99 | loss  5.40 | ppl   222.23\n",
            "| epoch   4 |   400/ 1155 batches | lr 17.15 | ms/batch  4.98 | loss  5.37 | ppl   215.44\n",
            "| epoch   4 |   600/ 1155 batches | lr 17.15 | ms/batch  5.12 | loss  5.37 | ppl   215.17\n",
            "| epoch   4 |   800/ 1155 batches | lr 17.15 | ms/batch  4.66 | loss  5.35 | ppl   211.10\n",
            "| epoch   4 |  1000/ 1155 batches | lr 17.15 | ms/batch  4.68 | loss  5.35 | ppl   210.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time:  5.84s | valid loss  5.45 | valid ppl   232.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/ 1155 batches | lr 16.29 | ms/batch  5.24 | loss  5.34 | ppl   208.90\n",
            "| epoch   5 |   400/ 1155 batches | lr 16.29 | ms/batch  5.75 | loss  5.31 | ppl   203.15\n",
            "| epoch   5 |   600/ 1155 batches | lr 16.29 | ms/batch  4.96 | loss  5.32 | ppl   204.31\n",
            "| epoch   5 |   800/ 1155 batches | lr 16.29 | ms/batch  5.09 | loss  5.30 | ppl   200.39\n",
            "| epoch   5 |  1000/ 1155 batches | lr 16.29 | ms/batch  4.92 | loss  5.30 | ppl   200.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time:  6.10s | valid loss  5.42 | valid ppl   226.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/ 1155 batches | lr 15.48 | ms/batch  4.74 | loss  5.30 | ppl   199.84\n",
            "| epoch   6 |   400/ 1155 batches | lr 15.48 | ms/batch  4.72 | loss  5.27 | ppl   194.36\n",
            "| epoch   6 |   600/ 1155 batches | lr 15.48 | ms/batch  4.70 | loss  5.28 | ppl   196.19\n",
            "| epoch   6 |   800/ 1155 batches | lr 15.48 | ms/batch  4.71 | loss  5.26 | ppl   192.53\n",
            "| epoch   6 |  1000/ 1155 batches | lr 15.48 | ms/batch  4.71 | loss  5.26 | ppl   193.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time:  5.61s | valid loss  5.40 | valid ppl   222.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/ 1155 batches | lr 14.70 | ms/batch  4.75 | loss  5.26 | ppl   192.67\n",
            "| epoch   7 |   400/ 1155 batches | lr 14.70 | ms/batch  4.72 | loss  5.23 | ppl   187.56\n",
            "| epoch   7 |   600/ 1155 batches | lr 14.70 | ms/batch  4.73 | loss  5.25 | ppl   189.91\n",
            "| epoch   7 |   800/ 1155 batches | lr 14.70 | ms/batch  4.75 | loss  5.23 | ppl   186.60\n",
            "| epoch   7 |  1000/ 1155 batches | lr 14.70 | ms/batch  4.77 | loss  5.23 | ppl   187.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time:  5.65s | valid loss  5.39 | valid ppl   218.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/ 1155 batches | lr 13.97 | ms/batch  4.78 | loss  5.23 | ppl   186.74\n",
            "| epoch   8 |   400/ 1155 batches | lr 13.97 | ms/batch  4.80 | loss  5.21 | ppl   182.55\n",
            "| epoch   8 |   600/ 1155 batches | lr 13.97 | ms/batch  4.76 | loss  5.22 | ppl   185.14\n",
            "| epoch   8 |   800/ 1155 batches | lr 13.97 | ms/batch  4.78 | loss  5.20 | ppl   181.71\n",
            "| epoch   8 |  1000/ 1155 batches | lr 13.97 | ms/batch  4.83 | loss  5.21 | ppl   182.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time:  5.70s | valid loss  5.38 | valid ppl   216.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/ 1155 batches | lr 13.27 | ms/batch  4.81 | loss  5.20 | ppl   182.00\n",
            "| epoch   9 |   400/ 1155 batches | lr 13.27 | ms/batch  4.79 | loss  5.18 | ppl   177.78\n",
            "| epoch   9 |   600/ 1155 batches | lr 13.27 | ms/batch  4.81 | loss  5.20 | ppl   180.43\n",
            "| epoch   9 |   800/ 1155 batches | lr 13.27 | ms/batch  4.82 | loss  5.18 | ppl   177.37\n",
            "| epoch   9 |  1000/ 1155 batches | lr 13.27 | ms/batch  4.81 | loss  5.18 | ppl   178.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time:  5.72s | valid loss  5.37 | valid ppl   214.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/ 1155 batches | lr 12.60 | ms/batch  4.86 | loss  5.18 | ppl   178.23\n",
            "| epoch  10 |   400/ 1155 batches | lr 12.60 | ms/batch  4.83 | loss  5.16 | ppl   174.00\n",
            "| epoch  10 |   600/ 1155 batches | lr 12.60 | ms/batch  4.84 | loss  5.18 | ppl   176.94\n",
            "| epoch  10 |   800/ 1155 batches | lr 12.60 | ms/batch  4.83 | loss  5.16 | ppl   173.81\n",
            "| epoch  10 |  1000/ 1155 batches | lr 12.60 | ms/batch  4.81 | loss  5.16 | ppl   175.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time:  5.75s | valid loss  5.36 | valid ppl   213.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   200/ 1155 batches | lr 11.97 | ms/batch  4.86 | loss  5.16 | ppl   174.53\n",
            "| epoch  11 |   400/ 1155 batches | lr 11.97 | ms/batch  4.84 | loss  5.14 | ppl   170.40\n",
            "| epoch  11 |   600/ 1155 batches | lr 11.97 | ms/batch  4.85 | loss  5.16 | ppl   173.77\n",
            "| epoch  11 |   800/ 1155 batches | lr 11.97 | ms/batch  4.85 | loss  5.14 | ppl   170.55\n",
            "| epoch  11 |  1000/ 1155 batches | lr 11.97 | ms/batch  4.83 | loss  5.15 | ppl   172.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time:  5.76s | valid loss  5.35 | valid ppl   211.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |   200/ 1155 batches | lr 11.38 | ms/batch  4.85 | loss  5.15 | ppl   171.62\n",
            "| epoch  12 |   400/ 1155 batches | lr 11.38 | ms/batch  4.83 | loss  5.12 | ppl   167.74\n",
            "| epoch  12 |   600/ 1155 batches | lr 11.38 | ms/batch  4.82 | loss  5.14 | ppl   171.26\n",
            "| epoch  12 |   800/ 1155 batches | lr 11.38 | ms/batch  4.80 | loss  5.12 | ppl   167.75\n",
            "| epoch  12 |  1000/ 1155 batches | lr 11.38 | ms/batch  4.80 | loss  5.13 | ppl   169.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time:  5.74s | valid loss  5.35 | valid ppl   210.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   200/ 1155 batches | lr 10.81 | ms/batch  4.82 | loss  5.13 | ppl   168.93\n",
            "| epoch  13 |   400/ 1155 batches | lr 10.81 | ms/batch  4.77 | loss  5.11 | ppl   165.13\n",
            "| epoch  13 |   600/ 1155 batches | lr 10.81 | ms/batch  4.77 | loss  5.13 | ppl   168.72\n",
            "| epoch  13 |   800/ 1155 batches | lr 10.81 | ms/batch  4.78 | loss  5.11 | ppl   165.03\n",
            "| epoch  13 |  1000/ 1155 batches | lr 10.81 | ms/batch  4.78 | loss  5.12 | ppl   167.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time:  5.69s | valid loss  5.35 | valid ppl   210.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   200/ 1155 batches | lr 10.27 | ms/batch  4.77 | loss  5.12 | ppl   166.53\n",
            "| epoch  14 |   400/ 1155 batches | lr 10.27 | ms/batch  4.77 | loss  5.09 | ppl   162.80\n",
            "| epoch  14 |   600/ 1155 batches | lr 10.27 | ms/batch  4.75 | loss  5.11 | ppl   166.41\n",
            "| epoch  14 |   800/ 1155 batches | lr 10.27 | ms/batch  4.76 | loss  5.09 | ppl   162.83\n",
            "| epoch  14 |  1000/ 1155 batches | lr 10.27 | ms/batch  4.74 | loss  5.11 | ppl   164.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time:  5.66s | valid loss  5.34 | valid ppl   209.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |   200/ 1155 batches | lr 9.75 | ms/batch  4.77 | loss  5.10 | ppl   164.49\n",
            "| epoch  15 |   400/ 1155 batches | lr 9.75 | ms/batch  4.76 | loss  5.08 | ppl   160.82\n",
            "| epoch  15 |   600/ 1155 batches | lr 9.75 | ms/batch  4.75 | loss  5.10 | ppl   164.39\n",
            "| epoch  15 |   800/ 1155 batches | lr 9.75 | ms/batch  4.84 | loss  5.08 | ppl   161.11\n",
            "| epoch  15 |  1000/ 1155 batches | lr 9.75 | ms/batch  5.25 | loss  5.09 | ppl   163.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time:  5.82s | valid loss  5.34 | valid ppl   208.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   200/ 1155 batches | lr 9.27 | ms/batch  4.73 | loss  5.09 | ppl   162.29\n",
            "| epoch  16 |   400/ 1155 batches | lr 9.27 | ms/batch  4.73 | loss  5.07 | ppl   159.00\n",
            "| epoch  16 |   600/ 1155 batches | lr 9.27 | ms/batch  4.73 | loss  5.09 | ppl   161.99\n",
            "| epoch  16 |   800/ 1155 batches | lr 9.27 | ms/batch  4.71 | loss  5.07 | ppl   159.04\n",
            "| epoch  16 |  1000/ 1155 batches | lr 9.27 | ms/batch  4.71 | loss  5.08 | ppl   161.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time:  5.62s | valid loss  5.34 | valid ppl   207.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   200/ 1155 batches | lr 8.80 | ms/batch  4.74 | loss  5.08 | ppl   160.60\n",
            "| epoch  17 |   400/ 1155 batches | lr 8.80 | ms/batch  4.73 | loss  5.06 | ppl   156.99\n",
            "| epoch  17 |   600/ 1155 batches | lr 8.80 | ms/batch  4.71 | loss  5.08 | ppl   160.72\n",
            "| epoch  17 |   800/ 1155 batches | lr 8.80 | ms/batch  4.73 | loss  5.06 | ppl   157.44\n",
            "| epoch  17 |  1000/ 1155 batches | lr 8.80 | ms/batch  4.75 | loss  5.07 | ppl   159.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time:  5.63s | valid loss  5.34 | valid ppl   207.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   200/ 1155 batches | lr 8.36 | ms/batch  4.73 | loss  5.07 | ppl   159.04\n",
            "| epoch  18 |   400/ 1155 batches | lr 8.36 | ms/batch  4.73 | loss  5.05 | ppl   155.68\n",
            "| epoch  18 |   600/ 1155 batches | lr 8.36 | ms/batch  4.71 | loss  5.07 | ppl   158.95\n",
            "| epoch  18 |   800/ 1155 batches | lr 8.36 | ms/batch  4.72 | loss  5.05 | ppl   156.00\n",
            "| epoch  18 |  1000/ 1155 batches | lr 8.36 | ms/batch  4.72 | loss  5.06 | ppl   157.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time:  5.62s | valid loss  5.33 | valid ppl   207.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |   200/ 1155 batches | lr 7.94 | ms/batch  4.74 | loss  5.06 | ppl   157.61\n",
            "| epoch  19 |   400/ 1155 batches | lr 7.94 | ms/batch  4.72 | loss  5.04 | ppl   153.95\n",
            "| epoch  19 |   600/ 1155 batches | lr 7.94 | ms/batch  4.73 | loss  5.06 | ppl   157.68\n",
            "| epoch  19 |   800/ 1155 batches | lr 7.94 | ms/batch  4.73 | loss  5.04 | ppl   154.52\n",
            "| epoch  19 |  1000/ 1155 batches | lr 7.94 | ms/batch  4.72 | loss  5.06 | ppl   156.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time:  5.63s | valid loss  5.33 | valid ppl   207.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |   200/ 1155 batches | lr 7.55 | ms/batch  4.74 | loss  5.05 | ppl   156.10\n",
            "| epoch  20 |   400/ 1155 batches | lr 7.55 | ms/batch  4.73 | loss  5.03 | ppl   152.69\n",
            "| epoch  20 |   600/ 1155 batches | lr 7.55 | ms/batch  4.73 | loss  5.05 | ppl   156.19\n",
            "| epoch  20 |   800/ 1155 batches | lr 7.55 | ms/batch  4.74 | loss  5.03 | ppl   153.01\n",
            "| epoch  20 |  1000/ 1155 batches | lr 7.55 | ms/batch  4.73 | loss  5.05 | ppl   155.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time:  5.63s | valid loss  5.33 | valid ppl   206.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |   200/ 1155 batches | lr 7.17 | ms/batch  4.75 | loss  5.04 | ppl   154.85\n",
            "| epoch  21 |   400/ 1155 batches | lr 7.17 | ms/batch  4.74 | loss  5.02 | ppl   151.52\n",
            "| epoch  21 |   600/ 1155 batches | lr 7.17 | ms/batch  4.75 | loss  5.04 | ppl   154.98\n",
            "| epoch  21 |   800/ 1155 batches | lr 7.17 | ms/batch  4.73 | loss  5.02 | ppl   151.63\n",
            "| epoch  21 |  1000/ 1155 batches | lr 7.17 | ms/batch  4.75 | loss  5.04 | ppl   154.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time:  5.65s | valid loss  5.33 | valid ppl   206.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |   200/ 1155 batches | lr 6.81 | ms/batch  4.76 | loss  5.03 | ppl   153.59\n",
            "| epoch  22 |   400/ 1155 batches | lr 6.81 | ms/batch  4.75 | loss  5.01 | ppl   150.53\n",
            "| epoch  22 |   600/ 1155 batches | lr 6.81 | ms/batch  4.74 | loss  5.04 | ppl   153.90\n",
            "| epoch  22 |   800/ 1155 batches | lr 6.81 | ms/batch  4.76 | loss  5.02 | ppl   150.91\n",
            "| epoch  22 |  1000/ 1155 batches | lr 6.81 | ms/batch  4.75 | loss  5.03 | ppl   152.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time:  5.65s | valid loss  5.33 | valid ppl   206.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |   200/ 1155 batches | lr 6.47 | ms/batch  4.77 | loss  5.03 | ppl   152.67\n",
            "| epoch  23 |   400/ 1155 batches | lr 6.47 | ms/batch  4.77 | loss  5.00 | ppl   148.86\n",
            "| epoch  23 |   600/ 1155 batches | lr 6.47 | ms/batch  4.74 | loss  5.03 | ppl   152.85\n",
            "| epoch  23 |   800/ 1155 batches | lr 6.47 | ms/batch  4.76 | loss  5.01 | ppl   149.79\n",
            "| epoch  23 |  1000/ 1155 batches | lr 6.47 | ms/batch  4.77 | loss  5.02 | ppl   151.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time:  5.67s | valid loss  5.33 | valid ppl   206.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |   200/ 1155 batches | lr 6.15 | ms/batch  4.80 | loss  5.02 | ppl   151.62\n",
            "| epoch  24 |   400/ 1155 batches | lr 6.15 | ms/batch  4.76 | loss  5.00 | ppl   148.30\n",
            "| epoch  24 |   600/ 1155 batches | lr 6.15 | ms/batch  4.80 | loss  5.02 | ppl   152.01\n",
            "| epoch  24 |   800/ 1155 batches | lr 6.15 | ms/batch  4.76 | loss  5.00 | ppl   148.70\n",
            "| epoch  24 |  1000/ 1155 batches | lr 6.15 | ms/batch  4.76 | loss  5.02 | ppl   150.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time:  5.68s | valid loss  5.33 | valid ppl   206.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |   200/ 1155 batches | lr 5.84 | ms/batch  4.79 | loss  5.01 | ppl   150.35\n",
            "| epoch  25 |   400/ 1155 batches | lr 5.84 | ms/batch  4.77 | loss  4.99 | ppl   147.22\n",
            "| epoch  25 |   600/ 1155 batches | lr 5.84 | ms/batch  4.79 | loss  5.02 | ppl   150.71\n",
            "| epoch  25 |   800/ 1155 batches | lr 5.84 | ms/batch  4.76 | loss  4.99 | ppl   147.66\n",
            "| epoch  25 |  1000/ 1155 batches | lr 5.84 | ms/batch  4.77 | loss  5.01 | ppl   149.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time:  5.69s | valid loss  5.33 | valid ppl   206.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |   200/ 1155 batches | lr 5.55 | ms/batch  4.79 | loss  5.01 | ppl   149.44\n",
            "| epoch  26 |   400/ 1155 batches | lr 5.55 | ms/batch  4.76 | loss  4.99 | ppl   146.30\n",
            "| epoch  26 |   600/ 1155 batches | lr 5.55 | ms/batch  4.76 | loss  5.01 | ppl   149.56\n",
            "| epoch  26 |   800/ 1155 batches | lr 5.55 | ms/batch  4.78 | loss  4.99 | ppl   146.68\n",
            "| epoch  26 |  1000/ 1155 batches | lr 5.55 | ms/batch  4.75 | loss  5.00 | ppl   148.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time:  5.68s | valid loss  5.33 | valid ppl   206.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |   200/ 1155 batches | lr 5.27 | ms/batch  4.79 | loss  5.00 | ppl   148.70\n",
            "| epoch  27 |   400/ 1155 batches | lr 5.27 | ms/batch  4.78 | loss  4.98 | ppl   145.42\n",
            "| epoch  27 |   600/ 1155 batches | lr 5.27 | ms/batch  4.76 | loss  5.00 | ppl   148.89\n",
            "| epoch  27 |   800/ 1155 batches | lr 5.27 | ms/batch  4.77 | loss  4.98 | ppl   146.15\n",
            "| epoch  27 |  1000/ 1155 batches | lr 5.27 | ms/batch  4.77 | loss  5.00 | ppl   147.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time:  5.68s | valid loss  5.33 | valid ppl   206.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |   200/ 1155 batches | lr 5.01 | ms/batch  4.78 | loss  5.00 | ppl   147.90\n",
            "| epoch  28 |   400/ 1155 batches | lr 5.01 | ms/batch  4.77 | loss  4.97 | ppl   144.56\n",
            "| epoch  28 |   600/ 1155 batches | lr 5.01 | ms/batch  4.76 | loss  5.00 | ppl   147.89\n",
            "| epoch  28 |   800/ 1155 batches | lr 5.01 | ms/batch  4.77 | loss  4.98 | ppl   145.26\n",
            "| epoch  28 |  1000/ 1155 batches | lr 5.01 | ms/batch  4.80 | loss  4.99 | ppl   147.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time:  5.68s | valid loss  5.33 | valid ppl   206.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |   200/ 1155 batches | lr 4.76 | ms/batch  4.78 | loss  4.99 | ppl   147.06\n",
            "| epoch  29 |   400/ 1155 batches | lr 4.76 | ms/batch  4.76 | loss  4.97 | ppl   143.76\n",
            "| epoch  29 |   600/ 1155 batches | lr 4.76 | ms/batch  4.76 | loss  4.99 | ppl   147.08\n",
            "| epoch  29 |   800/ 1155 batches | lr 4.76 | ms/batch  4.76 | loss  4.98 | ppl   144.83\n",
            "| epoch  29 |  1000/ 1155 batches | lr 4.76 | ms/batch  4.76 | loss  4.99 | ppl   146.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time:  5.67s | valid loss  5.33 | valid ppl   206.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |   200/ 1155 batches | lr 4.52 | ms/batch  4.78 | loss  4.99 | ppl   146.27\n",
            "| epoch  30 |   400/ 1155 batches | lr 4.52 | ms/batch  4.76 | loss  4.96 | ppl   143.11\n",
            "| epoch  30 |   600/ 1155 batches | lr 4.52 | ms/batch  4.75 | loss  4.99 | ppl   146.36\n",
            "| epoch  30 |   800/ 1155 batches | lr 4.52 | ms/batch  4.76 | loss  4.97 | ppl   143.79\n",
            "| epoch  30 |  1000/ 1155 batches | lr 4.52 | ms/batch  4.76 | loss  4.98 | ppl   145.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time:  5.66s | valid loss  5.33 | valid ppl   206.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  31 |   200/ 1155 batches | lr 4.29 | ms/batch  4.79 | loss  4.98 | ppl   145.62\n",
            "| epoch  31 |   400/ 1155 batches | lr 4.29 | ms/batch  4.75 | loss  4.96 | ppl   142.47\n",
            "| epoch  31 |   600/ 1155 batches | lr 4.29 | ms/batch  4.76 | loss  4.98 | ppl   145.74\n",
            "| epoch  31 |   800/ 1155 batches | lr 4.29 | ms/batch  4.75 | loss  4.96 | ppl   143.02\n",
            "| epoch  31 |  1000/ 1155 batches | lr 4.29 | ms/batch  4.76 | loss  4.98 | ppl   145.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time:  5.67s | valid loss  5.33 | valid ppl   206.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |   200/ 1155 batches | lr 4.08 | ms/batch  4.77 | loss  4.98 | ppl   145.12\n",
            "| epoch  32 |   400/ 1155 batches | lr 4.08 | ms/batch  4.73 | loss  4.96 | ppl   141.96\n",
            "| epoch  32 |   600/ 1155 batches | lr 4.08 | ms/batch  4.76 | loss  4.98 | ppl   145.24\n",
            "| epoch  32 |   800/ 1155 batches | lr 4.08 | ms/batch  4.76 | loss  4.96 | ppl   142.62\n",
            "| epoch  32 |  1000/ 1155 batches | lr 4.08 | ms/batch  4.73 | loss  4.97 | ppl   144.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time:  5.66s | valid loss  5.33 | valid ppl   206.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |   200/ 1155 batches | lr 3.87 | ms/batch  4.78 | loss  4.97 | ppl   144.42\n",
            "| epoch  33 |   400/ 1155 batches | lr 3.87 | ms/batch  4.75 | loss  4.95 | ppl   141.19\n",
            "| epoch  33 |   600/ 1155 batches | lr 3.87 | ms/batch  4.76 | loss  4.97 | ppl   144.15\n",
            "| epoch  33 |   800/ 1155 batches | lr 3.87 | ms/batch  4.75 | loss  4.96 | ppl   141.94\n",
            "| epoch  33 |  1000/ 1155 batches | lr 3.87 | ms/batch  4.73 | loss  4.97 | ppl   144.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time:  5.66s | valid loss  5.33 | valid ppl   206.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |   200/ 1155 batches | lr 3.68 | ms/batch  4.80 | loss  4.97 | ppl   143.92\n",
            "| epoch  34 |   400/ 1155 batches | lr 3.68 | ms/batch  4.79 | loss  4.95 | ppl   140.57\n",
            "| epoch  34 |   600/ 1155 batches | lr 3.68 | ms/batch  4.75 | loss  4.97 | ppl   143.75\n",
            "| epoch  34 |   800/ 1155 batches | lr 3.68 | ms/batch  4.75 | loss  4.95 | ppl   141.25\n",
            "| epoch  34 |  1000/ 1155 batches | lr 3.68 | ms/batch  4.74 | loss  4.97 | ppl   143.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time:  5.67s | valid loss  5.33 | valid ppl   206.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |   200/ 1155 batches | lr 3.50 | ms/batch  4.76 | loss  4.96 | ppl   142.95\n",
            "| epoch  35 |   400/ 1155 batches | lr 3.50 | ms/batch  4.74 | loss  4.94 | ppl   140.11\n",
            "| epoch  35 |   600/ 1155 batches | lr 3.50 | ms/batch  4.77 | loss  4.96 | ppl   143.12\n",
            "| epoch  35 |   800/ 1155 batches | lr 3.50 | ms/batch  4.74 | loss  4.95 | ppl   140.88\n",
            "| epoch  35 |  1000/ 1155 batches | lr 3.50 | ms/batch  4.75 | loss  4.96 | ppl   142.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time:  5.66s | valid loss  5.33 | valid ppl   206.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |   200/ 1155 batches | lr 3.32 | ms/batch  4.78 | loss  4.96 | ppl   142.51\n",
            "| epoch  36 |   400/ 1155 batches | lr 3.32 | ms/batch  4.74 | loss  4.94 | ppl   139.59\n",
            "| epoch  36 |   600/ 1155 batches | lr 3.32 | ms/batch  4.75 | loss  4.96 | ppl   142.98\n",
            "| epoch  36 |   800/ 1155 batches | lr 3.32 | ms/batch  4.76 | loss  4.94 | ppl   140.04\n",
            "| epoch  36 |  1000/ 1155 batches | lr 3.32 | ms/batch  4.74 | loss  4.96 | ppl   142.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time:  5.68s | valid loss  5.33 | valid ppl   206.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  37 |   200/ 1155 batches | lr 3.16 | ms/batch  4.82 | loss  4.96 | ppl   141.89\n",
            "| epoch  37 |   400/ 1155 batches | lr 3.16 | ms/batch  4.78 | loss  4.93 | ppl   139.06\n",
            "| epoch  37 |   600/ 1155 batches | lr 3.16 | ms/batch  4.75 | loss  4.96 | ppl   142.52\n",
            "| epoch  37 |   800/ 1155 batches | lr 3.16 | ms/batch  4.74 | loss  4.94 | ppl   139.71\n",
            "| epoch  37 |  1000/ 1155 batches | lr 3.16 | ms/batch  4.78 | loss  4.95 | ppl   141.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time:  5.68s | valid loss  5.33 | valid ppl   206.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |   200/ 1155 batches | lr 3.00 | ms/batch  4.79 | loss  4.95 | ppl   141.50\n",
            "| epoch  38 |   400/ 1155 batches | lr 3.00 | ms/batch  4.74 | loss  4.93 | ppl   138.71\n",
            "| epoch  38 |   600/ 1155 batches | lr 3.00 | ms/batch  4.78 | loss  4.96 | ppl   141.94\n",
            "| epoch  38 |   800/ 1155 batches | lr 3.00 | ms/batch  4.76 | loss  4.94 | ppl   139.22\n",
            "| epoch  38 |  1000/ 1155 batches | lr 3.00 | ms/batch  4.75 | loss  4.95 | ppl   141.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time:  5.67s | valid loss  5.33 | valid ppl   206.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |   200/ 1155 batches | lr 2.85 | ms/batch  4.79 | loss  4.95 | ppl   141.04\n",
            "| epoch  39 |   400/ 1155 batches | lr 2.85 | ms/batch  4.76 | loss  4.93 | ppl   138.02\n",
            "| epoch  39 |   600/ 1155 batches | lr 2.85 | ms/batch  4.76 | loss  4.95 | ppl   141.46\n",
            "| epoch  39 |   800/ 1155 batches | lr 2.85 | ms/batch  4.75 | loss  4.93 | ppl   138.87\n",
            "| epoch  39 |  1000/ 1155 batches | lr 2.85 | ms/batch  4.79 | loss  4.95 | ppl   140.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time:  5.68s | valid loss  5.33 | valid ppl   206.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |   200/ 1155 batches | lr 2.71 | ms/batch  4.80 | loss  4.94 | ppl   140.41\n",
            "| epoch  40 |   400/ 1155 batches | lr 2.71 | ms/batch  4.76 | loss  4.92 | ppl   137.30\n",
            "| epoch  40 |   600/ 1155 batches | lr 2.71 | ms/batch  4.75 | loss  4.95 | ppl   140.86\n",
            "| epoch  40 |   800/ 1155 batches | lr 2.71 | ms/batch  4.76 | loss  4.93 | ppl   138.24\n",
            "| epoch  40 |  1000/ 1155 batches | lr 2.71 | ms/batch  4.78 | loss  4.94 | ppl   140.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time:  5.67s | valid loss  5.33 | valid ppl   206.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  41 |   200/ 1155 batches | lr 2.57 | ms/batch  4.80 | loss  4.94 | ppl   140.31\n",
            "| epoch  41 |   400/ 1155 batches | lr 2.57 | ms/batch  4.74 | loss  4.92 | ppl   136.83\n",
            "| epoch  41 |   600/ 1155 batches | lr 2.57 | ms/batch  4.78 | loss  4.95 | ppl   140.53\n",
            "| epoch  41 |   800/ 1155 batches | lr 2.57 | ms/batch  4.76 | loss  4.93 | ppl   138.07\n",
            "| epoch  41 |  1000/ 1155 batches | lr 2.57 | ms/batch  4.79 | loss  4.94 | ppl   139.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time:  5.68s | valid loss  5.33 | valid ppl   206.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  42 |   200/ 1155 batches | lr 2.44 | ms/batch  4.79 | loss  4.94 | ppl   139.73\n",
            "| epoch  42 |   400/ 1155 batches | lr 2.44 | ms/batch  4.76 | loss  4.92 | ppl   136.86\n",
            "| epoch  42 |   600/ 1155 batches | lr 2.44 | ms/batch  4.78 | loss  4.94 | ppl   140.01\n",
            "| epoch  42 |   800/ 1155 batches | lr 2.44 | ms/batch  4.76 | loss  4.92 | ppl   137.43\n",
            "| epoch  42 |  1000/ 1155 batches | lr 2.44 | ms/batch  4.79 | loss  4.94 | ppl   139.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time:  5.68s | valid loss  5.33 | valid ppl   206.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  43 |   200/ 1155 batches | lr 2.32 | ms/batch  4.78 | loss  4.94 | ppl   139.52\n",
            "| epoch  43 |   400/ 1155 batches | lr 2.32 | ms/batch  4.79 | loss  4.92 | ppl   136.39\n",
            "| epoch  43 |   600/ 1155 batches | lr 2.32 | ms/batch  4.77 | loss  4.94 | ppl   139.59\n",
            "| epoch  43 |   800/ 1155 batches | lr 2.32 | ms/batch  4.79 | loss  4.92 | ppl   137.20\n",
            "| epoch  43 |  1000/ 1155 batches | lr 2.32 | ms/batch  4.77 | loss  4.93 | ppl   138.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time:  5.69s | valid loss  5.33 | valid ppl   206.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  44 |   200/ 1155 batches | lr 2.20 | ms/batch  4.80 | loss  4.94 | ppl   139.14\n",
            "| epoch  44 |   400/ 1155 batches | lr 2.20 | ms/batch  4.75 | loss  4.91 | ppl   135.89\n",
            "| epoch  44 |   600/ 1155 batches | lr 2.20 | ms/batch  4.76 | loss  4.94 | ppl   139.32\n",
            "| epoch  44 |   800/ 1155 batches | lr 2.20 | ms/batch  4.76 | loss  4.92 | ppl   136.71\n",
            "| epoch  44 |  1000/ 1155 batches | lr 2.20 | ms/batch  4.77 | loss  4.93 | ppl   138.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time:  5.67s | valid loss  5.33 | valid ppl   206.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  45 |   200/ 1155 batches | lr 2.09 | ms/batch  4.79 | loss  4.93 | ppl   138.60\n",
            "| epoch  45 |   400/ 1155 batches | lr 2.09 | ms/batch  4.76 | loss  4.91 | ppl   135.84\n",
            "| epoch  45 |   600/ 1155 batches | lr 2.09 | ms/batch  4.80 | loss  4.93 | ppl   139.04\n",
            "| epoch  45 |   800/ 1155 batches | lr 2.09 | ms/batch  4.79 | loss  4.91 | ppl   136.13\n",
            "| epoch  45 |  1000/ 1155 batches | lr 2.09 | ms/batch  4.78 | loss  4.93 | ppl   138.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time:  5.69s | valid loss  5.33 | valid ppl   207.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  46 |   200/ 1155 batches | lr 1.99 | ms/batch  4.81 | loss  4.93 | ppl   138.47\n",
            "| epoch  46 |   400/ 1155 batches | lr 1.99 | ms/batch  4.77 | loss  4.91 | ppl   135.66\n",
            "| epoch  46 |   600/ 1155 batches | lr 1.99 | ms/batch  4.76 | loss  4.93 | ppl   138.82\n",
            "| epoch  46 |   800/ 1155 batches | lr 1.99 | ms/batch  4.79 | loss  4.91 | ppl   136.05\n",
            "| epoch  46 |  1000/ 1155 batches | lr 1.99 | ms/batch  4.75 | loss  4.93 | ppl   138.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time:  5.69s | valid loss  5.33 | valid ppl   207.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  47 |   200/ 1155 batches | lr 1.89 | ms/batch  4.78 | loss  4.93 | ppl   138.01\n",
            "| epoch  47 |   400/ 1155 batches | lr 1.89 | ms/batch  4.76 | loss  4.90 | ppl   134.92\n",
            "| epoch  47 |   600/ 1155 batches | lr 1.89 | ms/batch  4.75 | loss  4.93 | ppl   138.41\n",
            "| epoch  47 |   800/ 1155 batches | lr 1.89 | ms/batch  4.76 | loss  4.91 | ppl   135.71\n",
            "| epoch  47 |  1000/ 1155 batches | lr 1.89 | ms/batch  4.77 | loss  4.93 | ppl   137.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time:  5.67s | valid loss  5.33 | valid ppl   207.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  48 |   200/ 1155 batches | lr 1.79 | ms/batch  4.79 | loss  4.92 | ppl   137.49\n",
            "| epoch  48 |   400/ 1155 batches | lr 1.79 | ms/batch  4.79 | loss  4.90 | ppl   134.64\n",
            "| epoch  48 |   600/ 1155 batches | lr 1.79 | ms/batch  4.77 | loss  4.93 | ppl   138.07\n",
            "| epoch  48 |   800/ 1155 batches | lr 1.79 | ms/batch  4.75 | loss  4.91 | ppl   135.52\n",
            "| epoch  48 |  1000/ 1155 batches | lr 1.79 | ms/batch  4.76 | loss  4.92 | ppl   137.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time:  5.68s | valid loss  5.33 | valid ppl   207.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  49 |   200/ 1155 batches | lr 1.71 | ms/batch  4.82 | loss  4.92 | ppl   137.40\n",
            "| epoch  49 |   400/ 1155 batches | lr 1.71 | ms/batch  4.76 | loss  4.90 | ppl   134.58\n",
            "| epoch  49 |   600/ 1155 batches | lr 1.71 | ms/batch  4.77 | loss  4.92 | ppl   137.44\n",
            "| epoch  49 |   800/ 1155 batches | lr 1.71 | ms/batch  4.75 | loss  4.91 | ppl   134.99\n",
            "| epoch  49 |  1000/ 1155 batches | lr 1.71 | ms/batch  4.77 | loss  4.92 | ppl   137.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time:  5.68s | valid loss  5.33 | valid ppl   207.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  50 |   200/ 1155 batches | lr 1.62 | ms/batch  4.80 | loss  4.92 | ppl   137.26\n",
            "| epoch  50 |   400/ 1155 batches | lr 1.62 | ms/batch  4.77 | loss  4.90 | ppl   134.29\n",
            "| epoch  50 |   600/ 1155 batches | lr 1.62 | ms/batch  4.75 | loss  4.92 | ppl   137.35\n",
            "| epoch  50 |   800/ 1155 batches | lr 1.62 | ms/batch  4.75 | loss  4.91 | ppl   135.18\n",
            "| epoch  50 |  1000/ 1155 batches | lr 1.62 | ms/batch  4.76 | loss  4.92 | ppl   136.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time:  5.67s | valid loss  5.33 | valid ppl   207.19\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
        "      f'test ppl {test_ppl:8.2f}')\n",
        "print('=' * 89)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FbM0c93_J03",
        "outputId": "e04e513f-d8ec-4b7f-e199-96b28eb686fd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  5.24 | test ppl   188.99\n",
            "=========================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:774: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        }
      ]
    }
  ]
}