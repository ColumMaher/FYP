{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from typing import Tuple\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.utils.data import dataset\n",
        "import math\n",
        "import torchtext\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "ZxVrhiF0Mh00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchdata"
      ],
      "metadata": {
        "id": "pD7cbU9dwHYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import PennTreebank "
      ],
      "metadata": {
        "id": "YOX72cCBS71c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter = PennTreebank(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = torchtext.vocab.build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])"
      ],
      "metadata": {
        "id": "0FioMPfTdBJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "  \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "  data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
      ],
      "metadata": {
        "id": "S5Ae9CdAyZQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter, val_iter, test_iter = PennTreebank()\n",
        "train_data= data_process(train_iter)\n",
        "val_data= data_process(val_iter)\n",
        "test_data= data_process(test_iter)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "R5cZes9WiHFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device)"
      ],
      "metadata": {
        "id": "VVTtf0cPycRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ],
      "metadata": {
        "id": "Vz5mzO8wycub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0-OPgEJ0lgm",
        "outputId": "38131f5b-0f66-4abc-c661-c4fa9f6fceb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([9892,    4,   31,   16, 1925,  178,    3, 2295,    5,   42, 3412, 1611,\n",
            "           1,  730, 3722, 5722, 2124, 1678,  796,  829], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bptt = 35\n",
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target"
      ],
      "metadata": {
        "id": "jN-g8V7ByvRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, tie_weights):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
        "                    batch_first=True)\n",
        "    self.linear = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    if tie_weights:\n",
        "          assert embedding_dim == hidden_dim, 'cannot tie, check dims'\n",
        "          self.linear.weight = self.embedding.weight\n",
        "    self.init_weights()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x is a batch of input sequences\n",
        "    x = self.embedding(x)\n",
        "    x, _ = self.lstm(x)\n",
        "    x = self.linear(x)\n",
        "    return x\n",
        "\n",
        "  def init_weights(self):\n",
        "    init_range_emb = 0.1\n",
        "    init_range_other = 1/math.sqrt(self.hidden_dim)\n",
        "    self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
        "    self.linear.weight.data.uniform_(-init_range_other, init_range_other)\n",
        "    self.linear.bias.data.zero_()\n",
        "    for i in range(self.num_layers):\n",
        "        self.lstm.all_weights[i][0] = torch.FloatTensor(self.embedding_dim,\n",
        "                self.hidden_dim).uniform_(-init_range_other, init_range_other) \n",
        "        self.lstm.all_weights[i][1] = torch.FloatTensor(self.hidden_dim, \n",
        "                self.hidden_dim).uniform_(-init_range_other, init_range_other)\n",
        "                 "
      ],
      "metadata": {
        "id": "xYF8ErZ2zE6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 200\n",
        "hidden_dim = 200\n",
        "num_layers = 2\n",
        "tie_weights = True\n",
        "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, tie_weights)"
      ],
      "metadata": {
        "id": "5vPNWmqoUWmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0  # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "def train(model: nn.Module) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "\n",
        "    num_batches = len(train_data) // bptt\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        #seq_len = data.size(0)\n",
        "        output = model(data)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ],
      "metadata": {
        "id": "VbUtSuzbUbea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(eval_data, i)\n",
        "            seq_len = data.size(0)\n",
        "            output = model(data)\n",
        "            output_flat = output.view(-1, vocab_size)\n",
        "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(eval_data) - 1)"
      ],
      "metadata": {
        "id": "5SjEJ3OY91mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 3\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model)\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = copy.deepcopy(model)\n",
        "\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RQth0cB_Jj8",
        "outputId": "a6bdb232-9b52-41f1-f109-9aab94e3a897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/26411 batches | lr 5.00 | ms/batch 61.93 | loss  7.06 | ppl  1163.18\n",
            "| epoch   1 |   400/26411 batches | lr 5.00 | ms/batch 60.25 | loss  6.59 | ppl   724.39\n",
            "| epoch   1 |   600/26411 batches | lr 5.00 | ms/batch 59.65 | loss  6.48 | ppl   652.72\n",
            "| epoch   1 |   800/26411 batches | lr 5.00 | ms/batch 59.92 | loss  6.28 | ppl   532.78\n",
            "| epoch   1 |  1000/26411 batches | lr 5.00 | ms/batch 60.27 | loss  6.16 | ppl   472.22\n",
            "| epoch   1 |  1200/26411 batches | lr 5.00 | ms/batch 60.87 | loss  6.07 | ppl   433.82\n",
            "| epoch   1 |  1400/26411 batches | lr 5.00 | ms/batch 60.76 | loss  6.08 | ppl   435.98\n",
            "| epoch   1 |  1600/26411 batches | lr 5.00 | ms/batch 64.71 | loss  5.99 | ppl   400.76\n",
            "| epoch   1 |  1800/26411 batches | lr 5.00 | ms/batch 59.89 | loss  5.99 | ppl   400.10\n",
            "| epoch   1 |  2000/26411 batches | lr 5.00 | ms/batch 60.65 | loss  5.85 | ppl   348.62\n",
            "| epoch   1 |  2200/26411 batches | lr 5.00 | ms/batch 60.17 | loss  5.67 | ppl   289.36\n",
            "| epoch   1 |  2400/26411 batches | lr 5.00 | ms/batch 61.12 | loss  5.82 | ppl   338.47\n",
            "| epoch   1 |  2600/26411 batches | lr 5.00 | ms/batch 61.49 | loss  5.83 | ppl   340.15\n",
            "| epoch   1 |  2800/26411 batches | lr 5.00 | ms/batch 61.07 | loss  5.59 | ppl   268.11\n",
            "| epoch   1 |  3000/26411 batches | lr 5.00 | ms/batch 60.55 | loss  5.72 | ppl   305.79\n",
            "| epoch   1 |  3200/26411 batches | lr 5.00 | ms/batch 60.47 | loss  5.60 | ppl   269.18\n",
            "| epoch   1 |  3400/26411 batches | lr 5.00 | ms/batch 60.40 | loss  5.73 | ppl   307.88\n",
            "| epoch   1 |  3600/26411 batches | lr 5.00 | ms/batch 60.78 | loss  5.85 | ppl   347.25\n",
            "| epoch   1 |  3800/26411 batches | lr 5.00 | ms/batch 60.56 | loss  5.67 | ppl   291.29\n",
            "| epoch   1 |  4000/26411 batches | lr 5.00 | ms/batch 60.71 | loss  5.58 | ppl   265.78\n",
            "| epoch   1 |  4200/26411 batches | lr 5.00 | ms/batch 60.29 | loss  5.50 | ppl   245.17\n",
            "| epoch   1 |  4400/26411 batches | lr 5.00 | ms/batch 60.57 | loss  5.45 | ppl   233.54\n",
            "| epoch   1 |  4600/26411 batches | lr 5.00 | ms/batch 65.72 | loss  5.68 | ppl   292.54\n",
            "| epoch   1 |  4800/26411 batches | lr 5.00 | ms/batch 60.19 | loss  5.38 | ppl   216.33\n",
            "| epoch   1 |  5000/26411 batches | lr 5.00 | ms/batch 60.34 | loss  5.64 | ppl   281.41\n",
            "| epoch   1 |  5200/26411 batches | lr 5.00 | ms/batch 60.69 | loss  5.64 | ppl   281.28\n",
            "| epoch   1 |  5400/26411 batches | lr 5.00 | ms/batch 60.48 | loss  5.63 | ppl   279.80\n",
            "| epoch   1 |  5600/26411 batches | lr 5.00 | ms/batch 61.14 | loss  5.58 | ppl   265.88\n",
            "| epoch   1 |  5800/26411 batches | lr 5.00 | ms/batch 60.34 | loss  5.60 | ppl   269.37\n",
            "| epoch   1 |  6000/26411 batches | lr 5.00 | ms/batch 60.60 | loss  5.48 | ppl   239.66\n",
            "| epoch   1 |  6200/26411 batches | lr 5.00 | ms/batch 60.38 | loss  5.42 | ppl   226.22\n",
            "| epoch   1 |  6400/26411 batches | lr 5.00 | ms/batch 60.59 | loss  5.53 | ppl   252.28\n",
            "| epoch   1 |  6600/26411 batches | lr 5.00 | ms/batch 60.94 | loss  5.50 | ppl   243.75\n",
            "| epoch   1 |  6800/26411 batches | lr 5.00 | ms/batch 60.32 | loss  5.59 | ppl   268.01\n",
            "| epoch   1 |  7000/26411 batches | lr 5.00 | ms/batch 60.60 | loss  5.40 | ppl   221.63\n",
            "| epoch   1 |  7200/26411 batches | lr 5.00 | ms/batch 60.28 | loss  5.37 | ppl   214.50\n",
            "| epoch   1 |  7400/26411 batches | lr 5.00 | ms/batch 61.01 | loss  5.58 | ppl   265.86\n",
            "| epoch   1 |  7600/26411 batches | lr 5.00 | ms/batch 65.57 | loss  5.53 | ppl   251.53\n",
            "| epoch   1 |  7800/26411 batches | lr 5.00 | ms/batch 60.17 | loss  5.35 | ppl   211.11\n",
            "| epoch   1 |  8000/26411 batches | lr 5.00 | ms/batch 60.21 | loss  5.45 | ppl   232.25\n",
            "| epoch   1 |  8200/26411 batches | lr 5.00 | ms/batch 60.38 | loss  5.09 | ppl   162.12\n",
            "| epoch   1 |  8400/26411 batches | lr 5.00 | ms/batch 60.16 | loss  5.32 | ppl   203.80\n",
            "| epoch   1 |  8600/26411 batches | lr 5.00 | ms/batch 59.96 | loss  5.59 | ppl   268.58\n",
            "| epoch   1 |  8800/26411 batches | lr 5.00 | ms/batch 59.89 | loss  4.92 | ppl   137.16\n",
            "| epoch   1 |  9000/26411 batches | lr 5.00 | ms/batch 60.11 | loss  5.58 | ppl   264.71\n",
            "| epoch   1 |  9200/26411 batches | lr 5.00 | ms/batch 59.92 | loss  5.51 | ppl   248.27\n",
            "| epoch   1 |  9400/26411 batches | lr 5.00 | ms/batch 60.01 | loss  5.46 | ppl   234.93\n",
            "| epoch   1 |  9600/26411 batches | lr 5.00 | ms/batch 60.03 | loss  5.45 | ppl   231.73\n",
            "| epoch   1 |  9800/26411 batches | lr 5.00 | ms/batch 59.88 | loss  5.41 | ppl   224.29\n",
            "| epoch   1 | 10000/26411 batches | lr 5.00 | ms/batch 59.86 | loss  5.29 | ppl   199.27\n",
            "| epoch   1 | 10200/26411 batches | lr 5.00 | ms/batch 59.59 | loss  5.27 | ppl   194.66\n",
            "| epoch   1 | 10400/26411 batches | lr 5.00 | ms/batch 59.82 | loss  5.09 | ppl   162.40\n",
            "| epoch   1 | 10600/26411 batches | lr 5.00 | ms/batch 65.05 | loss  5.13 | ppl   169.62\n",
            "| epoch   1 | 10800/26411 batches | lr 5.00 | ms/batch 59.57 | loss  5.38 | ppl   216.10\n",
            "| epoch   1 | 11000/26411 batches | lr 5.00 | ms/batch 59.51 | loss  5.24 | ppl   189.19\n",
            "| epoch   1 | 11200/26411 batches | lr 5.00 | ms/batch 59.53 | loss  5.42 | ppl   225.97\n",
            "| epoch   1 | 11400/26411 batches | lr 5.00 | ms/batch 59.57 | loss  5.43 | ppl   227.25\n",
            "| epoch   1 | 11600/26411 batches | lr 5.00 | ms/batch 59.96 | loss  5.57 | ppl   263.52\n",
            "| epoch   1 | 11800/26411 batches | lr 5.00 | ms/batch 60.27 | loss  4.99 | ppl   146.32\n",
            "| epoch   1 | 12000/26411 batches | lr 5.00 | ms/batch 59.76 | loss  5.11 | ppl   164.97\n",
            "| epoch   1 | 12200/26411 batches | lr 5.00 | ms/batch 59.61 | loss  5.12 | ppl   167.05\n",
            "| epoch   1 | 12400/26411 batches | lr 5.00 | ms/batch 59.96 | loss  5.26 | ppl   192.22\n",
            "| epoch   1 | 12600/26411 batches | lr 5.00 | ms/batch 59.80 | loss  5.36 | ppl   213.37\n",
            "| epoch   1 | 12800/26411 batches | lr 5.00 | ms/batch 59.61 | loss  5.01 | ppl   149.79\n",
            "| epoch   1 | 13000/26411 batches | lr 5.00 | ms/batch 59.96 | loss  5.02 | ppl   151.78\n",
            "| epoch   1 | 13200/26411 batches | lr 5.00 | ms/batch 60.27 | loss  5.52 | ppl   249.57\n",
            "| epoch   1 | 13400/26411 batches | lr 5.00 | ms/batch 59.85 | loss  5.39 | ppl   219.68\n",
            "| epoch   1 | 13600/26411 batches | lr 5.00 | ms/batch 64.81 | loss  5.44 | ppl   229.84\n",
            "| epoch   1 | 13800/26411 batches | lr 5.00 | ms/batch 59.26 | loss  4.72 | ppl   112.35\n",
            "| epoch   1 | 14000/26411 batches | lr 5.00 | ms/batch 59.19 | loss  5.21 | ppl   183.74\n",
            "| epoch   1 | 14200/26411 batches | lr 5.00 | ms/batch 58.35 | loss  5.15 | ppl   171.69\n",
            "| epoch   1 | 14400/26411 batches | lr 5.00 | ms/batch 58.88 | loss  4.95 | ppl   141.59\n",
            "| epoch   1 | 14600/26411 batches | lr 5.00 | ms/batch 58.88 | loss  5.04 | ppl   155.24\n",
            "| epoch   1 | 14800/26411 batches | lr 5.00 | ms/batch 58.96 | loss  4.95 | ppl   141.37\n",
            "| epoch   1 | 15000/26411 batches | lr 5.00 | ms/batch 58.96 | loss  5.54 | ppl   254.39\n",
            "| epoch   1 | 15200/26411 batches | lr 5.00 | ms/batch 58.81 | loss  5.38 | ppl   217.64\n",
            "| epoch   1 | 15400/26411 batches | lr 5.00 | ms/batch 58.60 | loss  5.14 | ppl   170.02\n",
            "| epoch   1 | 15600/26411 batches | lr 5.00 | ms/batch 58.06 | loss  5.06 | ppl   157.00\n",
            "| epoch   1 | 15800/26411 batches | lr 5.00 | ms/batch 58.32 | loss  4.68 | ppl   107.33\n",
            "| epoch   1 | 16000/26411 batches | lr 5.00 | ms/batch 58.95 | loss  5.15 | ppl   173.21\n",
            "| epoch   1 | 16200/26411 batches | lr 5.00 | ms/batch 57.99 | loss  5.07 | ppl   159.03\n",
            "| epoch   1 | 16400/26411 batches | lr 5.00 | ms/batch 58.52 | loss  4.65 | ppl   105.11\n",
            "| epoch   1 | 16600/26411 batches | lr 5.00 | ms/batch 63.43 | loss  5.18 | ppl   178.03\n",
            "| epoch   1 | 16800/26411 batches | lr 5.00 | ms/batch 58.17 | loss  5.21 | ppl   183.93\n",
            "| epoch   1 | 17000/26411 batches | lr 5.00 | ms/batch 58.43 | loss  5.34 | ppl   208.81\n",
            "| epoch   1 | 17200/26411 batches | lr 5.00 | ms/batch 58.22 | loss  5.33 | ppl   206.12\n",
            "| epoch   1 | 17400/26411 batches | lr 5.00 | ms/batch 58.65 | loss  5.19 | ppl   179.97\n",
            "| epoch   1 | 17600/26411 batches | lr 5.00 | ms/batch 58.83 | loss  5.17 | ppl   176.10\n",
            "| epoch   1 | 17800/26411 batches | lr 5.00 | ms/batch 58.76 | loss  5.15 | ppl   172.18\n",
            "| epoch   1 | 18000/26411 batches | lr 5.00 | ms/batch 58.43 | loss  5.06 | ppl   158.31\n",
            "| epoch   1 | 18200/26411 batches | lr 5.00 | ms/batch 57.89 | loss  5.22 | ppl   185.04\n",
            "| epoch   1 | 18400/26411 batches | lr 5.00 | ms/batch 57.77 | loss  4.98 | ppl   145.94\n",
            "| epoch   1 | 18600/26411 batches | lr 5.00 | ms/batch 58.42 | loss  5.16 | ppl   174.29\n",
            "| epoch   1 | 18800/26411 batches | lr 5.00 | ms/batch 58.44 | loss  5.01 | ppl   149.19\n",
            "| epoch   1 | 19000/26411 batches | lr 5.00 | ms/batch 58.08 | loss  5.25 | ppl   189.91\n",
            "| epoch   1 | 19200/26411 batches | lr 5.00 | ms/batch 57.92 | loss  5.34 | ppl   208.59\n",
            "| epoch   1 | 19400/26411 batches | lr 5.00 | ms/batch 58.04 | loss  5.45 | ppl   232.80\n",
            "| epoch   1 | 19600/26411 batches | lr 5.00 | ms/batch 63.15 | loss  5.28 | ppl   196.06\n",
            "| epoch   1 | 19800/26411 batches | lr 5.00 | ms/batch 58.68 | loss  4.93 | ppl   138.07\n",
            "| epoch   1 | 20000/26411 batches | lr 5.00 | ms/batch 58.72 | loss  5.16 | ppl   174.04\n",
            "| epoch   1 | 20200/26411 batches | lr 5.00 | ms/batch 58.24 | loss  5.26 | ppl   192.77\n",
            "| epoch   1 | 20400/26411 batches | lr 5.00 | ms/batch 57.77 | loss  5.10 | ppl   164.08\n",
            "| epoch   1 | 20600/26411 batches | lr 5.00 | ms/batch 58.38 | loss  5.14 | ppl   171.39\n",
            "| epoch   1 | 20800/26411 batches | lr 5.00 | ms/batch 58.17 | loss  5.04 | ppl   155.14\n",
            "| epoch   1 | 21000/26411 batches | lr 5.00 | ms/batch 57.96 | loss  4.92 | ppl   137.61\n",
            "| epoch   1 | 21200/26411 batches | lr 5.00 | ms/batch 58.28 | loss  5.07 | ppl   159.07\n",
            "| epoch   1 | 21400/26411 batches | lr 5.00 | ms/batch 58.18 | loss  5.39 | ppl   219.42\n",
            "| epoch   1 | 21600/26411 batches | lr 5.00 | ms/batch 58.08 | loss  5.18 | ppl   178.43\n",
            "| epoch   1 | 21800/26411 batches | lr 5.00 | ms/batch 58.48 | loss  5.00 | ppl   148.71\n",
            "| epoch   1 | 22000/26411 batches | lr 5.00 | ms/batch 58.20 | loss  4.73 | ppl   112.74\n",
            "| epoch   1 | 22200/26411 batches | lr 5.00 | ms/batch 58.05 | loss  4.89 | ppl   132.49\n",
            "| epoch   1 | 22400/26411 batches | lr 5.00 | ms/batch 57.58 | loss  5.00 | ppl   148.02\n",
            "| epoch   1 | 22600/26411 batches | lr 5.00 | ms/batch 63.18 | loss  5.18 | ppl   178.23\n",
            "| epoch   1 | 22800/26411 batches | lr 5.00 | ms/batch 58.84 | loss  5.12 | ppl   166.84\n",
            "| epoch   1 | 23000/26411 batches | lr 5.00 | ms/batch 58.95 | loss  5.20 | ppl   181.80\n",
            "| epoch   1 | 23200/26411 batches | lr 5.00 | ms/batch 58.14 | loss  5.26 | ppl   193.12\n",
            "| epoch   1 | 23400/26411 batches | lr 5.00 | ms/batch 57.66 | loss  5.28 | ppl   195.78\n",
            "| epoch   1 | 23600/26411 batches | lr 5.00 | ms/batch 57.62 | loss  4.88 | ppl   131.21\n",
            "| epoch   1 | 23800/26411 batches | lr 5.00 | ms/batch 58.21 | loss  5.01 | ppl   149.95\n",
            "| epoch   1 | 24000/26411 batches | lr 5.00 | ms/batch 58.25 | loss  4.72 | ppl   112.63\n",
            "| epoch   1 | 24200/26411 batches | lr 5.00 | ms/batch 57.88 | loss  4.76 | ppl   116.27\n",
            "| epoch   1 | 24400/26411 batches | lr 5.00 | ms/batch 58.18 | loss  5.19 | ppl   179.08\n",
            "| epoch   1 | 24600/26411 batches | lr 5.00 | ms/batch 57.67 | loss  5.13 | ppl   169.03\n",
            "| epoch   1 | 24800/26411 batches | lr 5.00 | ms/batch 58.27 | loss  4.71 | ppl   111.09\n",
            "| epoch   1 | 25000/26411 batches | lr 5.00 | ms/batch 58.00 | loss  4.72 | ppl   112.10\n",
            "| epoch   1 | 25200/26411 batches | lr 5.00 | ms/batch 58.32 | loss  4.78 | ppl   119.33\n",
            "| epoch   1 | 25400/26411 batches | lr 5.00 | ms/batch 60.92 | loss  5.03 | ppl   153.69\n",
            "| epoch   1 | 25600/26411 batches | lr 5.00 | ms/batch 60.68 | loss  5.15 | ppl   171.93\n",
            "| epoch   1 | 25800/26411 batches | lr 5.00 | ms/batch 59.01 | loss  4.98 | ppl   145.91\n",
            "| epoch   1 | 26000/26411 batches | lr 5.00 | ms/batch 58.73 | loss  5.24 | ppl   188.55\n",
            "| epoch   1 | 26200/26411 batches | lr 5.00 | ms/batch 57.87 | loss  5.10 | ppl   164.78\n",
            "| epoch   1 | 26400/26411 batches | lr 5.00 | ms/batch 58.60 | loss  4.82 | ppl   123.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 1591.40s | valid loss  5.20 | valid ppl   181.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/26411 batches | lr 4.75 | ms/batch 58.40 | loss  5.05 | ppl   156.08\n",
            "| epoch   2 |   400/26411 batches | lr 4.75 | ms/batch 58.12 | loss  5.20 | ppl   180.39\n",
            "| epoch   2 |   600/26411 batches | lr 4.75 | ms/batch 58.20 | loss  5.29 | ppl   198.11\n",
            "| epoch   2 |   800/26411 batches | lr 4.75 | ms/batch 57.89 | loss  4.98 | ppl   145.01\n",
            "| epoch   2 |  1000/26411 batches | lr 4.75 | ms/batch 58.20 | loss  4.96 | ppl   142.98\n",
            "| epoch   2 |  1200/26411 batches | lr 4.75 | ms/batch 57.54 | loss  5.15 | ppl   172.37\n",
            "| epoch   2 |  1400/26411 batches | lr 4.75 | ms/batch 58.03 | loss  5.20 | ppl   181.05\n",
            "| epoch   2 |  1600/26411 batches | lr 4.75 | ms/batch 59.93 | loss  5.07 | ppl   159.91\n",
            "| epoch   2 |  1800/26411 batches | lr 4.75 | ms/batch 61.09 | loss  5.09 | ppl   161.69\n",
            "| epoch   2 |  2000/26411 batches | lr 4.75 | ms/batch 57.57 | loss  4.80 | ppl   121.02\n",
            "| epoch   2 |  2200/26411 batches | lr 4.75 | ms/batch 57.60 | loss  4.68 | ppl   107.65\n",
            "| epoch   2 |  2400/26411 batches | lr 4.75 | ms/batch 57.71 | loss  4.97 | ppl   143.91\n",
            "| epoch   2 |  2600/26411 batches | lr 4.75 | ms/batch 57.45 | loss  5.08 | ppl   161.32\n",
            "| epoch   2 |  2800/26411 batches | lr 4.75 | ms/batch 57.10 | loss  4.73 | ppl   113.84\n",
            "| epoch   2 |  3000/26411 batches | lr 4.75 | ms/batch 57.70 | loss  5.01 | ppl   149.82\n",
            "| epoch   2 |  3200/26411 batches | lr 4.75 | ms/batch 58.10 | loss  4.80 | ppl   121.52\n",
            "| epoch   2 |  3400/26411 batches | lr 4.75 | ms/batch 58.16 | loss  5.12 | ppl   167.86\n",
            "| epoch   2 |  3600/26411 batches | lr 4.75 | ms/batch 57.20 | loss  5.29 | ppl   197.43\n",
            "| epoch   2 |  3800/26411 batches | lr 4.75 | ms/batch 57.40 | loss  5.06 | ppl   158.38\n",
            "| epoch   2 |  4000/26411 batches | lr 4.75 | ms/batch 56.96 | loss  4.97 | ppl   143.57\n",
            "| epoch   2 |  4200/26411 batches | lr 4.75 | ms/batch 57.31 | loss  4.79 | ppl   120.52\n",
            "| epoch   2 |  4400/26411 batches | lr 4.75 | ms/batch 58.03 | loss  4.72 | ppl   111.70\n",
            "| epoch   2 |  4600/26411 batches | lr 4.75 | ms/batch 62.16 | loss  5.04 | ppl   155.02\n",
            "| epoch   2 |  4800/26411 batches | lr 4.75 | ms/batch 56.96 | loss  4.72 | ppl   112.00\n",
            "| epoch   2 |  5000/26411 batches | lr 4.75 | ms/batch 57.17 | loss  5.17 | ppl   175.62\n",
            "| epoch   2 |  5200/26411 batches | lr 4.75 | ms/batch 56.71 | loss  5.05 | ppl   156.45\n",
            "| epoch   2 |  5400/26411 batches | lr 4.75 | ms/batch 57.40 | loss  5.16 | ppl   173.59\n",
            "| epoch   2 |  5600/26411 batches | lr 4.75 | ms/batch 56.88 | loss  5.05 | ppl   156.12\n",
            "| epoch   2 |  5800/26411 batches | lr 4.75 | ms/batch 57.20 | loss  4.98 | ppl   145.51\n",
            "| epoch   2 |  6000/26411 batches | lr 4.75 | ms/batch 57.40 | loss  4.85 | ppl   127.28\n",
            "| epoch   2 |  6200/26411 batches | lr 4.75 | ms/batch 56.67 | loss  4.80 | ppl   121.63\n",
            "| epoch   2 |  6400/26411 batches | lr 4.75 | ms/batch 57.36 | loss  5.03 | ppl   153.37\n",
            "| epoch   2 |  6600/26411 batches | lr 4.75 | ms/batch 57.75 | loss  5.00 | ppl   148.73\n",
            "| epoch   2 |  6800/26411 batches | lr 4.75 | ms/batch 57.13 | loss  5.16 | ppl   174.24\n",
            "| epoch   2 |  7000/26411 batches | lr 4.75 | ms/batch 57.24 | loss  4.96 | ppl   142.38\n",
            "| epoch   2 |  7200/26411 batches | lr 4.75 | ms/batch 57.70 | loss  4.93 | ppl   139.06\n",
            "| epoch   2 |  7400/26411 batches | lr 4.75 | ms/batch 61.17 | loss  5.05 | ppl   156.22\n",
            "| epoch   2 |  7600/26411 batches | lr 4.75 | ms/batch 59.93 | loss  4.95 | ppl   141.71\n",
            "| epoch   2 |  7800/26411 batches | lr 4.75 | ms/batch 58.51 | loss  4.84 | ppl   126.81\n",
            "| epoch   2 |  8000/26411 batches | lr 4.75 | ms/batch 57.63 | loss  5.04 | ppl   154.34\n",
            "| epoch   2 |  8200/26411 batches | lr 4.75 | ms/batch 57.42 | loss  4.58 | ppl    97.57\n",
            "| epoch   2 |  8400/26411 batches | lr 4.75 | ms/batch 57.42 | loss  4.80 | ppl   121.02\n",
            "| epoch   2 |  8600/26411 batches | lr 4.75 | ms/batch 58.30 | loss  5.17 | ppl   175.68\n",
            "| epoch   2 |  8800/26411 batches | lr 4.75 | ms/batch 58.13 | loss  4.36 | ppl    78.06\n",
            "| epoch   2 |  9000/26411 batches | lr 4.75 | ms/batch 56.99 | loss  5.18 | ppl   177.86\n",
            "| epoch   2 |  9200/26411 batches | lr 4.75 | ms/batch 57.28 | loss  5.09 | ppl   162.52\n",
            "| epoch   2 |  9400/26411 batches | lr 4.75 | ms/batch 57.12 | loss  5.08 | ppl   161.46\n",
            "| epoch   2 |  9600/26411 batches | lr 4.75 | ms/batch 57.36 | loss  5.06 | ppl   157.32\n",
            "| epoch   2 |  9800/26411 batches | lr 4.75 | ms/batch 57.32 | loss  4.99 | ppl   146.73\n",
            "| epoch   2 | 10000/26411 batches | lr 4.75 | ms/batch 57.07 | loss  4.91 | ppl   135.86\n",
            "| epoch   2 | 10200/26411 batches | lr 4.75 | ms/batch 58.77 | loss  4.87 | ppl   130.81\n",
            "| epoch   2 | 10400/26411 batches | lr 4.75 | ms/batch 60.47 | loss  4.70 | ppl   109.61\n",
            "| epoch   2 | 10600/26411 batches | lr 4.75 | ms/batch 57.69 | loss  4.74 | ppl   114.37\n",
            "| epoch   2 | 10800/26411 batches | lr 4.75 | ms/batch 57.51 | loss  5.06 | ppl   157.01\n",
            "| epoch   2 | 11000/26411 batches | lr 4.75 | ms/batch 57.20 | loss  4.88 | ppl   131.14\n",
            "| epoch   2 | 11200/26411 batches | lr 4.75 | ms/batch 57.20 | loss  5.06 | ppl   157.24\n",
            "| epoch   2 | 11400/26411 batches | lr 4.75 | ms/batch 57.80 | loss  4.95 | ppl   141.60\n",
            "| epoch   2 | 11600/26411 batches | lr 4.75 | ms/batch 57.85 | loss  5.22 | ppl   184.55\n",
            "| epoch   2 | 11800/26411 batches | lr 4.75 | ms/batch 57.60 | loss  4.51 | ppl    90.50\n",
            "| epoch   2 | 12000/26411 batches | lr 4.75 | ms/batch 57.04 | loss  4.66 | ppl   105.23\n",
            "| epoch   2 | 12200/26411 batches | lr 4.75 | ms/batch 57.59 | loss  4.68 | ppl   107.46\n",
            "| epoch   2 | 12400/26411 batches | lr 4.75 | ms/batch 58.04 | loss  4.84 | ppl   126.00\n",
            "| epoch   2 | 12600/26411 batches | lr 4.75 | ms/batch 58.14 | loss  5.04 | ppl   153.76\n",
            "| epoch   2 | 12800/26411 batches | lr 4.75 | ms/batch 58.60 | loss  4.62 | ppl   101.30\n",
            "| epoch   2 | 13000/26411 batches | lr 4.75 | ms/batch 58.65 | loss  4.57 | ppl    96.76\n",
            "| epoch   2 | 13200/26411 batches | lr 4.75 | ms/batch 62.44 | loss  5.21 | ppl   183.38\n",
            "| epoch   2 | 13400/26411 batches | lr 4.75 | ms/batch 57.77 | loss  5.08 | ppl   161.49\n",
            "| epoch   2 | 13600/26411 batches | lr 4.75 | ms/batch 58.15 | loss  5.12 | ppl   166.93\n",
            "| epoch   2 | 13800/26411 batches | lr 4.75 | ms/batch 58.47 | loss  4.31 | ppl    74.40\n",
            "| epoch   2 | 14000/26411 batches | lr 4.75 | ms/batch 58.38 | loss  4.87 | ppl   130.45\n",
            "| epoch   2 | 14200/26411 batches | lr 4.75 | ms/batch 57.62 | loss  4.79 | ppl   120.23\n",
            "| epoch   2 | 14400/26411 batches | lr 4.75 | ms/batch 57.77 | loss  4.58 | ppl    97.60\n",
            "| epoch   2 | 14600/26411 batches | lr 4.75 | ms/batch 58.19 | loss  4.66 | ppl   105.78\n",
            "| epoch   2 | 14800/26411 batches | lr 4.75 | ms/batch 58.78 | loss  4.60 | ppl    99.01\n",
            "| epoch   2 | 15000/26411 batches | lr 4.75 | ms/batch 58.39 | loss  5.22 | ppl   185.80\n",
            "| epoch   2 | 15200/26411 batches | lr 4.75 | ms/batch 58.37 | loss  5.10 | ppl   163.83\n",
            "| epoch   2 | 15400/26411 batches | lr 4.75 | ms/batch 58.83 | loss  4.81 | ppl   123.26\n",
            "| epoch   2 | 15600/26411 batches | lr 4.75 | ms/batch 58.09 | loss  4.77 | ppl   118.12\n",
            "| epoch   2 | 15800/26411 batches | lr 4.75 | ms/batch 62.76 | loss  4.30 | ppl    73.87\n",
            "| epoch   2 | 16000/26411 batches | lr 4.75 | ms/batch 58.04 | loss  4.80 | ppl   121.98\n",
            "| epoch   2 | 16200/26411 batches | lr 4.75 | ms/batch 57.75 | loss  4.76 | ppl   117.14\n",
            "| epoch   2 | 16400/26411 batches | lr 4.75 | ms/batch 57.45 | loss  4.23 | ppl    68.61\n",
            "| epoch   2 | 16600/26411 batches | lr 4.75 | ms/batch 57.34 | loss  4.87 | ppl   130.66\n",
            "| epoch   2 | 16800/26411 batches | lr 4.75 | ms/batch 57.70 | loss  4.93 | ppl   138.02\n",
            "| epoch   2 | 17000/26411 batches | lr 4.75 | ms/batch 57.56 | loss  5.07 | ppl   159.10\n",
            "| epoch   2 | 17200/26411 batches | lr 4.75 | ms/batch 58.41 | loss  5.03 | ppl   153.68\n",
            "| epoch   2 | 17400/26411 batches | lr 4.75 | ms/batch 57.53 | loss  4.89 | ppl   132.65\n",
            "| epoch   2 | 17600/26411 batches | lr 4.75 | ms/batch 57.61 | loss  4.82 | ppl   124.36\n",
            "| epoch   2 | 17800/26411 batches | lr 4.75 | ms/batch 58.03 | loss  4.83 | ppl   125.72\n",
            "| epoch   2 | 18000/26411 batches | lr 4.75 | ms/batch 57.76 | loss  4.77 | ppl   118.43\n",
            "| epoch   2 | 18200/26411 batches | lr 4.75 | ms/batch 58.42 | loss  4.95 | ppl   141.66\n",
            "| epoch   2 | 18400/26411 batches | lr 4.75 | ms/batch 57.69 | loss  4.66 | ppl   105.63\n",
            "| epoch   2 | 18600/26411 batches | lr 4.75 | ms/batch 62.59 | loss  4.83 | ppl   125.53\n",
            "| epoch   2 | 18800/26411 batches | lr 4.75 | ms/batch 57.15 | loss  4.72 | ppl   112.21\n",
            "| epoch   2 | 19000/26411 batches | lr 4.75 | ms/batch 57.74 | loss  4.93 | ppl   138.36\n",
            "| epoch   2 | 19200/26411 batches | lr 4.75 | ms/batch 57.93 | loss  5.03 | ppl   152.66\n",
            "| epoch   2 | 19400/26411 batches | lr 4.75 | ms/batch 57.40 | loss  5.19 | ppl   179.26\n",
            "| epoch   2 | 19600/26411 batches | lr 4.75 | ms/batch 58.16 | loss  5.00 | ppl   148.97\n",
            "| epoch   2 | 19800/26411 batches | lr 4.75 | ms/batch 58.27 | loss  4.65 | ppl   104.35\n",
            "| epoch   2 | 20000/26411 batches | lr 4.75 | ms/batch 57.97 | loss  4.88 | ppl   131.64\n",
            "| epoch   2 | 20200/26411 batches | lr 4.75 | ms/batch 57.61 | loss  5.02 | ppl   151.48\n",
            "| epoch   2 | 20400/26411 batches | lr 4.75 | ms/batch 57.85 | loss  4.81 | ppl   122.96\n",
            "| epoch   2 | 20600/26411 batches | lr 4.75 | ms/batch 56.92 | loss  4.90 | ppl   133.84\n",
            "| epoch   2 | 20800/26411 batches | lr 4.75 | ms/batch 57.72 | loss  4.80 | ppl   121.85\n",
            "| epoch   2 | 21000/26411 batches | lr 4.75 | ms/batch 57.75 | loss  4.64 | ppl   103.84\n",
            "| epoch   2 | 21200/26411 batches | lr 4.75 | ms/batch 57.53 | loss  4.76 | ppl   116.19\n",
            "| epoch   2 | 21400/26411 batches | lr 4.75 | ms/batch 62.02 | loss  5.10 | ppl   163.78\n",
            "| epoch   2 | 21600/26411 batches | lr 4.75 | ms/batch 56.82 | loss  4.91 | ppl   135.72\n",
            "| epoch   2 | 21800/26411 batches | lr 4.75 | ms/batch 56.88 | loss  4.66 | ppl   105.67\n",
            "| epoch   2 | 22000/26411 batches | lr 4.75 | ms/batch 57.10 | loss  4.39 | ppl    80.75\n",
            "| epoch   2 | 22200/26411 batches | lr 4.75 | ms/batch 57.08 | loss  4.63 | ppl   102.31\n",
            "| epoch   2 | 22400/26411 batches | lr 4.75 | ms/batch 56.97 | loss  4.74 | ppl   113.92\n",
            "| epoch   2 | 22600/26411 batches | lr 4.75 | ms/batch 56.49 | loss  4.91 | ppl   135.93\n",
            "| epoch   2 | 22800/26411 batches | lr 4.75 | ms/batch 57.17 | loss  4.85 | ppl   127.56\n",
            "| epoch   2 | 23000/26411 batches | lr 4.75 | ms/batch 56.90 | loss  4.95 | ppl   140.84\n",
            "| epoch   2 | 23200/26411 batches | lr 4.75 | ms/batch 58.10 | loss  5.00 | ppl   147.71\n",
            "| epoch   2 | 23400/26411 batches | lr 4.75 | ms/batch 57.18 | loss  5.04 | ppl   154.74\n",
            "| epoch   2 | 23600/26411 batches | lr 4.75 | ms/batch 56.84 | loss  4.54 | ppl    94.10\n",
            "| epoch   2 | 23800/26411 batches | lr 4.75 | ms/batch 57.18 | loss  4.70 | ppl   109.52\n",
            "| epoch   2 | 24000/26411 batches | lr 4.75 | ms/batch 62.38 | loss  4.47 | ppl    87.27\n",
            "| epoch   2 | 24200/26411 batches | lr 4.75 | ms/batch 57.74 | loss  4.49 | ppl    88.69\n",
            "| epoch   2 | 24400/26411 batches | lr 4.75 | ms/batch 57.81 | loss  4.93 | ppl   138.62\n",
            "| epoch   2 | 24600/26411 batches | lr 4.75 | ms/batch 57.68 | loss  4.87 | ppl   130.11\n",
            "| epoch   2 | 24800/26411 batches | lr 4.75 | ms/batch 56.26 | loss  4.39 | ppl    80.94\n",
            "| epoch   2 | 25000/26411 batches | lr 4.75 | ms/batch 57.11 | loss  4.40 | ppl    81.56\n",
            "| epoch   2 | 25200/26411 batches | lr 4.75 | ms/batch 58.06 | loss  4.56 | ppl    95.97\n",
            "| epoch   2 | 25400/26411 batches | lr 4.75 | ms/batch 57.71 | loss  4.79 | ppl   120.70\n",
            "| epoch   2 | 25600/26411 batches | lr 4.75 | ms/batch 57.33 | loss  4.90 | ppl   133.97\n",
            "| epoch   2 | 25800/26411 batches | lr 4.75 | ms/batch 56.47 | loss  4.73 | ppl   113.77\n",
            "| epoch   2 | 26000/26411 batches | lr 4.75 | ms/batch 56.68 | loss  4.99 | ppl   146.73\n",
            "| epoch   2 | 26200/26411 batches | lr 4.75 | ms/batch 57.00 | loss  4.84 | ppl   126.49\n",
            "| epoch   2 | 26400/26411 batches | lr 4.75 | ms/batch 56.82 | loss  4.53 | ppl    92.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 1545.72s | valid loss  5.05 | valid ppl   155.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/26411 batches | lr 4.51 | ms/batch 61.01 | loss  4.80 | ppl   121.93\n",
            "| epoch   3 |   400/26411 batches | lr 4.51 | ms/batch 57.72 | loss  4.97 | ppl   143.46\n",
            "| epoch   3 |   600/26411 batches | lr 4.51 | ms/batch 58.25 | loss  5.06 | ppl   156.92\n",
            "| epoch   3 |   800/26411 batches | lr 4.51 | ms/batch 58.18 | loss  4.73 | ppl   113.37\n",
            "| epoch   3 |  1000/26411 batches | lr 4.51 | ms/batch 57.42 | loss  4.74 | ppl   114.14\n",
            "| epoch   3 |  1200/26411 batches | lr 4.51 | ms/batch 57.12 | loss  4.94 | ppl   140.02\n",
            "| epoch   3 |  1400/26411 batches | lr 4.51 | ms/batch 57.51 | loss  4.99 | ppl   147.62\n",
            "| epoch   3 |  1600/26411 batches | lr 4.51 | ms/batch 57.84 | loss  4.81 | ppl   122.92\n",
            "| epoch   3 |  1800/26411 batches | lr 4.51 | ms/batch 57.62 | loss  4.85 | ppl   127.55\n",
            "| epoch   3 |  2000/26411 batches | lr 4.51 | ms/batch 57.91 | loss  4.57 | ppl    96.40\n",
            "| epoch   3 |  2200/26411 batches | lr 4.51 | ms/batch 57.44 | loss  4.44 | ppl    85.05\n",
            "| epoch   3 |  2400/26411 batches | lr 4.51 | ms/batch 57.69 | loss  4.75 | ppl   115.19\n",
            "| epoch   3 |  2600/26411 batches | lr 4.51 | ms/batch 58.41 | loss  4.89 | ppl   133.27\n",
            "| epoch   3 |  2800/26411 batches | lr 4.51 | ms/batch 62.68 | loss  4.44 | ppl    84.67\n",
            "| epoch   3 |  3000/26411 batches | lr 4.51 | ms/batch 58.72 | loss  4.79 | ppl   120.55\n",
            "| epoch   3 |  3200/26411 batches | lr 4.51 | ms/batch 57.11 | loss  4.58 | ppl    97.05\n",
            "| epoch   3 |  3400/26411 batches | lr 4.51 | ms/batch 56.82 | loss  4.93 | ppl   138.26\n",
            "| epoch   3 |  3600/26411 batches | lr 4.51 | ms/batch 57.58 | loss  5.10 | ppl   163.44\n",
            "| epoch   3 |  3800/26411 batches | lr 4.51 | ms/batch 57.82 | loss  4.88 | ppl   131.34\n",
            "| epoch   3 |  4000/26411 batches | lr 4.51 | ms/batch 57.01 | loss  4.77 | ppl   117.94\n",
            "| epoch   3 |  4200/26411 batches | lr 4.51 | ms/batch 57.69 | loss  4.57 | ppl    96.09\n",
            "| epoch   3 |  4400/26411 batches | lr 4.51 | ms/batch 58.26 | loss  4.47 | ppl    87.06\n",
            "| epoch   3 |  4600/26411 batches | lr 4.51 | ms/batch 57.45 | loss  4.83 | ppl   124.65\n",
            "| epoch   3 |  4800/26411 batches | lr 4.51 | ms/batch 57.65 | loss  4.50 | ppl    90.42\n",
            "| epoch   3 |  5000/26411 batches | lr 4.51 | ms/batch 58.21 | loss  5.01 | ppl   149.67\n",
            "| epoch   3 |  5200/26411 batches | lr 4.51 | ms/batch 57.72 | loss  4.84 | ppl   126.05\n",
            "| epoch   3 |  5400/26411 batches | lr 4.51 | ms/batch 62.34 | loss  4.98 | ppl   145.85\n",
            "| epoch   3 |  5600/26411 batches | lr 4.51 | ms/batch 57.38 | loss  4.85 | ppl   127.75\n",
            "| epoch   3 |  5800/26411 batches | lr 4.51 | ms/batch 58.05 | loss  4.72 | ppl   112.13\n",
            "| epoch   3 |  6000/26411 batches | lr 4.51 | ms/batch 57.06 | loss  4.60 | ppl    99.84\n",
            "| epoch   3 |  6200/26411 batches | lr 4.51 | ms/batch 57.60 | loss  4.58 | ppl    97.60\n",
            "| epoch   3 |  6400/26411 batches | lr 4.51 | ms/batch 57.07 | loss  4.84 | ppl   126.97\n",
            "| epoch   3 |  6600/26411 batches | lr 4.51 | ms/batch 62.56 | loss  4.81 | ppl   123.33\n",
            "| epoch   3 |  6800/26411 batches | lr 4.51 | ms/batch 59.21 | loss  4.99 | ppl   147.46\n",
            "| epoch   3 |  7000/26411 batches | lr 4.51 | ms/batch 57.81 | loss  4.78 | ppl   119.31\n",
            "| epoch   3 |  7200/26411 batches | lr 4.51 | ms/batch 58.95 | loss  4.72 | ppl   112.46\n",
            "| epoch   3 |  7400/26411 batches | lr 4.51 | ms/batch 57.79 | loss  4.83 | ppl   125.32\n",
            "| epoch   3 |  7600/26411 batches | lr 4.51 | ms/batch 58.09 | loss  4.72 | ppl   112.14\n",
            "| epoch   3 |  7800/26411 batches | lr 4.51 | ms/batch 58.32 | loss  4.65 | ppl   104.60\n",
            "| epoch   3 |  8000/26411 batches | lr 4.51 | ms/batch 63.50 | loss  4.85 | ppl   128.26\n",
            "| epoch   3 |  8200/26411 batches | lr 4.51 | ms/batch 62.63 | loss  4.38 | ppl    79.93\n",
            "| epoch   3 |  8400/26411 batches | lr 4.51 | ms/batch 57.70 | loss  4.60 | ppl    99.24\n",
            "| epoch   3 |  8600/26411 batches | lr 4.51 | ms/batch 57.85 | loss  5.00 | ppl   148.08\n",
            "| epoch   3 |  8800/26411 batches | lr 4.51 | ms/batch 57.82 | loss  4.13 | ppl    62.14\n",
            "| epoch   3 |  9000/26411 batches | lr 4.51 | ms/batch 58.03 | loss  5.02 | ppl   151.32\n",
            "| epoch   3 |  9200/26411 batches | lr 4.51 | ms/batch 58.24 | loss  4.92 | ppl   136.34\n",
            "| epoch   3 |  9400/26411 batches | lr 4.51 | ms/batch 57.68 | loss  4.92 | ppl   137.09\n",
            "| epoch   3 |  9600/26411 batches | lr 4.51 | ms/batch 57.70 | loss  4.87 | ppl   130.00\n",
            "| epoch   3 |  9800/26411 batches | lr 4.51 | ms/batch 58.50 | loss  4.82 | ppl   124.15\n",
            "| epoch   3 | 10000/26411 batches | lr 4.51 | ms/batch 58.16 | loss  4.77 | ppl   117.69\n",
            "| epoch   3 | 10200/26411 batches | lr 4.51 | ms/batch 57.93 | loss  4.71 | ppl   111.31\n",
            "| epoch   3 | 10400/26411 batches | lr 4.51 | ms/batch 57.78 | loss  4.52 | ppl    91.84\n",
            "| epoch   3 | 10600/26411 batches | lr 4.51 | ms/batch 63.15 | loss  4.56 | ppl    95.67\n",
            "| epoch   3 | 10800/26411 batches | lr 4.51 | ms/batch 58.61 | loss  4.91 | ppl   135.05\n",
            "| epoch   3 | 11000/26411 batches | lr 4.51 | ms/batch 59.20 | loss  4.72 | ppl   111.82\n",
            "| epoch   3 | 11200/26411 batches | lr 4.51 | ms/batch 59.40 | loss  4.90 | ppl   134.46\n",
            "| epoch   3 | 11400/26411 batches | lr 4.51 | ms/batch 59.69 | loss  4.72 | ppl   111.75\n",
            "| epoch   3 | 11600/26411 batches | lr 4.51 | ms/batch 59.02 | loss  5.06 | ppl   157.27\n",
            "| epoch   3 | 11800/26411 batches | lr 4.51 | ms/batch 59.00 | loss  4.31 | ppl    74.12\n",
            "| epoch   3 | 12000/26411 batches | lr 4.51 | ms/batch 59.07 | loss  4.46 | ppl    86.63\n",
            "| epoch   3 | 12200/26411 batches | lr 4.51 | ms/batch 58.85 | loss  4.47 | ppl    87.42\n",
            "| epoch   3 | 12400/26411 batches | lr 4.51 | ms/batch 58.82 | loss  4.63 | ppl   102.89\n",
            "| epoch   3 | 12600/26411 batches | lr 4.51 | ms/batch 58.95 | loss  4.87 | ppl   130.64\n",
            "| epoch   3 | 12800/26411 batches | lr 4.51 | ms/batch 58.82 | loss  4.44 | ppl    84.97\n",
            "| epoch   3 | 13000/26411 batches | lr 4.51 | ms/batch 58.61 | loss  4.35 | ppl    77.82\n",
            "| epoch   3 | 13200/26411 batches | lr 4.51 | ms/batch 64.02 | loss  5.06 | ppl   156.93\n",
            "| epoch   3 | 13400/26411 batches | lr 4.51 | ms/batch 58.90 | loss  4.92 | ppl   137.31\n",
            "| epoch   3 | 13600/26411 batches | lr 4.51 | ms/batch 58.88 | loss  4.94 | ppl   140.41\n",
            "| epoch   3 | 13800/26411 batches | lr 4.51 | ms/batch 58.94 | loss  4.11 | ppl    61.08\n",
            "| epoch   3 | 14000/26411 batches | lr 4.51 | ms/batch 59.66 | loss  4.69 | ppl   109.30\n",
            "| epoch   3 | 14200/26411 batches | lr 4.51 | ms/batch 59.24 | loss  4.60 | ppl    99.37\n",
            "| epoch   3 | 14400/26411 batches | lr 4.51 | ms/batch 59.34 | loss  4.38 | ppl    79.95\n",
            "| epoch   3 | 14600/26411 batches | lr 4.51 | ms/batch 59.30 | loss  4.47 | ppl    87.37\n",
            "| epoch   3 | 14800/26411 batches | lr 4.51 | ms/batch 60.03 | loss  4.41 | ppl    82.20\n",
            "| epoch   3 | 15000/26411 batches | lr 4.51 | ms/batch 59.03 | loss  5.06 | ppl   157.09\n",
            "| epoch   3 | 15200/26411 batches | lr 4.51 | ms/batch 59.00 | loss  4.95 | ppl   140.87\n",
            "| epoch   3 | 15400/26411 batches | lr 4.51 | ms/batch 59.11 | loss  4.67 | ppl   106.55\n",
            "| epoch   3 | 15600/26411 batches | lr 4.51 | ms/batch 63.53 | loss  4.62 | ppl   101.01\n",
            "| epoch   3 | 15800/26411 batches | lr 4.51 | ms/batch 59.44 | loss  4.11 | ppl    61.16\n",
            "| epoch   3 | 16000/26411 batches | lr 4.51 | ms/batch 58.91 | loss  4.65 | ppl   104.72\n",
            "| epoch   3 | 16200/26411 batches | lr 4.51 | ms/batch 58.89 | loss  4.60 | ppl    99.94\n",
            "| epoch   3 | 16400/26411 batches | lr 4.51 | ms/batch 59.10 | loss  3.98 | ppl    53.77\n",
            "| epoch   3 | 16600/26411 batches | lr 4.51 | ms/batch 59.27 | loss  4.70 | ppl   109.87\n",
            "| epoch   3 | 16800/26411 batches | lr 4.51 | ms/batch 59.23 | loss  4.78 | ppl   119.04\n",
            "| epoch   3 | 17000/26411 batches | lr 4.51 | ms/batch 59.12 | loss  4.92 | ppl   136.87\n",
            "| epoch   3 | 17200/26411 batches | lr 4.51 | ms/batch 58.63 | loss  4.89 | ppl   133.32\n",
            "| epoch   3 | 17400/26411 batches | lr 4.51 | ms/batch 58.21 | loss  4.76 | ppl   116.62\n",
            "| epoch   3 | 17600/26411 batches | lr 4.51 | ms/batch 59.23 | loss  4.63 | ppl   102.07\n",
            "| epoch   3 | 17800/26411 batches | lr 4.51 | ms/batch 59.46 | loss  4.68 | ppl   107.94\n",
            "| epoch   3 | 18000/26411 batches | lr 4.51 | ms/batch 58.56 | loss  4.62 | ppl   101.12\n",
            "| epoch   3 | 18200/26411 batches | lr 4.51 | ms/batch 64.20 | loss  4.81 | ppl   122.12\n",
            "| epoch   3 | 18400/26411 batches | lr 4.51 | ms/batch 58.78 | loss  4.49 | ppl    88.86\n",
            "| epoch   3 | 18600/26411 batches | lr 4.51 | ms/batch 58.67 | loss  4.65 | ppl   104.10\n",
            "| epoch   3 | 18800/26411 batches | lr 4.51 | ms/batch 58.63 | loss  4.55 | ppl    94.54\n",
            "| epoch   3 | 19000/26411 batches | lr 4.51 | ms/batch 58.33 | loss  4.75 | ppl   116.15\n",
            "| epoch   3 | 19200/26411 batches | lr 4.51 | ms/batch 58.73 | loss  4.88 | ppl   131.28\n",
            "| epoch   3 | 19400/26411 batches | lr 4.51 | ms/batch 59.70 | loss  5.06 | ppl   157.36\n",
            "| epoch   3 | 19600/26411 batches | lr 4.51 | ms/batch 59.99 | loss  4.85 | ppl   128.16\n",
            "| epoch   3 | 19800/26411 batches | lr 4.51 | ms/batch 59.91 | loss  4.49 | ppl    88.97\n",
            "| epoch   3 | 20000/26411 batches | lr 4.51 | ms/batch 60.14 | loss  4.74 | ppl   114.39\n",
            "| epoch   3 | 20200/26411 batches | lr 4.51 | ms/batch 60.41 | loss  4.87 | ppl   130.49\n",
            "| epoch   3 | 20400/26411 batches | lr 4.51 | ms/batch 60.30 | loss  4.65 | ppl   104.75\n",
            "| epoch   3 | 20600/26411 batches | lr 4.51 | ms/batch 63.62 | loss  4.77 | ppl   117.75\n",
            "| epoch   3 | 20800/26411 batches | lr 4.51 | ms/batch 62.06 | loss  4.66 | ppl   105.37\n",
            "| epoch   3 | 21000/26411 batches | lr 4.51 | ms/batch 60.16 | loss  4.49 | ppl    88.91\n",
            "| epoch   3 | 21200/26411 batches | lr 4.51 | ms/batch 60.46 | loss  4.59 | ppl    98.79\n",
            "| epoch   3 | 21400/26411 batches | lr 4.51 | ms/batch 60.16 | loss  4.94 | ppl   139.18\n",
            "| epoch   3 | 21600/26411 batches | lr 4.51 | ms/batch 59.91 | loss  4.75 | ppl   115.21\n",
            "| epoch   3 | 21800/26411 batches | lr 4.51 | ms/batch 59.46 | loss  4.47 | ppl    87.28\n",
            "| epoch   3 | 22000/26411 batches | lr 4.51 | ms/batch 59.68 | loss  4.17 | ppl    64.61\n",
            "| epoch   3 | 22200/26411 batches | lr 4.51 | ms/batch 60.14 | loss  4.48 | ppl    88.25\n",
            "| epoch   3 | 22400/26411 batches | lr 4.51 | ms/batch 58.94 | loss  4.60 | ppl    99.65\n",
            "| epoch   3 | 22600/26411 batches | lr 4.51 | ms/batch 58.95 | loss  4.76 | ppl   116.58\n",
            "| epoch   3 | 22800/26411 batches | lr 4.51 | ms/batch 60.54 | loss  4.69 | ppl   108.60\n",
            "| epoch   3 | 23000/26411 batches | lr 4.51 | ms/batch 61.77 | loss  4.80 | ppl   121.78\n",
            "| epoch   3 | 23200/26411 batches | lr 4.51 | ms/batch 66.54 | loss  4.85 | ppl   127.54\n",
            "| epoch   3 | 23400/26411 batches | lr 4.51 | ms/batch 61.00 | loss  4.88 | ppl   132.27\n",
            "| epoch   3 | 23600/26411 batches | lr 4.51 | ms/batch 61.04 | loss  4.36 | ppl    78.55\n",
            "| epoch   3 | 23800/26411 batches | lr 4.51 | ms/batch 61.50 | loss  4.53 | ppl    92.77\n",
            "| epoch   3 | 24000/26411 batches | lr 4.51 | ms/batch 61.56 | loss  4.32 | ppl    75.22\n",
            "| epoch   3 | 24200/26411 batches | lr 4.51 | ms/batch 60.66 | loss  4.34 | ppl    76.76\n",
            "| epoch   3 | 24400/26411 batches | lr 4.51 | ms/batch 59.33 | loss  4.78 | ppl   119.15\n",
            "| epoch   3 | 24600/26411 batches | lr 4.51 | ms/batch 59.56 | loss  4.73 | ppl   113.24\n",
            "| epoch   3 | 24800/26411 batches | lr 4.51 | ms/batch 60.31 | loss  4.23 | ppl    68.64\n",
            "| epoch   3 | 25000/26411 batches | lr 4.51 | ms/batch 60.24 | loss  4.21 | ppl    67.57\n",
            "| epoch   3 | 25200/26411 batches | lr 4.51 | ms/batch 59.47 | loss  4.42 | ppl    83.01\n",
            "| epoch   3 | 25400/26411 batches | lr 4.51 | ms/batch 58.81 | loss  4.65 | ppl   104.42\n",
            "| epoch   3 | 25600/26411 batches | lr 4.51 | ms/batch 63.80 | loss  4.75 | ppl   115.20\n",
            "| epoch   3 | 25800/26411 batches | lr 4.51 | ms/batch 59.49 | loss  4.57 | ppl    96.20\n",
            "| epoch   3 | 26000/26411 batches | lr 4.51 | ms/batch 59.19 | loss  4.85 | ppl   127.59\n",
            "| epoch   3 | 26200/26411 batches | lr 4.51 | ms/batch 60.35 | loss  4.70 | ppl   110.15\n",
            "| epoch   3 | 26400/26411 batches | lr 4.51 | ms/batch 59.41 | loss  4.36 | ppl    77.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 1581.92s | valid loss  5.00 | valid ppl   148.65\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
        "      f'test ppl {test_ppl:8.2f}')\n",
        "print('=' * 89)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FbM0c93_J03",
        "outputId": "6f578a5a-41e9-4109-f308-025b2f07cafc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  4.94 | test ppl   140.42\n",
            "=========================================================================================\n"
          ]
        }
      ]
    }
  ]
}