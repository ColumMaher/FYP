{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZxVrhiF0Mh00"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from typing import Tuple\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.utils.data import dataset\n",
        "import torchtext\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YOX72cCBS71c"
      },
      "outputs": [],
      "source": [
        "from torchtext.datasets import PennTreebank "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0FioMPfTdBJT"
      },
      "outputs": [],
      "source": [
        "train_iter = PennTreebank(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = torchtext.vocab.build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "S5Ae9CdAyZQH"
      },
      "outputs": [],
      "source": [
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "  \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "  data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "R5cZes9WiHFJ"
      },
      "outputs": [],
      "source": [
        "train_iter, val_iter, test_iter = PennTreebank()\n",
        "train_data= data_process(train_iter)\n",
        "val_data= data_process(val_iter)\n",
        "test_data= data_process(test_iter)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "VVTtf0cPycRW"
      },
      "outputs": [],
      "source": [
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Vz5mzO8wycub"
      },
      "outputs": [],
      "source": [
        "batch_size = 10\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0-OPgEJ0lgm",
        "outputId": "00b09869-e726-4b1e-83e4-84951696f08e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([9892,  871,    2,   17,   12,    3,  318,   29, 2356,   34])\n"
          ]
        }
      ],
      "source": [
        "print(train_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jN-g8V7ByvRH"
      },
      "outputs": [],
      "source": [
        "bptt = 80\n",
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "xYF8ErZ2zE6b"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
        "                    dropout=dropout_rate, batch_first=True)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    self.linear = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    if tie_weights:\n",
        "          assert embedding_dim == hidden_dim, 'cannot tie, check dims'\n",
        "          self.linear.weight = self.embedding.weight\n",
        "    self.init_weights()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x is a batch of input sequences\n",
        "    x = self.embedding(x)\n",
        "    x, _ = self.lstm(x)\n",
        "    x = self.linear(x)\n",
        "    return x\n",
        "\n",
        "  def init_weights(self):\n",
        "    init_range_emb = 0.1\n",
        "    init_range_other = 1/math.sqrt(self.hidden_dim)\n",
        "    self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
        "    self.linear.weight.data.uniform_(-init_range_other, init_range_other)\n",
        "    self.linear.bias.data.zero_()\n",
        "    for i in range(self.num_layers):\n",
        "        self.lstm.all_weights[i][0] = torch.FloatTensor(self.embedding_dim,\n",
        "                self.hidden_dim).uniform_(-init_range_other, init_range_other) \n",
        "        self.lstm.all_weights[i][1] = torch.FloatTensor(self.hidden_dim, \n",
        "                self.hidden_dim).uniform_(-init_range_other, init_range_other)\n",
        "\n",
        "  def init_state(self, sequence_length):\n",
        "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n",
        "                torch.zeros(self.num_layers, sequence_length, self.lstm_size))       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vPNWmqoUWmJ",
        "outputId": "e57ab253-8b84-463c-a80c-66e05b5d44c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (embedding): Embedding(9922, 100)\n",
              "  (lstm): LSTM(100, 100, num_layers=2, batch_first=True, dropout=0.4)\n",
              "  (dropout): Dropout(p=0.4, inplace=False)\n",
              "  (linear): Linear(in_features=100, out_features=9922, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 100\n",
        "num_layers = 2\n",
        "dropout_rate = 0.4\n",
        "tie_weights = True\n",
        "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "VbUtSuzbUbea"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 20.0  # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "def train(model: nn.Module) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "    \n",
        "    num_batches = len(train_data) // bptt\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        seq_len = data.size(0)\n",
        "        output = model(data)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "5SjEJ3OY91mw"
      },
      "outputs": [],
      "source": [
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(eval_data, i)\n",
        "            seq_len = data.size(0)\n",
        "            output = model(data)\n",
        "            output_flat = output.view(-1, vocab_size)\n",
        "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(eval_data) - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RQth0cB_Jj8",
        "outputId": "982b6580-975e-4de5-8f3a-ffc756e874b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 1155 batches | lr 20.00 | ms/batch 56.07 | loss  6.92 | ppl  1007.48\n",
            "| epoch   1 |   400/ 1155 batches | lr 20.00 | ms/batch 56.49 | loss  6.24 | ppl   512.90\n",
            "| epoch   1 |   600/ 1155 batches | lr 20.00 | ms/batch 56.19 | loss  6.02 | ppl   409.56\n",
            "| epoch   1 |   800/ 1155 batches | lr 20.00 | ms/batch 57.75 | loss  5.88 | ppl   358.80\n",
            "| epoch   1 |  1000/ 1155 batches | lr 20.00 | ms/batch 55.54 | loss  5.78 | ppl   324.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 67.35s | valid loss  5.74 | valid ppl   310.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 1155 batches | lr 19.00 | ms/batch 54.39 | loss  5.69 | ppl   297.27\n",
            "| epoch   2 |   400/ 1155 batches | lr 19.00 | ms/batch 54.09 | loss  5.62 | ppl   277.14\n",
            "| epoch   2 |   600/ 1155 batches | lr 19.00 | ms/batch 53.98 | loss  5.59 | ppl   266.47\n",
            "| epoch   2 |   800/ 1155 batches | lr 19.00 | ms/batch 53.59 | loss  5.55 | ppl   257.88\n",
            "| epoch   2 |  1000/ 1155 batches | lr 19.00 | ms/batch 53.50 | loss  5.53 | ppl   251.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 64.50s | valid loss  5.56 | valid ppl   260.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 1155 batches | lr 18.05 | ms/batch 54.75 | loss  5.50 | ppl   245.68\n",
            "| epoch   3 |   400/ 1155 batches | lr 18.05 | ms/batch 55.39 | loss  5.46 | ppl   236.15\n",
            "| epoch   3 |   600/ 1155 batches | lr 18.05 | ms/batch 53.11 | loss  5.45 | ppl   232.96\n",
            "| epoch   3 |   800/ 1155 batches | lr 18.05 | ms/batch 53.65 | loss  5.43 | ppl   227.96\n",
            "| epoch   3 |  1000/ 1155 batches | lr 18.05 | ms/batch 53.35 | loss  5.42 | ppl   226.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 64.69s | valid loss  5.49 | valid ppl   242.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/ 1155 batches | lr 17.15 | ms/batch 53.74 | loss  5.41 | ppl   223.61\n",
            "| epoch   4 |   400/ 1155 batches | lr 17.15 | ms/batch 54.95 | loss  5.38 | ppl   216.46\n",
            "| epoch   4 |   600/ 1155 batches | lr 17.15 | ms/batch 54.51 | loss  5.38 | ppl   216.23\n",
            "| epoch   4 |   800/ 1155 batches | lr 17.15 | ms/batch 53.35 | loss  5.36 | ppl   212.17\n",
            "| epoch   4 |  1000/ 1155 batches | lr 17.15 | ms/batch 52.99 | loss  5.36 | ppl   211.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 64.62s | valid loss  5.45 | valid ppl   232.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/ 1155 batches | lr 16.29 | ms/batch 52.70 | loss  5.35 | ppl   210.52\n",
            "| epoch   5 |   400/ 1155 batches | lr 16.29 | ms/batch 54.06 | loss  5.32 | ppl   204.29\n",
            "| epoch   5 |   600/ 1155 batches | lr 16.29 | ms/batch 55.10 | loss  5.32 | ppl   205.05\n",
            "| epoch   5 |   800/ 1155 batches | lr 16.29 | ms/batch 54.26 | loss  5.31 | ppl   201.64\n",
            "| epoch   5 |  1000/ 1155 batches | lr 16.29 | ms/batch 54.35 | loss  5.31 | ppl   202.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 64.52s | valid loss  5.42 | valid ppl   226.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/ 1155 batches | lr 15.48 | ms/batch 54.62 | loss  5.30 | ppl   201.03\n",
            "| epoch   6 |   400/ 1155 batches | lr 15.48 | ms/batch 57.99 | loss  5.28 | ppl   195.46\n",
            "| epoch   6 |   600/ 1155 batches | lr 15.48 | ms/batch 52.62 | loss  5.28 | ppl   197.30\n",
            "| epoch   6 |   800/ 1155 batches | lr 15.48 | ms/batch 52.48 | loss  5.27 | ppl   193.82\n",
            "| epoch   6 |  1000/ 1155 batches | lr 15.48 | ms/batch 55.73 | loss  5.27 | ppl   194.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 65.70s | valid loss  5.40 | valid ppl   221.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/ 1155 batches | lr 14.70 | ms/batch 54.07 | loss  5.27 | ppl   193.68\n",
            "| epoch   7 |   400/ 1155 batches | lr 14.70 | ms/batch 54.05 | loss  5.24 | ppl   188.68\n",
            "| epoch   7 |   600/ 1155 batches | lr 14.70 | ms/batch 54.20 | loss  5.25 | ppl   190.72\n",
            "| epoch   7 |   800/ 1155 batches | lr 14.70 | ms/batch 53.56 | loss  5.23 | ppl   187.56\n",
            "| epoch   7 |  1000/ 1155 batches | lr 14.70 | ms/batch 52.66 | loss  5.24 | ppl   188.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 64.22s | valid loss  5.39 | valid ppl   218.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/ 1155 batches | lr 13.97 | ms/batch 52.23 | loss  5.24 | ppl   187.75\n",
            "| epoch   8 |   400/ 1155 batches | lr 13.97 | ms/batch 56.28 | loss  5.21 | ppl   183.04\n",
            "| epoch   8 |   600/ 1155 batches | lr 13.97 | ms/batch 58.41 | loss  5.22 | ppl   185.58\n",
            "| epoch   8 |   800/ 1155 batches | lr 13.97 | ms/batch 55.38 | loss  5.21 | ppl   182.28\n",
            "| epoch   8 |  1000/ 1155 batches | lr 13.97 | ms/batch 54.00 | loss  5.21 | ppl   184.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 65.84s | valid loss  5.38 | valid ppl   216.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/ 1155 batches | lr 13.27 | ms/batch 53.81 | loss  5.21 | ppl   183.03\n",
            "| epoch   9 |   400/ 1155 batches | lr 13.27 | ms/batch 53.82 | loss  5.18 | ppl   178.42\n",
            "| epoch   9 |   600/ 1155 batches | lr 13.27 | ms/batch 52.77 | loss  5.20 | ppl   181.48\n",
            "| epoch   9 |   800/ 1155 batches | lr 13.27 | ms/batch 52.66 | loss  5.18 | ppl   178.19\n",
            "| epoch   9 |  1000/ 1155 batches | lr 13.27 | ms/batch 53.05 | loss  5.19 | ppl   179.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 64.30s | valid loss  5.37 | valid ppl   214.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/ 1155 batches | lr 12.60 | ms/batch 55.74 | loss  5.19 | ppl   179.01\n",
            "| epoch  10 |   400/ 1155 batches | lr 12.60 | ms/batch 53.99 | loss  5.16 | ppl   174.56\n",
            "| epoch  10 |   600/ 1155 batches | lr 12.60 | ms/batch 54.09 | loss  5.18 | ppl   177.84\n",
            "| epoch  10 |   800/ 1155 batches | lr 12.60 | ms/batch 52.86 | loss  5.16 | ppl   174.56\n",
            "| epoch  10 |  1000/ 1155 batches | lr 12.60 | ms/batch 52.91 | loss  5.17 | ppl   176.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 64.59s | valid loss  5.36 | valid ppl   212.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   200/ 1155 batches | lr 11.97 | ms/batch 53.06 | loss  5.17 | ppl   175.18\n",
            "| epoch  11 |   400/ 1155 batches | lr 11.97 | ms/batch 54.76 | loss  5.14 | ppl   170.82\n",
            "| epoch  11 |   600/ 1155 batches | lr 11.97 | ms/batch 56.13 | loss  5.16 | ppl   174.16\n",
            "| epoch  11 |   800/ 1155 batches | lr 11.97 | ms/batch 53.76 | loss  5.14 | ppl   170.92\n",
            "| epoch  11 |  1000/ 1155 batches | lr 11.97 | ms/batch 52.32 | loss  5.15 | ppl   172.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 64.57s | valid loss  5.36 | valid ppl   211.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |   200/ 1155 batches | lr 11.38 | ms/batch 52.78 | loss  5.15 | ppl   172.33\n",
            "| epoch  12 |   400/ 1155 batches | lr 11.38 | ms/batch 54.33 | loss  5.12 | ppl   167.65\n",
            "| epoch  12 |   600/ 1155 batches | lr 11.38 | ms/batch 53.83 | loss  5.15 | ppl   171.63\n",
            "| epoch  12 |   800/ 1155 batches | lr 11.38 | ms/batch 53.06 | loss  5.12 | ppl   168.13\n",
            "| epoch  12 |  1000/ 1155 batches | lr 11.38 | ms/batch 53.07 | loss  5.14 | ppl   169.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 64.26s | valid loss  5.35 | valid ppl   210.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   200/ 1155 batches | lr 10.81 | ms/batch 55.29 | loss  5.13 | ppl   169.54\n",
            "| epoch  13 |   400/ 1155 batches | lr 10.81 | ms/batch 53.22 | loss  5.11 | ppl   165.28\n",
            "| epoch  13 |   600/ 1155 batches | lr 10.81 | ms/batch 52.60 | loss  5.13 | ppl   168.94\n",
            "| epoch  13 |   800/ 1155 batches | lr 10.81 | ms/batch 52.61 | loss  5.11 | ppl   165.73\n",
            "| epoch  13 |  1000/ 1155 batches | lr 10.81 | ms/batch 52.66 | loss  5.12 | ppl   167.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 63.76s | valid loss  5.35 | valid ppl   210.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   200/ 1155 batches | lr 10.27 | ms/batch 53.30 | loss  5.12 | ppl   167.01\n",
            "| epoch  14 |   400/ 1155 batches | lr 10.27 | ms/batch 52.99 | loss  5.09 | ppl   162.56\n",
            "| epoch  14 |   600/ 1155 batches | lr 10.27 | ms/batch 52.83 | loss  5.11 | ppl   166.20\n",
            "| epoch  14 |   800/ 1155 batches | lr 10.27 | ms/batch 53.25 | loss  5.10 | ppl   163.68\n",
            "| epoch  14 |  1000/ 1155 batches | lr 10.27 | ms/batch 53.65 | loss  5.10 | ppl   164.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 63.71s | valid loss  5.35 | valid ppl   210.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |   200/ 1155 batches | lr 9.75 | ms/batch 53.69 | loss  5.10 | ppl   164.37\n",
            "| epoch  15 |   400/ 1155 batches | lr 9.75 | ms/batch 52.80 | loss  5.08 | ppl   160.41\n",
            "| epoch  15 |   600/ 1155 batches | lr 9.75 | ms/batch 52.90 | loss  5.10 | ppl   163.93\n",
            "| epoch  15 |   800/ 1155 batches | lr 9.75 | ms/batch 52.27 | loss  5.09 | ppl   161.59\n",
            "| epoch  15 |  1000/ 1155 batches | lr 9.75 | ms/batch 55.80 | loss  5.09 | ppl   162.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 64.69s | valid loss  5.34 | valid ppl   209.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   200/ 1155 batches | lr 9.27 | ms/batch 54.74 | loss  5.09 | ppl   162.35\n",
            "| epoch  16 |   400/ 1155 batches | lr 9.27 | ms/batch 55.69 | loss  5.07 | ppl   158.57\n",
            "| epoch  16 |   600/ 1155 batches | lr 9.27 | ms/batch 52.93 | loss  5.09 | ppl   161.98\n",
            "| epoch  16 |   800/ 1155 batches | lr 9.27 | ms/batch 52.74 | loss  5.08 | ppl   160.23\n",
            "| epoch  16 |  1000/ 1155 batches | lr 9.27 | ms/batch 52.85 | loss  5.08 | ppl   160.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 64.25s | valid loss  5.34 | valid ppl   209.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   200/ 1155 batches | lr 8.80 | ms/batch 52.62 | loss  5.08 | ppl   160.41\n",
            "| epoch  17 |   400/ 1155 batches | lr 8.80 | ms/batch 52.91 | loss  5.05 | ppl   156.78\n",
            "| epoch  17 |   600/ 1155 batches | lr 8.80 | ms/batch 52.57 | loss  5.08 | ppl   160.11\n",
            "| epoch  17 |   800/ 1155 batches | lr 8.80 | ms/batch 53.63 | loss  5.06 | ppl   158.24\n",
            "| epoch  17 |  1000/ 1155 batches | lr 8.80 | ms/batch 52.37 | loss  5.07 | ppl   159.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 63.33s | valid loss  5.34 | valid ppl   208.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   200/ 1155 batches | lr 8.36 | ms/batch 53.20 | loss  5.07 | ppl   158.79\n",
            "| epoch  18 |   400/ 1155 batches | lr 8.36 | ms/batch 52.50 | loss  5.04 | ppl   155.05\n",
            "| epoch  18 |   600/ 1155 batches | lr 8.36 | ms/batch 53.17 | loss  5.07 | ppl   158.46\n",
            "| epoch  18 |   800/ 1155 batches | lr 8.36 | ms/batch 53.19 | loss  5.05 | ppl   156.55\n",
            "| epoch  18 |  1000/ 1155 batches | lr 8.36 | ms/batch 56.08 | loss  5.06 | ppl   157.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 64.39s | valid loss  5.34 | valid ppl   208.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |   200/ 1155 batches | lr 7.94 | ms/batch 54.34 | loss  5.06 | ppl   157.49\n",
            "| epoch  19 |   400/ 1155 batches | lr 7.94 | ms/batch 52.88 | loss  5.03 | ppl   153.44\n",
            "| epoch  19 |   600/ 1155 batches | lr 7.94 | ms/batch 52.62 | loss  5.06 | ppl   156.85\n",
            "| epoch  19 |   800/ 1155 batches | lr 7.94 | ms/batch 51.70 | loss  5.05 | ppl   155.40\n",
            "| epoch  19 |  1000/ 1155 batches | lr 7.94 | ms/batch 52.47 | loss  5.05 | ppl   156.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 63.21s | valid loss  5.34 | valid ppl   209.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |   200/ 1155 batches | lr 7.55 | ms/batch 52.18 | loss  5.05 | ppl   155.71\n",
            "| epoch  20 |   400/ 1155 batches | lr 7.55 | ms/batch 52.22 | loss  5.02 | ppl   152.09\n",
            "| epoch  20 |   600/ 1155 batches | lr 7.55 | ms/batch 52.15 | loss  5.05 | ppl   155.39\n",
            "| epoch  20 |   800/ 1155 batches | lr 7.55 | ms/batch 51.93 | loss  5.04 | ppl   153.93\n",
            "| epoch  20 |  1000/ 1155 batches | lr 7.55 | ms/batch 52.04 | loss  5.04 | ppl   155.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 62.40s | valid loss  5.34 | valid ppl   208.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |   200/ 1155 batches | lr 7.17 | ms/batch 53.52 | loss  5.04 | ppl   154.36\n",
            "| epoch  21 |   400/ 1155 batches | lr 7.17 | ms/batch 52.64 | loss  5.02 | ppl   151.03\n",
            "| epoch  21 |   600/ 1155 batches | lr 7.17 | ms/batch 53.26 | loss  5.03 | ppl   153.62\n",
            "| epoch  21 |   800/ 1155 batches | lr 7.17 | ms/batch 53.10 | loss  5.03 | ppl   152.68\n",
            "| epoch  21 |  1000/ 1155 batches | lr 7.17 | ms/batch 53.09 | loss  5.04 | ppl   153.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 63.70s | valid loss  5.34 | valid ppl   208.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |   200/ 1155 batches | lr 6.81 | ms/batch 53.54 | loss  5.03 | ppl   153.33\n",
            "| epoch  22 |   400/ 1155 batches | lr 6.81 | ms/batch 56.03 | loss  5.01 | ppl   149.26\n",
            "| epoch  22 |   600/ 1155 batches | lr 6.81 | ms/batch 53.93 | loss  5.03 | ppl   152.99\n",
            "| epoch  22 |   800/ 1155 batches | lr 6.81 | ms/batch 54.06 | loss  5.02 | ppl   151.78\n",
            "| epoch  22 |  1000/ 1155 batches | lr 6.81 | ms/batch 52.44 | loss  5.03 | ppl   152.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 64.53s | valid loss  5.34 | valid ppl   208.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |   200/ 1155 batches | lr 6.47 | ms/batch 54.46 | loss  5.02 | ppl   152.11\n",
            "| epoch  23 |   400/ 1155 batches | lr 6.47 | ms/batch 53.27 | loss  5.00 | ppl   148.70\n",
            "| epoch  23 |   600/ 1155 batches | lr 6.47 | ms/batch 53.96 | loss  5.02 | ppl   151.77\n",
            "| epoch  23 |   800/ 1155 batches | lr 6.47 | ms/batch 53.10 | loss  5.01 | ppl   150.05\n",
            "| epoch  23 |  1000/ 1155 batches | lr 6.47 | ms/batch 53.61 | loss  5.02 | ppl   151.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 64.32s | valid loss  5.34 | valid ppl   208.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |   200/ 1155 batches | lr 6.15 | ms/batch 54.06 | loss  5.02 | ppl   150.70\n",
            "| epoch  24 |   400/ 1155 batches | lr 6.15 | ms/batch 54.53 | loss  4.99 | ppl   147.56\n",
            "| epoch  24 |   600/ 1155 batches | lr 6.15 | ms/batch 55.45 | loss  5.01 | ppl   150.49\n",
            "| epoch  24 |   800/ 1155 batches | lr 6.15 | ms/batch 55.73 | loss  5.01 | ppl   149.51\n",
            "| epoch  24 |  1000/ 1155 batches | lr 6.15 | ms/batch 52.85 | loss  5.01 | ppl   150.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 65.00s | valid loss  5.34 | valid ppl   208.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |   200/ 1155 batches | lr 5.84 | ms/batch 52.90 | loss  5.01 | ppl   149.72\n",
            "| epoch  25 |   400/ 1155 batches | lr 5.84 | ms/batch 53.15 | loss  4.99 | ppl   146.62\n",
            "| epoch  25 |   600/ 1155 batches | lr 5.84 | ms/batch 53.30 | loss  5.01 | ppl   149.84\n",
            "| epoch  25 |   800/ 1155 batches | lr 5.84 | ms/batch 53.73 | loss  5.00 | ppl   147.87\n",
            "| epoch  25 |  1000/ 1155 batches | lr 5.84 | ms/batch 52.66 | loss  5.01 | ppl   149.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 63.61s | valid loss  5.34 | valid ppl   208.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |   200/ 1155 batches | lr 5.55 | ms/batch 52.70 | loss  5.00 | ppl   148.68\n",
            "| epoch  26 |   400/ 1155 batches | lr 5.55 | ms/batch 52.33 | loss  4.98 | ppl   145.52\n",
            "| epoch  26 |   600/ 1155 batches | lr 5.55 | ms/batch 52.41 | loss  5.00 | ppl   148.74\n",
            "| epoch  26 |   800/ 1155 batches | lr 5.55 | ms/batch 52.35 | loss  4.99 | ppl   147.07\n",
            "| epoch  26 |  1000/ 1155 batches | lr 5.55 | ms/batch 52.97 | loss  5.00 | ppl   148.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 63.13s | valid loss  5.34 | valid ppl   208.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |   200/ 1155 batches | lr 5.27 | ms/batch 54.53 | loss  5.00 | ppl   147.82\n",
            "| epoch  27 |   400/ 1155 batches | lr 5.27 | ms/batch 55.97 | loss  4.98 | ppl   144.77\n",
            "| epoch  27 |   600/ 1155 batches | lr 5.27 | ms/batch 53.31 | loss  5.00 | ppl   148.04\n",
            "| epoch  27 |   800/ 1155 batches | lr 5.27 | ms/batch 53.89 | loss  4.99 | ppl   146.21\n",
            "| epoch  27 |  1000/ 1155 batches | lr 5.27 | ms/batch 53.23 | loss  5.00 | ppl   147.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 64.81s | valid loss  5.34 | valid ppl   208.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |   200/ 1155 batches | lr 5.01 | ms/batch 52.89 | loss  4.99 | ppl   147.03\n",
            "| epoch  28 |   400/ 1155 batches | lr 5.01 | ms/batch 52.88 | loss  4.97 | ppl   143.92\n",
            "| epoch  28 |   600/ 1155 batches | lr 5.01 | ms/batch 52.94 | loss  4.99 | ppl   147.28\n",
            "| epoch  28 |   800/ 1155 batches | lr 5.01 | ms/batch 52.73 | loss  4.98 | ppl   145.26\n",
            "| epoch  28 |  1000/ 1155 batches | lr 5.01 | ms/batch 53.13 | loss  4.99 | ppl   146.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 63.51s | valid loss  5.34 | valid ppl   208.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |   200/ 1155 batches | lr 4.76 | ms/batch 53.64 | loss  4.98 | ppl   146.11\n",
            "| epoch  29 |   400/ 1155 batches | lr 4.76 | ms/batch 52.34 | loss  4.96 | ppl   143.03\n",
            "| epoch  29 |   600/ 1155 batches | lr 4.76 | ms/batch 52.93 | loss  4.99 | ppl   146.38\n",
            "| epoch  29 |   800/ 1155 batches | lr 4.76 | ms/batch 52.54 | loss  4.97 | ppl   144.45\n",
            "| epoch  29 |  1000/ 1155 batches | lr 4.76 | ms/batch 52.89 | loss  4.98 | ppl   146.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 63.38s | valid loss  5.34 | valid ppl   208.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |   200/ 1155 batches | lr 4.52 | ms/batch 56.62 | loss  4.98 | ppl   145.31\n",
            "| epoch  30 |   400/ 1155 batches | lr 4.52 | ms/batch 54.29 | loss  4.96 | ppl   142.54\n",
            "| epoch  30 |   600/ 1155 batches | lr 4.52 | ms/batch 52.37 | loss  4.98 | ppl   145.64\n",
            "| epoch  30 |   800/ 1155 batches | lr 4.52 | ms/batch 52.65 | loss  4.97 | ppl   143.73\n",
            "| epoch  30 |  1000/ 1155 batches | lr 4.52 | ms/batch 52.55 | loss  4.98 | ppl   145.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 64.18s | valid loss  5.34 | valid ppl   208.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  31 |   200/ 1155 batches | lr 4.29 | ms/batch 53.55 | loss  4.98 | ppl   144.87\n",
            "| epoch  31 |   400/ 1155 batches | lr 4.29 | ms/batch 53.07 | loss  4.95 | ppl   141.45\n",
            "| epoch  31 |   600/ 1155 batches | lr 4.29 | ms/batch 52.70 | loss  4.98 | ppl   145.09\n",
            "| epoch  31 |   800/ 1155 batches | lr 4.29 | ms/batch 56.70 | loss  4.96 | ppl   142.97\n",
            "| epoch  31 |  1000/ 1155 batches | lr 4.29 | ms/batch 55.97 | loss  4.98 | ppl   144.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 65.79s | valid loss  5.34 | valid ppl   208.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |   200/ 1155 batches | lr 4.08 | ms/batch 54.33 | loss  4.97 | ppl   143.94\n",
            "| epoch  32 |   400/ 1155 batches | lr 4.08 | ms/batch 52.89 | loss  4.95 | ppl   140.81\n",
            "| epoch  32 |   600/ 1155 batches | lr 4.08 | ms/batch 52.80 | loss  4.97 | ppl   144.30\n",
            "| epoch  32 |   800/ 1155 batches | lr 4.08 | ms/batch 52.44 | loss  4.96 | ppl   142.47\n",
            "| epoch  32 |  1000/ 1155 batches | lr 4.08 | ms/batch 52.47 | loss  4.97 | ppl   144.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 64.14s | valid loss  5.34 | valid ppl   208.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |   200/ 1155 batches | lr 3.87 | ms/batch 54.22 | loss  4.97 | ppl   143.32\n",
            "| epoch  33 |   400/ 1155 batches | lr 3.87 | ms/batch 53.73 | loss  4.94 | ppl   140.17\n",
            "| epoch  33 |   600/ 1155 batches | lr 3.87 | ms/batch 52.05 | loss  4.97 | ppl   144.04\n",
            "| epoch  33 |   800/ 1155 batches | lr 3.87 | ms/batch 54.03 | loss  4.95 | ppl   141.76\n",
            "| epoch  33 |  1000/ 1155 batches | lr 3.87 | ms/batch 52.24 | loss  4.97 | ppl   143.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 63.68s | valid loss  5.34 | valid ppl   209.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |   200/ 1155 batches | lr 3.68 | ms/batch 53.28 | loss  4.96 | ppl   142.56\n",
            "| epoch  34 |   400/ 1155 batches | lr 3.68 | ms/batch 52.76 | loss  4.94 | ppl   139.81\n",
            "| epoch  34 |   600/ 1155 batches | lr 3.68 | ms/batch 53.18 | loss  4.96 | ppl   143.21\n",
            "| epoch  34 |   800/ 1155 batches | lr 3.68 | ms/batch 53.23 | loss  4.95 | ppl   140.95\n",
            "| epoch  34 |  1000/ 1155 batches | lr 3.68 | ms/batch 52.83 | loss  4.96 | ppl   142.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 63.96s | valid loss  5.34 | valid ppl   209.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |   200/ 1155 batches | lr 3.50 | ms/batch 53.43 | loss  4.96 | ppl   142.16\n",
            "| epoch  35 |   400/ 1155 batches | lr 3.50 | ms/batch 53.03 | loss  4.93 | ppl   138.91\n",
            "| epoch  35 |   600/ 1155 batches | lr 3.50 | ms/batch 53.55 | loss  4.96 | ppl   142.64\n",
            "| epoch  35 |   800/ 1155 batches | lr 3.50 | ms/batch 52.56 | loss  4.95 | ppl   140.66\n",
            "| epoch  35 |  1000/ 1155 batches | lr 3.50 | ms/batch 52.48 | loss  4.96 | ppl   142.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 63.56s | valid loss  5.34 | valid ppl   209.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |   200/ 1155 batches | lr 3.32 | ms/batch 53.62 | loss  4.95 | ppl   141.47\n",
            "| epoch  36 |   400/ 1155 batches | lr 3.32 | ms/batch 52.96 | loss  4.93 | ppl   138.30\n",
            "| epoch  36 |   600/ 1155 batches | lr 3.32 | ms/batch 52.95 | loss  4.96 | ppl   142.23\n",
            "| epoch  36 |   800/ 1155 batches | lr 3.32 | ms/batch 52.40 | loss  4.94 | ppl   140.01\n",
            "| epoch  36 |  1000/ 1155 batches | lr 3.32 | ms/batch 53.01 | loss  4.96 | ppl   141.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 63.55s | valid loss  5.34 | valid ppl   208.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  37 |   200/ 1155 batches | lr 3.16 | ms/batch 53.49 | loss  4.95 | ppl   141.12\n",
            "| epoch  37 |   400/ 1155 batches | lr 3.16 | ms/batch 53.64 | loss  4.93 | ppl   138.22\n",
            "| epoch  37 |   600/ 1155 batches | lr 3.16 | ms/batch 52.78 | loss  4.95 | ppl   141.83\n",
            "| epoch  37 |   800/ 1155 batches | lr 3.16 | ms/batch 52.85 | loss  4.94 | ppl   139.36\n",
            "| epoch  37 |  1000/ 1155 batches | lr 3.16 | ms/batch 53.02 | loss  4.95 | ppl   141.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 63.62s | valid loss  5.34 | valid ppl   208.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |   200/ 1155 batches | lr 3.00 | ms/batch 53.31 | loss  4.94 | ppl   140.44\n",
            "| epoch  38 |   400/ 1155 batches | lr 3.00 | ms/batch 53.25 | loss  4.92 | ppl   137.22\n",
            "| epoch  38 |   600/ 1155 batches | lr 3.00 | ms/batch 52.08 | loss  4.95 | ppl   141.47\n",
            "| epoch  38 |   800/ 1155 batches | lr 3.00 | ms/batch 53.83 | loss  4.93 | ppl   139.03\n",
            "| epoch  38 |  1000/ 1155 batches | lr 3.00 | ms/batch 53.35 | loss  4.95 | ppl   140.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 63.82s | valid loss  5.34 | valid ppl   209.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |   200/ 1155 batches | lr 2.85 | ms/batch 53.20 | loss  4.94 | ppl   140.19\n",
            "| epoch  39 |   400/ 1155 batches | lr 2.85 | ms/batch 52.42 | loss  4.92 | ppl   136.79\n",
            "| epoch  39 |   600/ 1155 batches | lr 2.85 | ms/batch 52.60 | loss  4.95 | ppl   140.91\n",
            "| epoch  39 |   800/ 1155 batches | lr 2.85 | ms/batch 52.75 | loss  4.93 | ppl   138.44\n",
            "| epoch  39 |  1000/ 1155 batches | lr 2.85 | ms/batch 52.85 | loss  4.94 | ppl   140.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 63.34s | valid loss  5.34 | valid ppl   208.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |   200/ 1155 batches | lr 2.71 | ms/batch 56.49 | loss  4.94 | ppl   139.49\n",
            "| epoch  40 |   400/ 1155 batches | lr 2.71 | ms/batch 53.29 | loss  4.91 | ppl   136.22\n",
            "| epoch  40 |   600/ 1155 batches | lr 2.71 | ms/batch 53.10 | loss  4.94 | ppl   140.32\n",
            "| epoch  40 |   800/ 1155 batches | lr 2.71 | ms/batch 52.90 | loss  4.93 | ppl   137.86\n",
            "| epoch  40 |  1000/ 1155 batches | lr 2.71 | ms/batch 52.80 | loss  4.94 | ppl   139.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 64.34s | valid loss  5.34 | valid ppl   208.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  41 |   200/ 1155 batches | lr 2.57 | ms/batch 53.33 | loss  4.94 | ppl   139.54\n",
            "| epoch  41 |   400/ 1155 batches | lr 2.57 | ms/batch 53.27 | loss  4.91 | ppl   135.88\n",
            "| epoch  41 |   600/ 1155 batches | lr 2.57 | ms/batch 53.71 | loss  4.94 | ppl   139.98\n",
            "| epoch  41 |   800/ 1155 batches | lr 2.57 | ms/batch 55.15 | loss  4.92 | ppl   137.52\n",
            "| epoch  41 |  1000/ 1155 batches | lr 2.57 | ms/batch 58.69 | loss  4.94 | ppl   139.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 66.20s | valid loss  5.34 | valid ppl   208.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  42 |   200/ 1155 batches | lr 2.44 | ms/batch 55.35 | loss  4.93 | ppl   138.68\n",
            "| epoch  42 |   400/ 1155 batches | lr 2.44 | ms/batch 57.29 | loss  4.91 | ppl   135.54\n",
            "| epoch  42 |   600/ 1155 batches | lr 2.44 | ms/batch 57.37 | loss  4.94 | ppl   139.74\n",
            "| epoch  42 |   800/ 1155 batches | lr 2.44 | ms/batch 61.38 | loss  4.92 | ppl   137.32\n",
            "| epoch  42 |  1000/ 1155 batches | lr 2.44 | ms/batch 55.90 | loss  4.94 | ppl   139.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 68.41s | valid loss  5.34 | valid ppl   208.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  43 |   200/ 1155 batches | lr 2.32 | ms/batch 54.47 | loss  4.93 | ppl   138.04\n",
            "| epoch  43 |   400/ 1155 batches | lr 2.32 | ms/batch 56.60 | loss  4.91 | ppl   135.15\n",
            "| epoch  43 |   600/ 1155 batches | lr 2.32 | ms/batch 57.27 | loss  4.94 | ppl   139.50\n",
            "| epoch  43 |   800/ 1155 batches | lr 2.32 | ms/batch 56.96 | loss  4.92 | ppl   136.84\n",
            "| epoch  43 |  1000/ 1155 batches | lr 2.32 | ms/batch 54.71 | loss  4.93 | ppl   138.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 67.90s | valid loss  5.34 | valid ppl   208.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  44 |   200/ 1155 batches | lr 2.20 | ms/batch 58.68 | loss  4.93 | ppl   137.91\n",
            "| epoch  44 |   400/ 1155 batches | lr 2.20 | ms/batch 56.98 | loss  4.90 | ppl   134.78\n",
            "| epoch  44 |   600/ 1155 batches | lr 2.20 | ms/batch 56.43 | loss  4.93 | ppl   138.93\n",
            "| epoch  44 |   800/ 1155 batches | lr 2.20 | ms/batch 56.35 | loss  4.92 | ppl   136.32\n",
            "| epoch  44 |  1000/ 1155 batches | lr 2.20 | ms/batch 57.29 | loss  4.93 | ppl   138.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 68.08s | valid loss  5.34 | valid ppl   209.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  45 |   200/ 1155 batches | lr 2.09 | ms/batch 60.55 | loss  4.92 | ppl   137.52\n",
            "| epoch  45 |   400/ 1155 batches | lr 2.09 | ms/batch 57.98 | loss  4.90 | ppl   134.16\n",
            "| epoch  45 |   600/ 1155 batches | lr 2.09 | ms/batch 56.87 | loss  4.93 | ppl   138.76\n",
            "| epoch  45 |   800/ 1155 batches | lr 2.09 | ms/batch 58.69 | loss  4.91 | ppl   135.87\n",
            "| epoch  45 |  1000/ 1155 batches | lr 2.09 | ms/batch 57.89 | loss  4.93 | ppl   138.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 70.10s | valid loss  5.34 | valid ppl   209.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  46 |   200/ 1155 batches | lr 1.99 | ms/batch 57.63 | loss  4.92 | ppl   137.22\n",
            "| epoch  46 |   400/ 1155 batches | lr 1.99 | ms/batch 61.54 | loss  4.90 | ppl   133.91\n",
            "| epoch  46 |   600/ 1155 batches | lr 1.99 | ms/batch 59.69 | loss  4.93 | ppl   138.64\n",
            "| epoch  46 |   800/ 1155 batches | lr 1.99 | ms/batch 60.48 | loss  4.91 | ppl   135.59\n",
            "| epoch  46 |  1000/ 1155 batches | lr 1.99 | ms/batch 59.95 | loss  4.92 | ppl   137.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 71.89s | valid loss  5.34 | valid ppl   209.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  47 |   200/ 1155 batches | lr 1.89 | ms/batch 60.95 | loss  4.92 | ppl   136.95\n",
            "| epoch  47 |   400/ 1155 batches | lr 1.89 | ms/batch 61.01 | loss  4.90 | ppl   133.82\n",
            "| epoch  47 |   600/ 1155 batches | lr 1.89 | ms/batch 63.00 | loss  4.93 | ppl   138.47\n",
            "| epoch  47 |   800/ 1155 batches | lr 1.89 | ms/batch 63.43 | loss  4.91 | ppl   135.27\n",
            "| epoch  47 |  1000/ 1155 batches | lr 1.89 | ms/batch 61.12 | loss  4.92 | ppl   137.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 74.10s | valid loss  5.34 | valid ppl   209.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  48 |   200/ 1155 batches | lr 1.79 | ms/batch 62.26 | loss  4.92 | ppl   136.59\n",
            "| epoch  48 |   400/ 1155 batches | lr 1.79 | ms/batch 62.37 | loss  4.89 | ppl   133.28\n",
            "| epoch  48 |   600/ 1155 batches | lr 1.79 | ms/batch 62.19 | loss  4.93 | ppl   138.00\n",
            "| epoch  48 |   800/ 1155 batches | lr 1.79 | ms/batch 61.79 | loss  4.90 | ppl   134.91\n",
            "| epoch  48 |  1000/ 1155 batches | lr 1.79 | ms/batch 61.66 | loss  4.92 | ppl   136.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 74.29s | valid loss  5.34 | valid ppl   209.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  49 |   200/ 1155 batches | lr 1.71 | ms/batch 62.66 | loss  4.91 | ppl   136.23\n",
            "| epoch  49 |   400/ 1155 batches | lr 1.71 | ms/batch 62.64 | loss  4.89 | ppl   133.00\n",
            "| epoch  49 |   600/ 1155 batches | lr 1.71 | ms/batch 62.76 | loss  4.92 | ppl   137.64\n",
            "| epoch  49 |   800/ 1155 batches | lr 1.71 | ms/batch 62.29 | loss  4.90 | ppl   134.70\n",
            "| epoch  49 |  1000/ 1155 batches | lr 1.71 | ms/batch 59.99 | loss  4.92 | ppl   136.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 74.34s | valid loss  5.34 | valid ppl   209.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  50 |   200/ 1155 batches | lr 1.62 | ms/batch 64.47 | loss  4.91 | ppl   135.71\n",
            "| epoch  50 |   400/ 1155 batches | lr 1.62 | ms/batch 63.38 | loss  4.89 | ppl   132.81\n",
            "| epoch  50 |   600/ 1155 batches | lr 1.62 | ms/batch 61.66 | loss  4.92 | ppl   137.66\n",
            "| epoch  50 |   800/ 1155 batches | lr 1.62 | ms/batch 61.27 | loss  4.90 | ppl   134.80\n",
            "| epoch  50 |  1000/ 1155 batches | lr 1.62 | ms/batch 62.42 | loss  4.92 | ppl   136.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 75.11s | valid loss  5.34 | valid ppl   209.24\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 50\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model)\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = copy.deepcopy(model)\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FbM0c93_J03",
        "outputId": "e04e513f-d8ec-4b7f-e199-96b28eb686fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  5.26 | test ppl   191.54\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
        "      f'test ppl {test_ppl:8.2f}')\n",
        "print('=' * 89)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(best_model.state_dict(), 'C:/Users/Colum/Documents/CS4125Repo/cs4125_project/FYP/Models/LSTM_Model.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "9778a034fa636adf5723e513073b31ecce0d3523c0ed5b8bcdd8a89b84fb4c6c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
